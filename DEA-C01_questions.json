[
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/133045-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 1 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 1,
    "question_text": "A data engineer is configuring an AWS Glue job to read data from an Amazon S3 bucket. The data engineer has set up the necessary AWS Glue connection details and an associated IAM role. However, when the data engineer attempts to run the AWS Glue job, the data engineer receives an error message that indicates that there are problems with the Amazon S3 VPC gateway endpoint.\nThe data engineer must resolve the error and connect the AWS Glue job to the S3 bucket.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Update the AWS Glue security group to allow inbound traffic from the Amazon S3 VPC gateway endpoint."
      },
      {
        "letter": "B",
        "text": "Configure an S3 bucket policy to explicitly grant the AWS Glue job permissions to access the S3 bucket."
      },
      {
        "letter": "C",
        "text": "Review the AWS Glue job code to ensure that the AWS Glue connection details include a fully qualified domain name."
      },
      {
        "letter": "D",
        "text": "Verify that the VPC's route table includes inbound and outbound routes for the Amazon S3 VPC gateway endpoint."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HunkyBunky",
        "date": "Mon 23 Sep 2024 10:25",
        "comment": "A - wrong - AWS glue - are serverless service, so it don't have any security groups\nB - wrong - Because we have error with VPC, not with S3 itself\nC - wrong - Becuase with S3 - we always have only FQDN for buckets",
        "selected_answer": "D"
      },
      {
        "author": "ASH1_4_3",
        "date": "Tue 14 Oct 2025 04:27",
        "comment": "This ensures that the Glue job can access the S3 bucket through the VPC endpoint and resolves connectivity errors.",
        "selected_answer": "D"
      },
      {
        "author": "Dreamer78692",
        "date": "Mon 12 May 2025 10:01",
        "comment": "THought needed an explicit security setting",
        "selected_answer": "B"
      },
      {
        "author": "ninomfr64",
        "date": "Mon 07 Apr 2025 16:32",
        "comment": "A- NO: on SG we just need to allow outbound traffic, as SG i statefull reurn traffic is allowed\nB - NO: since we configured IAM permission for Glue Job, there is no need to configure a resource-policy (cross account is not mentioned)\nC- NO: in bucket connection configuration you just need to provide s3://bucket-name/prefix\nD - YES: although there is no inbound and outbound routes in route table, we need to ensure a route is in place to reach a the VPC Gateway Policy",
        "selected_answer": "D"
      },
      {
        "author": "MephiboshethGumani",
        "date": "Sun 23 Feb 2025 13:48",
        "comment": "D. Verify that the VPC's route table includes inbound and outbound routes for the Amazon S3 VPC gateway endpoint.\n\nExplanation:\n\nAWS Glue jobs need to connect to the S3 bucket through the Amazon S3 VPC gateway endpoint when they are in a VPC. If the route table does not have proper inbound and outbound routes to the S3 VPC gateway endpoint, the AWS Glue job will not be able to access S3, which results in an error.",
        "selected_answer": "D"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 23 Sep 2024 10:25",
        "comment": "Although there is no such thing as \"inbound and outbound routes\" when we talk about VPC route table, when we define a S3 gateway endpoint we must have proper routes in place. I will go with D.",
        "selected_answer": "D"
      },
      {
        "author": "ampersandor",
        "date": "Fri 20 Sep 2024 22:03",
        "comment": "Be sure that the subnet configured for your AWS Glue connection has an Amazon S3 VPC gateway endpoint or a route to a NAT gateway in the subnet's route table.",
        "selected_answer": "D"
      },
      {
        "author": "GZMartinelli",
        "date": "Sat 07 Sep 2024 15:13",
        "comment": "D is correct",
        "selected_answer": "D"
      },
      {
        "author": "lunachi4",
        "date": "Thu 18 Jul 2024 03:57",
        "comment": "I think D.  We check \"VPC's route table\"",
        "selected_answer": "D"
      },
      {
        "author": "teo2157",
        "date": "Wed 03 Jul 2024 10:18",
        "comment": "A - wrong - AWS glue doesn't have any security groups\nB - wrong - You can´t give permissions in the S3 to the AWS glue job but to the role\nD. wrong because there has to be a definend route for the S3 gateway endpoint in the subnet assigned to the glue job but not in the VPC's route table and also route tables doesn´t have inbound and outbound routes.",
        "selected_answer": "C"
      },
      {
        "author": "nanaw770",
        "date": "Mon 03 Jun 2024 00:38",
        "comment": "D is correct answer.",
        "selected_answer": "D"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 03:35",
        "comment": "\"problems with the Amazon S3 VPC gateway endpoint\"",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131714-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 2 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 2,
    "question_text": "A retail company has a customer data hub in an Amazon S3 bucket. Employees from many countries use the data hub to support company-wide analytics. A governance team must ensure that the company's data analysts can access data only for customers who are within the same country as the analysts.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a separate table for each country's customer data. Provide access to each analyst based on the country that the analyst serves."
      },
      {
        "letter": "B",
        "text": "Register the S3 bucket as a data lake location in AWS Lake Formation. Use the Lake Formation row-level security features to enforce the company's access policies."
      },
      {
        "letter": "C",
        "text": "Move the data to AWS Regions that are close to the countries where the customers are. Provide access to each analyst based on the country that the analyst serves."
      },
      {
        "letter": "D",
        "text": "Load the data into Amazon Redshift. Create a view for each country. Create separate IAM roles for each country to provide access to data from each country. Assign the appropriate roles to the analysts."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "k350Secops",
        "date": "Thu 09 May 2024 05:27",
        "comment": "AWS Lake Formation: It's specifically designed for managing data lakes on AWS, providing capabilities for securing and controlling access to data.\nRow-Level Security: With Lake Formation, you can define fine-grained access control policies, including row-level security. This means you can enforce policies to restrict access to data based on specific conditions, such as the country associated with each customer.\nLeast Operational Effort: Once the policies are defined within Lake Formation, they can be centrally managed and applied to the data in the S3 bucket without the need for creating separate tables or views for each country, as in options A, C, and D. This reduces operational overhead and complexity.",
        "selected_answer": "B"
      },
      {
        "author": "swadesh2001",
        "date": "Sun 24 Aug 2025 14:46",
        "comment": "The correct answer is B",
        "selected_answer": "B"
      },
      {
        "author": "Mike_27",
        "date": "Tue 10 Jun 2025 02:01",
        "comment": "Strongly agree with B",
        "selected_answer": "B"
      },
      {
        "author": "dried0extents",
        "date": "Mon 10 Mar 2025 23:26",
        "comment": "I agree that it is A",
        "selected_answer": "A"
      },
      {
        "author": "lunachi4",
        "date": "Thu 18 Jul 2024 04:13",
        "comment": "Select B. It means \"with the LEAST operational effort\".",
        "selected_answer": "B"
      },
      {
        "author": "nanaw770",
        "date": "Mon 03 Jun 2024 00:38",
        "comment": "B is correct answer.",
        "selected_answer": "B"
      },
      {
        "author": "mattia_besharp",
        "date": "Tue 02 Apr 2024 09:56",
        "comment": "AWS really likes Lakeformation, plus creating separate tables might require some refactoring, and the requirements is about the LEAST operational effor",
        "selected_answer": "B"
      },
      {
        "author": "rishadhb",
        "date": "Wed 27 Mar 2024 18:29",
        "comment": "Agreed with Bartosz. I think setup DataLake, then integrate it with LakeFormation take a lot of effort than just separate the table",
        "selected_answer": "A"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 09:00",
        "comment": "Keyword \"LEAST operational effort\" - I will go with B",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 3 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 3,
    "question_text": "A media company wants to improve a system that recommends media content to customer based on user behavior and preferences. To improve the recommendation system, the company needs to incorporate insights from third-party datasets into the company's existing analytics platform.\nThe company wants to minimize the effort and time required to incorporate third-party datasets.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use API calls to access and integrate third-party datasets from AWS Data Exchange."
      },
      {
        "letter": "B",
        "text": "Use API calls to access and integrate third-party datasets from AWS DataSync."
      },
      {
        "letter": "C",
        "text": "Use Amazon Kinesis Data Streams to access and integrate third-party datasets from AWS CodeCommit repositories."
      },
      {
        "letter": "D",
        "text": "Use Amazon Kinesis Data Streams to access and integrate third-party datasets from Amazon Elastic Container Registry (Amazon ECR)."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "KelvinPun",
        "date": "Fri 20 Sep 2024 22:03",
        "comment": "AWS DataSync is primarily used for data transfer services designed to simplify, automate, and accelerate moving data between on-premises storage systems and AWS storage services, as well as between different AWS storage services. Its primary role is not for accessing third-party datasets but for efficiently transferring large volumes of data.\nIn contrast, AWS Data Exchange is designed specifically for discovering and subscribing to third-party data in the cloud, providing direct API access to these datasets, which aligns perfectly with the company's need to integrate this data into their recommendation systems with minimal overhead.",
        "selected_answer": "A"
      },
      {
        "author": "hsnin",
        "date": "Mon 18 Mar 2024 08:25",
        "comment": "AWS Data Exchange is a service that makes it easy to share and manage data permissions from other organizations",
        "selected_answer": "A"
      },
      {
        "author": "swadesh2001",
        "date": "Sun 24 Aug 2025 14:48",
        "comment": "Answer should be A",
        "selected_answer": "A"
      },
      {
        "author": "ttpro1995",
        "date": "Mon 23 Dec 2024 17:48",
        "comment": "Yeah, AWS want people to buy data from their marketplace. So, ... you know.",
        "selected_answer": "A"
      },
      {
        "author": "lunachi4",
        "date": "Thu 18 Jul 2024 08:07",
        "comment": "I will go with A. Kinesis Data Stram is more operational overhead.",
        "selected_answer": "A"
      },
      {
        "author": "Manohar24",
        "date": "Thu 11 Jul 2024 07:36",
        "comment": "A is correct",
        "selected_answer": "A"
      },
      {
        "author": "nanaw770",
        "date": "Mon 03 Jun 2024 00:39",
        "comment": "A is correct answer.",
        "selected_answer": "A"
      },
      {
        "author": "0060594",
        "date": "Wed 22 May 2024 16:55",
        "comment": "AWS DataExchange",
        "selected_answer": "A"
      },
      {
        "author": "k350Secops",
        "date": "Thu 09 May 2024 05:30",
        "comment": "options B, C, and D involve using Amazon Kinesis Data Streams or other services that may not be directly suited for integrating third-party datasets from external sources like AWS Data Exchange. These options might require additional configurations, data processing steps, or infrastructure management, resulting in higher operational overhead compared to directly leveraging AWS Data Exchange's capabilities through API calls (Option A).",
        "selected_answer": "A"
      },
      {
        "author": "Josa2",
        "date": "Sun 10 Mar 2024 10:40",
        "comment": "There is no info or guarantee this third-party dataset is available in AWS to be part of a data-share, hence the more assertive answer is B",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 09:03",
        "comment": "A for me. \"You can also discover and subscribe to new third-party data sets available through AWS Data Exchange\" \nhttps://docs.aws.amazon.com/data-exchange/latest/userguide/what-is.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131467-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 4 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 4,
    "question_text": "A financial company wants to implement a data mesh. The data mesh must support centralized data governance, data analysis, and data access control. The company has decided to use AWS Glue for data catalogs and extract, transform, and load (ETL) operations.\nWhich combination of AWS services will implement a data mesh? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Aurora for data storage. Use an Amazon Redshift provisioned cluster for data analysis."
      },
      {
        "letter": "B",
        "text": "Use Amazon S3 for data storage. Use Amazon Athena for data analysis."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue DataBrew for centralized data governance and access control."
      },
      {
        "letter": "D",
        "text": "Use Amazon RDS for data storage. Use Amazon EMR for data analysis."
      },
      {
        "letter": "E",
        "text": "Use AWS Lake Formation for centralized data governance and access control."
      }
    ],
    "correct_answer": "BE",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "hsnin",
        "date": "Mon 18 Mar 2024 08:30",
        "comment": "The answer is B and E.\nThe data mesh implementation uses Amazon S3 and Athena for data storage and analysis, and AWS Lake Formation for centralized data governance and access control. When combined with AWS Glue, you can efficiently manage your data.",
        "selected_answer": "BE"
      },
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:07",
        "comment": "Textbook question, the keyword data mesh means S3, the keyword data governance means LakeFormation",
        "selected_answer": "BE"
      },
      {
        "author": "ninomfr64",
        "date": "Tue 08 Apr 2025 05:09",
        "comment": "S3 (storage) LakeFormation (governance) Athena (analytics) can be used to implement data mesh. In real life you would use DataZone or nowadays SageMaker Unified Studio",
        "selected_answer": "BE"
      },
      {
        "author": "Shubham1989",
        "date": "Wed 25 Sep 2024 05:08",
        "comment": "S3 is best storage for data lake, and AWS lake formation is best for management.",
        "selected_answer": "BE"
      },
      {
        "author": "nanaw770",
        "date": "Mon 23 Sep 2024 10:27",
        "comment": "BE are correct answer.",
        "selected_answer": "BE"
      },
      {
        "author": "Josa2",
        "date": "Mon 23 Sep 2024 10:27",
        "comment": "Sometimes I think examtopics uses us to calibrate the right answers hehehe, by the goal statement and the services outlines and objectives there are no way the answer be different then B,E",
        "selected_answer": "BE"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 23 Sep 2024 10:27",
        "comment": "A: Cost-effective storage: Amazon S3 is a highly scalable and cost-effective object storage service perfect for storing large datasets commonly found in financial institutions.\nCentralized data lake: S3 acts as the central data lake where all data from different domains can reside in its raw or processed form.\nEasy data access: Athena provides a serverless interactive query service that allows data analysts to directly query data stored in S3 using standard SQL. This simplifies data exploration and analysis without managing servers.\nB: Data governance: Lake Formation helps establish data ownership, access control, and lineage for data products within the data mesh. It ensures data quality, security, and compliance with regulations.\nFine-grained access control: Lake Formation allows you to define granular access policies for each data domain, ensuring only authorized users can access specific data sets. This aligns with the need for centralized control in a data mesh.",
        "selected_answer": "BE"
      },
      {
        "author": "minhtien1707",
        "date": "Mon 25 Mar 2024 03:11",
        "comment": "i thing so",
        "selected_answer": "BE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 5 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 5,
    "question_text": "A data engineer maintains custom Python scripts that perform a data formatting process that many AWS Lambda functions use. When the data engineer needs to modify the Python scripts, the data engineer must manually update all the Lambda functions.\nThe data engineer requires a less manual way to update the Lambda functions.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Store a pointer to the custom Python scripts in the execution context object in a shared Amazon S3 bucket."
      },
      {
        "letter": "B",
        "text": "Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions."
      },
      {
        "letter": "C",
        "text": "Store a pointer to the custom Python scripts in environment variables in a shared Amazon S3 bucket."
      },
      {
        "letter": "D",
        "text": "Assign the same alias to each Lambda function. Call reach Lambda function by specifying the function's alias."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Mon 23 Sep 2024 10:28",
        "comment": "Centralized Code Management: Lambda layers allow you to store and manage the custom Python scripts in a central location outside the individual Lambda function code. This eliminates the need to update the script in each Lambda function manually.\nReusable Code: Layers provide a way to share code across multiple Lambda functions. Any changes made to the layer code are automatically reflected in all the functions using that layer, streamlining updates.\nReduced Deployment Size: By separating core functionality into layers, you can keep the individual Lambda function code focused and smaller. This reduces deployment package size and potentially improves Lambda execution times.",
        "selected_answer": "B"
      },
      {
        "author": "JavierEF",
        "date": "Tue 03 Sep 2024 13:16",
        "comment": "Lambda Layers is a feature created with this literal objective in mind.",
        "selected_answer": "B"
      },
      {
        "author": "4c78df0",
        "date": "Mon 27 May 2024 14:03",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "4c78df0",
        "date": "Sun 26 May 2024 09:12",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "FunkyFresco",
        "date": "Fri 17 May 2024 01:22",
        "comment": "Lamba layers",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131469-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 6 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 6,
    "question_text": "A company created an extract, transform, and load (ETL) data pipeline in AWS Glue. A data engineer must crawl a table that is in Microsoft SQL Server. The data engineer needs to extract, transform, and load the output of the crawl to an Amazon S3 bucket. The data engineer also must orchestrate the data pipeline.\nWhich AWS service or feature will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Step Functions"
      },
      {
        "letter": "B",
        "text": "AWS Glue workflows"
      },
      {
        "letter": "C",
        "text": "AWS Glue Studio"
      },
      {
        "letter": "D",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "dev_vicente",
        "date": "Mon 23 Sep 2024 10:28",
        "comment": "I asked an AI.\nAnalysis of the answers:\nA. AWS Step Functions:\nIt is a good option for orchestrating workflows with steps from different AWS services, but requires additional development to connect to Microsoft SQL Server.\nB. AWS Glue Workflows:\nThis is the best and most profitable option. AWS Glue is designed specifically for ETL on AWS and integrates directly with data sources such as Microsoft SQL Server through connectors. This allows for easier configuration and avoids the need for additional development.\nC. AWS Glue Studio:\nIt is a visual interface for AWS Glue that makes it easy to create and manage ETL jobs. However, the underlying functionality comes from AWS Glue (B) workflows.\nD. Amazon Managed Workflows for Apache Airflow (Amazon MWAA):\nIt's a viable option, but it's generally more expensive than native AWS services like AWS Glue Workflows. Additionally, it requires some Airflow experience for setup and maintenance.",
        "selected_answer": "B"
      },
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:32",
        "comment": "Glue workflows are the easiest solution here:\n\nhttps://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/\n\nhttps://aws.amazon.com/blogs/big-data/extracting-multidimensional-data-from-microsoft-sql-server-analysis-services-using-aws-glue/",
        "selected_answer": "B"
      },
      {
        "author": "Adrifersilva",
        "date": "Mon 30 Sep 2024 03:38",
        "comment": "https://community.aws/content/2iBQiAGS4RvEolgSQKu4iF8InTV/choose-the-right-data-orchestration-service-for-your-data-pipeline?lang=en",
        "selected_answer": "B"
      },
      {
        "author": "Shubham1989",
        "date": "Wed 25 Sep 2024 05:12",
        "comment": "Glue is easiest here to choose from.",
        "selected_answer": "B"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Fri 03 May 2024 09:18",
        "comment": "Agree with B. CRAWLING and ETL are the main functions of a Glue workflow and MS SQL is supported: https://docs.aws.amazon.com/glue/latest/dg/crawler-data-stores.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131470-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 7 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 7,
    "question_text": "A financial services company stores financial data in Amazon Redshift. A data engineer wants to run real-time queries on the financial data to support a web-based trading application. The data engineer wants to run the queries from within the trading application.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Establish WebSocket connections to Amazon Redshift."
      },
      {
        "letter": "B",
        "text": "Use the Amazon Redshift Data API."
      },
      {
        "letter": "C",
        "text": "Set up Java Database Connectivity (JDBC) connections to Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Store frequently accessed data in Amazon S3. Use Amazon S3 Select to run the queries."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "techai_geeks",
        "date": "Fri 17 Oct 2025 03:21",
        "comment": "“If your app talks to Redshift — think Data API, not JDBC!”\n(Data API = serverless way to query Redshift data from web/Lambda)\n\nB is correct",
        "selected_answer": "B"
      },
      {
        "author": "ninomfr64",
        "date": "Tue 08 Apr 2025 05:24",
        "comment": "You can query a Redshift cluster with either JDBC/ODBC or Data API. The latter just requires you to maintain AWS SDK and IAM role, while the former needs network plumbing to access VPC, JDBC/ODBC driver, database credentials possibly stored in Secret Manager",
        "selected_answer": "B"
      },
      {
        "author": "Scotty_Nguyen",
        "date": "Mon 24 Mar 2025 13:53",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "Palee",
        "date": "Wed 12 Mar 2025 21:42",
        "comment": "Most efficient solution",
        "selected_answer": "B"
      },
      {
        "author": "markill123",
        "date": "Tue 10 Sep 2024 23:13",
        "comment": "A) Redshift doesn't support WebSockets;\nC) It is way harder to manage DB connections than using Redshift Data API which will offer you the possibility to run SQL queries directly.\nD)",
        "selected_answer": "B"
      },
      {
        "author": "04e06cb",
        "date": "Sun 07 Jul 2024 13:36",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 00:14",
        "comment": "Inside application with minimal effort then using API would be correct",
        "selected_answer": "B"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Fri 03 May 2024 09:39",
        "comment": "\"The Amazon Redshift Data API enables you to painlessly access data from Amazon Redshift with all types of traditional, cloud-native, and containerized, serverless web service-based applications and event-driven applications.\"\nhttps://aws.amazon.com/de/blogs/big-data/using-the-amazon-redshift-data-api-to-interact-with-amazon-redshift-clusters/#:~:text=The%20Amazon%20Redshift%20Data%20API%20is%20not%20a%20replacement%20for,supported%20by%20the%20AWS%20SDK.",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 14:19",
        "comment": "Even if you don't know nothing about them, you will still choose B because it seems the \"LEAST operational overhead\" :)",
        "selected_answer": "B"
      },
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:35",
        "comment": "Real time queries with S3 are obviously BS. B it is:\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131471-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 8 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 8,
    "question_text": "A company uses Amazon Athena for one-time queries against data that is in Amazon S3. The company has several use cases. The company must implement permission controls to separate query processes and access to query history among users, teams, and applications that are in the same AWS account.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an S3 bucket for each use case. Create an S3 bucket policy that grants permissions to appropriate individual IAM users. Apply the S3 bucket policy to the S3 bucket."
      },
      {
        "letter": "B",
        "text": "Create an Athena workgroup for each use case. Apply tags to the workgroup. Create an IAM policy that uses the tags to apply appropriate permissions to the workgroup."
      },
      {
        "letter": "C",
        "text": "Create an IAM role for each use case. Assign appropriate permissions to the role for each use case. Associate the role with Athena."
      },
      {
        "letter": "D",
        "text": "Create an AWS Glue Data Catalog resource policy that grants permissions to appropriate individual IAM users for each use case. Apply the resource policy to the specific tables that Athena uses."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:38",
        "comment": "Haha they copied this from the old DA Specialty. It's B\n\nhttps://docs.aws.amazon.com/athena/latest/ug/user-created-workgroups.html",
        "selected_answer": "B"
      },
      {
        "author": "Scotty_Nguyen",
        "date": "Mon 24 Mar 2025 14:52",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "Manohar24",
        "date": "Thu 11 Jul 2024 07:47",
        "comment": "B is correct.",
        "selected_answer": "B"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 00:19",
        "comment": "The only other answer that's confusing is C But its not the one. Creating separate IAM roles for each use case and associating them with Athena would not provide the necessary isolation and access control for query processes and query history.",
        "selected_answer": "B"
      },
      {
        "author": "dev_vicente",
        "date": "Mon 25 Mar 2024 13:01",
        "comment": "B is more granular",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132628-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 9 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 9,
    "question_text": "A data engineer needs to schedule a workflow that runs a set of AWS Glue jobs every day. The data engineer does not require the Glue jobs to run or finish at a specific time.\nWhich solution will run the Glue jobs in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Choose the FLEX execution class in the Glue job properties."
      },
      {
        "letter": "B",
        "text": "Use the Spot Instance type in Glue job properties."
      },
      {
        "letter": "C",
        "text": "Choose the STANDARD execution class in the Glue job properties."
      },
      {
        "letter": "D",
        "text": "Choose the latest version in the GlueVersion field in the Glue job properties."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Sat 08 Jun 2024 16:58",
        "comment": "The FLEX execution class leverages spare capacity within the AWS infrastructure to run Glue jobs at a discounted price compared to the standard execution class. Since the data engineer doesn't have specific time constraints, utilizing spare capacity is ideal for cost savings.\nToday's date its a checkbox in order to spare capacity and will mean we dont know when is going to finish, which is recommended to increase a timeout",
        "selected_answer": "A"
      },
      {
        "author": "GabrielSGoncalves",
        "date": "Wed 24 Jul 2024 17:18",
        "comment": "FLEX is how you lower Glue cost when you dont have urgency to run ETLs.",
        "selected_answer": "A"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 00:22",
        "comment": "As its said the FLEX job comes cheaper that hiring a spot instance",
        "selected_answer": "A"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 21:58",
        "comment": "I'd go with A",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131472-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 10 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 10,
    "question_text": "A data engineer needs to create an AWS Lambda function that converts the format of data from .csv to Apache Parquet. The Lambda function must run only if a user uploads a .csv file to an Amazon S3 bucket.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification."
      },
      {
        "letter": "B",
        "text": "Create an S3 event notification that has an event type of s3:ObjectTagging:* for objects that have a tag set to .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification."
      },
      {
        "letter": "C",
        "text": "Create an S3 event notification that has an event type of s3:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set the Amazon Resource Name (ARN) of the Lambda function as the destination for the event notification."
      },
      {
        "letter": "D",
        "text": "Create an S3 event notification that has an event type of s3:ObjectCreated:*. Use a filter rule to generate notifications only when the suffix includes .csv. Set an Amazon Simple Notification Service (Amazon SNS) topic as the destination for the event notification. Subscribe the Lambda function to the SNS topic."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:43",
        "comment": "\"only if a user uploads data to an Amazon S3 bucket\" that excludes B & C because we need s3:ObjectCreated:*\n\nYou don't need SNS for S3 event notifications so A is easier.",
        "selected_answer": "A"
      },
      {
        "author": "TheWheelsofSteel",
        "date": "Tue 29 Jul 2025 15:26",
        "comment": "A is correct. SNS is useless in this scenario and it adds an extra layer of complexity",
        "selected_answer": "A"
      },
      {
        "author": "Adrifersilva",
        "date": "Mon 30 Sep 2024 05:34",
        "comment": "s3:ObjectCreated:* instead of s3:*: triggers the Lambda function only when objects are created in the bucket.",
        "selected_answer": "A"
      },
      {
        "author": "theloseralreadytaken",
        "date": "Tue 24 Sep 2024 09:24",
        "comment": "A is the answer for least operational. C also correct!",
        "selected_answer": "A"
      },
      {
        "author": "pypelyncar",
        "date": "Sat 08 Jun 2024 17:01",
        "comment": "since is the least operational, the D its a candidate, however add a SNS operation, which in this case is not needed. so A includes S3 and triggering towards the lambda function. 2 services.",
        "selected_answer": "A"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 00:28",
        "comment": "S3 event notification to lamba for file prefix with.csv is the least overhead way",
        "selected_answer": "A"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Fri 03 May 2024 09:53",
        "comment": "\"You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131473-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 11 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 11,
    "question_text": "A data engineer needs Amazon Athena queries to finish faster. The data engineer notices that all the files the Athena queries use are currently stored in uncompressed .csv format. The data engineer also notices that users perform most queries by selecting a specific column.\nWhich solution will MOST speed up the Athena query performance?",
    "choices": [
      {
        "letter": "A",
        "text": "Change the data format from .csv to JSON format. Apply Snappy compression."
      },
      {
        "letter": "B",
        "text": "Compress the .csv files by using Snappy compression."
      },
      {
        "letter": "C",
        "text": "Change the data format from .csv to Apache Parquet. Apply Snappy compression."
      },
      {
        "letter": "D",
        "text": "Compress the .csv files by using gzip compression."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:47",
        "comment": "If the exam would only have these kinds of questions everyone would be blessed",
        "selected_answer": "C"
      },
      {
        "author": "vasimahamed",
        "date": "Mon 07 Jul 2025 17:30",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "younux",
        "date": "Wed 02 Jul 2025 09:14",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "Scotty_Nguyen",
        "date": "Tue 25 Mar 2025 13:29",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "GabrielSGoncalves",
        "date": "Wed 24 Jul 2024 17:22",
        "comment": "C is the way to do It based on best practices recommended by AWS (https://aws.amazon.com/pt/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)",
        "selected_answer": "C"
      },
      {
        "author": "hnk",
        "date": "Sun 12 May 2024 07:10",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 00:40",
        "comment": "switching to Apache Parquet format with Snappy compression offers the most significant improvement in Athena query performance, especially for queries that select specific columns",
        "selected_answer": "C"
      },
      {
        "author": "d8945a1",
        "date": "Mon 06 May 2024 03:23",
        "comment": "Parquet is columnar storage and the question specifies that users performs most queries by selecting a specific column.",
        "selected_answer": "C"
      },
      {
        "author": "wa212",
        "date": "Mon 08 Apr 2024 14:46",
        "comment": "https://aws.amazon.com/jp/blogs/news/top-10-performance-tuning-tips-for-amazon-athena/",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131474-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 12 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 12,
    "question_text": "A manufacturing company collects sensor data from its factory floor to monitor and enhance operational efficiency. The company uses Amazon Kinesis Data Streams to publish the data that the sensors collect to a data stream. Then Amazon Kinesis Data Firehose writes the data to an Amazon S3 bucket.\nThe company needs to display a real-time view of operational efficiency on a large screen in the manufacturing facility.\nWhich solution will meet these requirements with the LOWEST latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Use a connector for Apache Flink to write data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard."
      },
      {
        "letter": "B",
        "text": "Configure the S3 bucket to send a notification to an AWS Lambda function when any new object is created. Use the Lambda function to publish the data to Amazon Aurora. Use Aurora as a source to create an Amazon QuickSight dashboard."
      },
      {
        "letter": "C",
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to process the sensor data. Create a new Data Firehose delivery stream to publish data directly to an Amazon Timestream database. Use the Timestream database as a source to create an Amazon QuickSight dashboard."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue bookmarks to read sensor data from the S3 bucket in real time. Publish the data to an Amazon Timestream database. Use the Timestream database as a source to create a Grafana dashboard."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "fceb2c1",
        "date": "Sat 23 Mar 2024 14:16",
        "comment": "https://aws.amazon.com/blogs/database/near-real-time-processing-with-amazon-kinesis-amazon-timestream-and-grafana/\n\nLook at the architecture diagram",
        "selected_answer": "A"
      },
      {
        "author": "milofficial",
        "date": "Thu 18 Jan 2024 09:51",
        "comment": "real time -> no Quicksight. And bookmarks to read sensor data real time is just as stupid as the flat earth theory. A it is.",
        "selected_answer": "A"
      },
      {
        "author": "Scotty_Nguyen",
        "date": "Tue 25 Mar 2025 13:47",
        "comment": "A is correct",
        "selected_answer": "A"
      },
      {
        "author": "Adrifersilva",
        "date": "Mon 30 Sep 2024 06:02",
        "comment": "Grafana:\nReal-time Performance:\nGrafana is known for its excellent real-time data visualization capabilities.\nIt's often used for operational dashboards that require frequent updates.\n\nIntegration:\nWorks well with time-series databases and streaming data sources. [2]",
        "selected_answer": "A"
      },
      {
        "author": "deepcloud",
        "date": "Tue 20 Aug 2024 06:12",
        "comment": "Firehose cannot use Timestream as destination. Answer is A",
        "selected_answer": "A"
      },
      {
        "author": "teo2157",
        "date": "Tue 13 Aug 2024 15:39",
        "comment": "Amazon QuickSight is primarily designed for business intelligence and data visualization, and it can provide near real-time views depending on the data refresh rate. However, it is not typically used for real-time streaming data visualization with very low latency. For real-time dashboards with very low latency, services like Grafana are more suitable.\nYou can use Amazon Managed Grafana to setup the dashboard so you're using an AWS service which is always preferible on these exams.",
        "selected_answer": "A"
      },
      {
        "author": "V0811",
        "date": "Mon 05 Aug 2024 07:02",
        "comment": "Because: The company needs to display a real-time view of operational efficiency on a large screen in the manufacturing facility.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131709-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 13 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 13,
    "question_text": "A company stores daily records of the financial performance of investment portfolios in .csv format in an Amazon S3 bucket. A data engineer uses AWS Glue crawlers to crawl the S3 data.\nThe data engineer must make the S3 data accessible daily in the AWS Glue Data Catalog.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Configure the output destination to a new path in the existing S3 bucket."
      },
      {
        "letter": "B",
        "text": "Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Create a daily schedule to run the crawler. Specify a database name for the output."
      },
      {
        "letter": "C",
        "text": "Create an IAM role that includes the AmazonS3FullAccess policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Specify a database name for the output."
      },
      {
        "letter": "D",
        "text": "Create an IAM role that includes the AWSGlueServiceRole policy. Associate the role with the crawler. Specify the S3 bucket path of the source data as the crawler's data store. Allocate data processing units (DPUs) to run the crawler every day. Configure the output destination to a new path in the existing S3 bucket."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 12:52",
        "comment": "A,C are wrong because you use don't need full S3 access. D is wrong because you don't need to provision DPU and the destination should be a database, not an s3 bucket. so it's B",
        "selected_answer": "B"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 12:33",
        "comment": "answer B is incomplete. Even we include AWSGlueServiceRole policy on IAM role, S3 access is not garantee",
        "selected_answer": "B"
      },
      {
        "author": "k350Secops",
        "date": "Fri 10 May 2024 01:27",
        "comment": "Glue Crawlers are serverless. Assigning DPUs is the point where i decided it option B",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131675-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 14 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 14,
    "question_text": "A company loads transaction data for each day into Amazon Redshift tables at the end of each day. The company wants to have the ability to track which tables have been loaded and which tables still need to be loaded.\nA data engineer wants to store the load statuses of Redshift tables in an Amazon DynamoDB table. The data engineer creates an AWS Lambda function to publish the details of the load statuses to DynamoDB.\nHow should the data engineer invoke the Lambda function to write load statuses to the DynamoDB table?",
    "choices": [
      {
        "letter": "A",
        "text": "Use a second Lambda function to invoke the first Lambda function based on Amazon CloudWatch events."
      },
      {
        "letter": "B",
        "text": "Use the Amazon Redshift Data API to publish an event to Amazon EventBridge. Configure an EventBridge rule to invoke the Lambda function."
      },
      {
        "letter": "C",
        "text": "Use the Amazon Redshift Data API to publish a message to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke the Lambda function."
      },
      {
        "letter": "D",
        "text": "Use a second Lambda function to invoke the first Lambda function based on AWS CloudTrail events."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 11:48",
        "comment": "https://docs.aws.amazon.com/redshift/latest/mgmt/data-api-monitoring-events.html",
        "selected_answer": "B"
      },
      {
        "author": "MephiboshethGumani",
        "date": "Sat 10 May 2025 19:33",
        "comment": "Here's why Option B is best:\n\nAmazon Redshift Data API can be used by applications and scripts to interact with Redshift (e.g., run SQL queries, check load status).\n\nAmazon EventBridge can receive custom events or service-generated events and route them to targets like Lambda.\n\nThis approach decouples the data load process from status logging, using EventBridge as a clean integration point.\n\nEventBridge rule can filter and trigger the Lambda function when Redshift (or your data pipeline) signals a successful data load.",
        "selected_answer": "B"
      },
      {
        "author": "MephiboshethGumani",
        "date": "Fri 14 Mar 2025 19:04",
        "comment": "the data engineer should use Amazon EventBridge (formerly CloudWatch Events) to trigger the Lambda function based on a schedule or events that correspond to the completion of the data load process in Amazon Redshift.",
        "selected_answer": "B"
      },
      {
        "author": "altonh",
        "date": "Wed 04 Dec 2024 02:01",
        "comment": "The statement in B is inaccurate.\nYou don't 'use Amazon Redshift Data API to publish' event to EventBridge. Redshift Data API has no function to write to EventBridge. Instead, the statement should be \"Use EventBridge to monitor Data API events...\" Perhaps this is a typo.\n\nBut if I assume there are no typos in all the statements, then I would go for D. Although not a perfect solution, the cloud trail events have more info than the Redshift Data API events.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 15 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 15,
    "question_text": "A data engineer needs to securely transfer 5 TB of data from an on-premises data center to an Amazon S3 bucket. Approximately 5% of the data changes every day. Updates to the data need to be regularly proliferated to the S3 bucket. The data includes files that are in multiple formats. The data engineer needs to automate the transfer process and must schedule the process to run periodically.\nWhich AWS service should the data engineer use to transfer the data in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS DataSync"
      },
      {
        "letter": "B",
        "text": "AWS Glue"
      },
      {
        "letter": "C",
        "text": "AWS Direct Connect"
      },
      {
        "letter": "D",
        "text": "Amazon S3 Transfer Acceleration"
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ranginprithibi",
        "date": "Mon 21 Jul 2025 10:13",
        "comment": "Data Changes Everyday - It is only tracked by Data Sync",
        "selected_answer": "A"
      },
      {
        "author": "younux",
        "date": "Wed 02 Jul 2025 09:21",
        "comment": "C is correct",
        "selected_answer": "A"
      },
      {
        "author": "deepbro",
        "date": "Wed 04 Jun 2025 02:18",
        "comment": "Data Sync all the way",
        "selected_answer": "A"
      },
      {
        "author": "Scotty_Nguyen",
        "date": "Thu 24 Apr 2025 15:22",
        "comment": "AWS DataSync:\nPurpose: AWS DataSync is designed for automated, efficient, and secure data transfer between on-premises storage and AWS storage services like Amazon S3.\nKey Features:\nAutomates and schedules data transfers, supporting periodic syncs.\nHandles incremental transfers, only copying changed or new files (ideal for the 5% daily changes).\nSupports multiple file formats and preserves metadata.\nProvides encryption for secure transfers.\nScales to handle large datasets like 5 TB and optimizes transfer performance.\nWhy it fits: DataSync meets all requirements by automating periodic transfers, efficiently handling incremental updates, and supporting diverse file formats with minimal operational overhead.",
        "selected_answer": "A"
      },
      {
        "author": "sam_pre",
        "date": "Mon 24 Mar 2025 16:58",
        "comment": "DataSync perfectly fit for this requirement",
        "selected_answer": "A"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 02:39",
        "comment": "DataSync, locations, tasks, is all what you need.",
        "selected_answer": "A"
      },
      {
        "author": "FunkyFresco",
        "date": "Fri 17 May 2024 01:41",
        "comment": "is datasync",
        "selected_answer": "A"
      },
      {
        "author": "KelvinPun",
        "date": "Fri 19 Apr 2024 08:47",
        "comment": "That's the job of DataSync",
        "selected_answer": "A"
      },
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 11:52",
        "comment": "Typical DataSync use case",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 16 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 16,
    "question_text": "A company uses an on-premises Microsoft SQL Server database to store financial transaction data. The company migrates the transaction data from the on-premises database to AWS at the end of each month. The company has noticed that the cost to migrate data from the on-premises database to an Amazon RDS for SQL Server database has increased recently.\nThe company requires a cost-effective solution to migrate the data to AWS. The solution must cause minimal downtown for the applications that access the database.\nWhich AWS service should the company use to meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Lambda"
      },
      {
        "letter": "B",
        "text": "AWS Database Migration Service (AWS DMS)"
      },
      {
        "letter": "C",
        "text": "AWS Direct Connect"
      },
      {
        "letter": "D",
        "text": "AWS DataSync"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 11:54",
        "comment": "Whoever is the admin that pre-marks the answers, it's time to go",
        "selected_answer": "B"
      },
      {
        "author": "Ranginprithibi",
        "date": "Mon 21 Jul 2025 10:18",
        "comment": "DMS caters CDC (change data capture) thereby minimizing the data to be transferred.",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 02:44",
        "comment": "AWS DMS offers a cost-effective solution for database migrations compared to replicating data to a fully managed RDS instance.\nYou only pay for the resources used during the migration, making it ideal for infrequent, monthly transfers",
        "selected_answer": "B"
      },
      {
        "author": "lucas_rfsb",
        "date": "Sun 31 Mar 2024 00:37",
        "comment": "B Since it's for Migration porpouse, typical for DMS",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131679-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 17 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 17,
    "question_text": "A data engineer is building a data pipeline on AWS by using AWS Glue extract, transform, and load (ETL) jobs. The data engineer needs to process data from Amazon RDS and MongoDB, perform transformations, and load the transformed data into Amazon Redshift for analytics. The data updates must occur every hour.\nWhich combination of tasks will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Configure AWS Glue triggers to run the ETL jobs every hour."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue DataBrew to clean and prepare the data for analytics."
      },
      {
        "letter": "C",
        "text": "Use AWS Lambda functions to schedule and run the ETL jobs every hour."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift."
      },
      {
        "letter": "E",
        "text": "Use the Redshift Data API to load transformed data into Amazon Redshift."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Mon 22 Jan 2024 06:58",
        "comment": "AWS Glue triggers provide a simple and integrated way to schedule ETL jobs. By configuring these triggers to run hourly, the data engineer can ensure that the data processing and updates occur as required without the need for external scheduling tools or custom scripts. This approach is directly integrated with AWS Glue, reducing the complexity and operational overhead.\nAWS Glue supports connections to various data sources, including Amazon RDS and MongoDB. By using AWS Glue connections, the data engineer can easily configure and manage the connectivity between these data sources and Amazon Redshift. This method leverages AWS Glue’s built-in capabilities for data source integration, thus minimizing operational complexity and ensuring a seamless data flow from the sources to the destination (Amazon Redshift).",
        "selected_answer": "AD"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 02:49",
        "comment": "A. Configure AWS Glue triggers to run the ETL jobs every hour.\n    Reduced Code Complexity: Glue triggers eliminate the need to write custom code for scheduling ETL jobs. This simplifies the pipeline and reduces maintenance overhead.\n    Scalability and Integration: Glue triggers work seamlessly with Glue ETL jobs, ensuring efficient scheduling and execution within the Glue ecosystem.\nD. Use AWS Glue connections to establish connectivity between the data sources and Amazon Redshift.\n    Pre-Built Connectors: Glue connections offer pre-built connectors for various data sources like RDS and Redshift. This eliminates the need for manual configuration and simplifies data source access within the ETL jobs.\n    Centralized Management: Glue connections are centrally managed within the Glue service, streamlining connection management and reducing operational overhead.",
        "selected_answer": "AD"
      },
      {
        "author": "saransh_001",
        "date": "Thu 13 Feb 2025 15:51",
        "comment": "A. AWS Glue provides a built-in mechanism to trigger ETL jobs at scheduled intervals, such as every hour. Using Glue triggers minimizes the need for additional custom code or services, reducing operational overhead.\nD. AWS Glue connections simplify the process of establishing secure and reliable connections to various data sources (Amazon RDS, MongoDB) and the destination (Amazon Redshift). This approach reduces the need for manually configuring connection settings and makes the ETL pipeline easier to maintain.",
        "selected_answer": "AD"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Fri 03 May 2024 11:12",
        "comment": "I was not sure about A - But in AWS console => Glue => Triggers => Add Trigger I have found the Trigger type: \"Schedule - Fire the trigger on a timer.\"",
        "selected_answer": "AD"
      },
      {
        "author": "lucas_rfsb",
        "date": "Sun 31 Mar 2024 21:24",
        "comment": "I found this question actually confusing. In which step the transformation would be implemented itself? I can be wrong, but with Glue triggers we would only run the job, but not the transformation logic itself. In this way, I would go in C and D",
        "selected_answer": "CD"
      },
      {
        "author": "milofficial",
        "date": "Mon 18 Mar 2024 16:28",
        "comment": "Not a clear question - B would kinda make sense - but AD seems to be more correct",
        "selected_answer": "AD"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 13:16",
        "comment": "A - this is obvious and D -https://docs.aws.amazon.com/glue/latest/dg/console-connections.html",
        "selected_answer": "AD"
      },
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 11:59",
        "comment": "Lambda triggers for Glue jobs make me dizzy",
        "selected_answer": "AB"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131680-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 18 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 18,
    "question_text": "A company uses an Amazon Redshift cluster that runs on RA3 nodes. The company wants to scale read and write capacity to meet demand. A data engineer needs to identify a solution that will turn on concurrency scaling.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Turn on concurrency scaling in workload management (WLM) for Redshift Serverless workgroups."
      },
      {
        "letter": "B",
        "text": "Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster."
      },
      {
        "letter": "C",
        "text": "Turn on concurrency scaling in the settings during the creation of any new Redshift cluster."
      },
      {
        "letter": "D",
        "text": "Turn on concurrency scaling for the daily usage quota for the Redshift cluster."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "saransh_001",
        "date": "Thu 13 Feb 2025 15:55",
        "comment": "Concurrency Scaling in Amazon Redshift is a feature that automatically adds temporary clusters to handle spikes in query traffic, providing additional read and write capacity.\nThis feature is enabled through Workload Management (WLM) at the queue level in Redshift. Each queue can be configured to use concurrency scaling for handling queries that exceed the capacity of the main cluster.\nWhy option A is incorrect:\nTurn on concurrency scaling in workload management (WLM) for Redshift Serverless workgroups: This option is for Redshift Serverless rather than clusters on RA3 nodes. Serverless clusters handle scaling differently and don't require manual concurrency scaling settings like the RA3 clusters.",
        "selected_answer": "B"
      },
      {
        "author": "d8945a1",
        "date": "Mon 06 May 2024 04:54",
        "comment": "B. Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster.",
        "selected_answer": "B"
      },
      {
        "author": "khchan123",
        "date": "Sat 27 Apr 2024 06:53",
        "comment": "Answer is B.\nB. Turn on concurrency scaling at the workload management (WLM) queue level in the Redshift cluster.",
        "selected_answer": "B"
      },
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 12:02",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 19 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 19,
    "question_text": "A data engineer must orchestrate a series of Amazon Athena queries that will run every day. Each query can run for more than 15 minutes.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Lambda function and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically."
      },
      {
        "letter": "B",
        "text": "Create an AWS Step Functions workflow and add two states. Add the first state before the Lambda function. Configure the second state as a Wait state to periodically check whether the Athena query has finished using the Athena Boto3 get_query_execution API call. Configure the workflow to invoke the next query when the current query has finished running."
      },
      {
        "letter": "C",
        "text": "Use an AWS Glue Python shell job and the Athena Boto3 client start_query_execution API call to invoke the Athena queries programmatically."
      },
      {
        "letter": "D",
        "text": "Use an AWS Glue Python shell script to run a sleep timer that checks every 5 minutes to determine whether the current Athena query has finished running successfully. Configure the Python shell script to invoke the next query when the current query has finished running."
      },
      {
        "letter": "E",
        "text": "Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the Athena queries in AWS Batch."
      }
    ],
    "correct_answer": "AB",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 13:42",
        "comment": "B - because\n https://docs.aws.amazon.com/step-functions/latest/dg/sample-athena-query.html\nE - because \nhttps://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-spark-jobs-with-amazon-mwaa-and-data-validation-using-amazon-athena/",
        "selected_answer": "BE"
      },
      {
        "author": "rralucard_",
        "date": "Mon 05 Feb 2024 04:13",
        "comment": "AWS Lambda can be effectively used to trigger Athena queries. By using the start_query_execution API from the Athena Boto3 client, you can programmatically start Athena queries. Lambda functions are cost-effective as they charge based on the compute time used, and there's no charge when the code is not running. However, Lambda has a maximum execution timeout of 15 minutes, which means it's not suitable for long-running operations but can be used to trigger or start queries.\nAWS Step Functions can orchestrate multiple AWS services in workflows. By using a Wait state, the workflow can periodically check the status of the Athena query, and proceed to the next step once the query is complete. This approach is more scalable and reliable compared to continuously running a Lambda function, as Step Functions can handle long-running processes better and can maintain the state of each step in the workflow.",
        "selected_answer": "AB"
      },
      {
        "author": "Evan_Lin",
        "date": "Wed 05 Feb 2025 09:11",
        "comment": "After real-world testing, A is a valid answer. This is because the Lambda only sends the API request to Athena, which runs the query. Even if the Lambda times out, the query result is still stored in the designated S3 bucket.",
        "selected_answer": "AB"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 15:11",
        "comment": "Why?\nB (Step Functions): Step Functions are ideal for orchestrating long-running workflows, including polling the Athena query status and invoking the next query when ready.\nA (Lambda): Lambda is used to programmatically trigger Athena queries within Step Functions, despite its 15-minute limitation, because Step Functions can manage the long runtime using Wait states.\nWhy Not C, D, or E?\nC and D involve Glue, which is better suited for ETL jobs than orchestration, making them less efficient and cost-effective.\nE (Amazon MWAA) introduces unnecessary cost and complexity for a straightforward workflow.",
        "selected_answer": "AB"
      },
      {
        "author": "haby",
        "date": "Sat 14 Dec 2024 16:13",
        "comment": "BC for me\nA - lambda function will stop at 900s, so it will stop before query finishes(more than 15mins)\nE - Airflow is way more complex and expensive than step function",
        "selected_answer": "BC"
      },
      {
        "author": "altonh",
        "date": "Wed 04 Dec 2024 03:45",
        "comment": "AB - Because of the Lambda timeout\n\nCE—is correct. The query will be executed by a glue job, which will be orchestrated by Airflow. The job will be scheduled using AWS Batch.",
        "selected_answer": "CE"
      },
      {
        "author": "Eleftheriia",
        "date": "Sun 01 Dec 2024 10:05",
        "comment": "Not E because \"You should use Step Functions if you prioritize cost and performance\"\nhttps://aws.amazon.com/managed-workflows-for-apache-airflow/faqs/\n\nAnd also the fact that the queries take longer than 15 min can be handled with step functions, therefore AB",
        "selected_answer": "AB"
      },
      {
        "author": "V0811",
        "date": "Mon 05 Aug 2024 07:14",
        "comment": "It should be AB",
        "selected_answer": "AB"
      },
      {
        "author": "alex1991",
        "date": "Sun 30 Jun 2024 12:03",
        "comment": "Since the Athena API supports async/await, users are able to separate the steps into trigger queries and get results after 15 minutes.",
        "selected_answer": "AB"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 03:03",
        "comment": "tricky, A is valid. Still, cost effective:\nB no one doubt on it. then why E?\nMWAA offers a managed Apache Airflow environment for orchestrating complex workflows.\nIt can handle long-running tasks like Athena queries efficiently.\nBatch Processing: Leveraging AWS Batch within the Airflow workflow allows for distributed and scalable execution of the Athena queries, improving overall processing efficiency.",
        "selected_answer": "BE"
      },
      {
        "author": "valuedate",
        "date": "Sat 25 May 2024 18:12",
        "comment": "my opinian",
        "selected_answer": "AB"
      },
      {
        "author": "valuedate",
        "date": "Wed 22 May 2024 11:45",
        "comment": "I would prefer AB",
        "selected_answer": "AB"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 05:52",
        "comment": "Lambda for kick start Athena\nStep Functions for orchestration",
        "selected_answer": "AB"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131684-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 20 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 20,
    "question_text": "A company is migrating on-premises workloads to AWS. The company wants to reduce overall operational overhead. The company also wants to explore serverless options.\nThe company's current workloads use Apache Pig, Apache Oozie, Apache Spark, Apache Hbase, and Apache Flink. The on-premises workloads process petabytes of data in seconds. The company must maintain similar or better performance after the migration to AWS.\nWhich extract, transform, and load (ETL) service will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Glue"
      },
      {
        "letter": "B",
        "text": "Amazon EMR"
      },
      {
        "letter": "C",
        "text": "AWS Lambda"
      },
      {
        "letter": "D",
        "text": "Amazon Redshift"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Sat 20 Jan 2024 12:20",
        "comment": "Glue is like the more good-looking one, but weaker brother of EMR. So when it's about petabyte scales, let EMR do the work and have Glue stay away from the action.",
        "selected_answer": "B"
      },
      {
        "author": "Ell89",
        "date": "Mon 24 Feb 2025 22:38",
        "comment": "Glue doesnt natively support Pig, HBase and Flink.",
        "selected_answer": "B"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 15:12",
        "comment": "Apache = EMR",
        "selected_answer": "B"
      },
      {
        "author": "heavenlypearl",
        "date": "Wed 06 Nov 2024 08:38",
        "comment": "Amazon EMR Serverless is a deployment option for Amazon EMR that provides a serverless runtime environment. This simplifies the operation of analytics applications that use the latest open-source frameworks, such as Apache Spark and Apache Hive. With EMR Serverless, you don’t have to configure, optimize, secure, or operate clusters to run applications with these frameworks.\n\nhttps://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/emr-serverless.html",
        "selected_answer": "B"
      },
      {
        "author": "Eleftheriia",
        "date": "Wed 28 Aug 2024 09:00",
        "comment": "I think it is A, Glue\n•  Amazon EMR is used for petabyte-scale data collection and data processing.\n•  AWS Glue is used as a serverless and managed ETL service, and also used for managing data quality with AWS Glue Data Quality.",
        "selected_answer": "A"
      },
      {
        "author": "San_Juan",
        "date": "Mon 26 Aug 2024 06:42",
        "comment": "Glue.\nIt talks about \"serverless\" so EMR is discarted. The mention of Spark, Hbase, etc is for confusing you, because it doesn't say that they wanted to keep using them. Glue can run Spark using \"glueContext\" (similar a SparkContext) for reading tables, files and create frames.",
        "selected_answer": "A"
      },
      {
        "author": "V0811",
        "date": "Mon 05 Aug 2024 07:17",
        "comment": "Serverless: AWS Glue is a fully managed, serverless ETL service that automates the process of data discovery, preparation, and transformation, helping minimize operational overhead.Integration with Big Data Tools: It integrates well with various AWS services and supports Spark jobs for ETL purposes, which aligns well with Apache Spark workloads.Performance: AWS Glue can handle large-scale ETL workloads, and it is designed to manage petabytes of data efficiently, comparable to the performance of on-premises solutions.While B. Amazon EMR could also be considered for its flexibility in handling big data workloads using tools like Apache Spark, it requires more management and doesn't fit the serverless requirement as closely as AWS Glue. Therefore, AWS Glue is the most suitable choice given the constraints and requirements.",
        "selected_answer": "A"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 03:06",
        "comment": "EMR provides a managed Hadoop framework that natively supports Apache Pig,\nOozie, Spark, and Flink. This allows the company to migrate their existing workloads with minimal code changes, reducing development effort",
        "selected_answer": "B"
      },
      {
        "author": "tgv",
        "date": "Sun 02 Jun 2024 07:27",
        "comment": "That's exactly the purpose of EMR. \n\n\"Amazon EMR is the industry-leading cloud big data solution for petabyte-scale data processing, interactive analytics, and machine learning using open-source frameworks such as Apache Spark, Apache Hive, and Presto.\"\n\nhttps://aws.amazon.com/emr/",
        "selected_answer": "B"
      },
      {
        "author": "Just_Ninja",
        "date": "Tue 07 May 2024 18:28",
        "comment": "Glue is Serverless :)",
        "selected_answer": "A"
      },
      {
        "author": "wa212",
        "date": "Tue 09 Apr 2024 00:55",
        "comment": "https://docs.aws.amazon.com/ja_jp/emr/latest/ManagementGuide/emr-what-is-emr.html",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 07 Mar 2024 13:48",
        "comment": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132653-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 21 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 21,
    "question_text": "A data engineer must use AWS services to ingest a dataset into an Amazon S3 data lake. The data engineer profiles the dataset and discovers that the dataset contains personally identifiable information (PII). The data engineer must implement a solution to profile the dataset and obfuscate the PII.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an Amazon Kinesis Data Firehose delivery stream to process the dataset. Create an AWS Lambda transform function to identify the PII. Use an AWS SDK to obfuscate the PII. Set the S3 data lake as the target for the delivery stream."
      },
      {
        "letter": "B",
        "text": "Use the Detect PII transform in AWS Glue Studio to identify the PII. Obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake."
      },
      {
        "letter": "C",
        "text": "Use the Detect PII transform in AWS Glue Studio to identify the PII. Create a rule in AWS Glue Data Quality to obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake."
      },
      {
        "letter": "D",
        "text": "Ingest the dataset into Amazon DynamoDB. Create an AWS Lambda function to identify and obfuscate the PII in the DynamoDB table and to transform the data. Use the same Lambda function to ingest the data into the S3 data lake."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "milofficial",
        "date": "Mon 18 Mar 2024 17:04",
        "comment": "How does Data Quality obfuscate PII? You can do this directly in Glue Studio: https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html",
        "selected_answer": "B"
      },
      {
        "author": "Khooks",
        "date": "Sat 22 Jun 2024 20:31",
        "comment": "Option C involves additional steps and complexity with creating rules in AWS Glue Data Quality, which adds more operational effort compared to directly using AWS Glue Studio's capabilities.",
        "selected_answer": "B"
      },
      {
        "author": "Kalyso",
        "date": "Sun 30 Mar 2025 00:57",
        "comment": "Actually it is B. No need to create a rule in AWS Glue.",
        "selected_answer": "B"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 13:13",
        "comment": "B. Use the Detect PII transform in AWS Glue Studio to identify the PII. Obfuscate the PII. Use an AWS Step Functions state machine to orchestrate a data pipeline to ingest the data into the S3 data lake.  Detect PII transform only detects. Obfuscate the PII ok but how ? Answer C explain how",
        "selected_answer": "C"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 15:16",
        "comment": "Why C is better than B:\nObfuscation clarity: Option C explicitly mentions using a Glue Data Quality rule to obfuscate PII, while option B does not specify how obfuscation is implemented.\nAccuracy: Glue Data Quality provides a more structured way to handle obfuscation compared to relying solely on Glue Studio's PII detection.\nThus, C is the most accurate and operationally efficient solution.",
        "selected_answer": "C"
      },
      {
        "author": "antun3ra",
        "date": "Thu 08 Aug 2024 03:30",
        "comment": "B provides a streamlined, mostly visual approach using purpose-built tools for data processing and PII handling, making it the solution with the least operational effort.",
        "selected_answer": "B"
      },
      {
        "author": "portland",
        "date": "Sat 27 Jul 2024 21:46",
        "comment": "https://aws.amazon.com/blogs/big-data/automated-data-governance-with-aws-glue-data-quality-sensitive-data-detection-and-aws-lake-formation/",
        "selected_answer": "C"
      },
      {
        "author": "qwertyuio",
        "date": "Fri 12 Jul 2024 08:30",
        "comment": "https://docs.aws.amazon.com/glue/latest/dg/detect-PII.html",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Mon 01 Jul 2024 01:20",
        "comment": "anwser is C",
        "selected_answer": "C"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 07:39",
        "comment": "We cannot directly handle PII with Glue Studio, and Glue Data Quality can be used to handle PII.",
        "selected_answer": "C"
      },
      {
        "author": "Just_Ninja",
        "date": "Wed 08 May 2024 05:55",
        "comment": "A very easy was is to use the SDK to identify PII.\n\nhttps://docs.aws.amazon.com/code-library/latest/ug/comprehend_example_comprehend_DetectPiiEntities_section.html",
        "selected_answer": "A"
      },
      {
        "author": "kairosfc",
        "date": "Sat 04 May 2024 14:39",
        "comment": "The transform Detect PII in AWS Glue Studio is specifically used to identify personally identifiable information (PII) within the data. It can detect and flag this information, but on its own, it does not perform the obfuscation or removal of these details.\n\nTo effectively obfuscate or alter the identified PII, an additional transformation would be necessary. This could be accomplished in several ways, such as:\n\nWriting a custom script within the same AWS Glue job using Python or Scala to modify the PII data as needed.\nUsing AWS Glue Data Quality, if available, to create rules that automatically obfuscate or modify the data identified as PII. AWS Glue Data Quality is a newer tool that helps improve data quality through rules and transformations, but whether it's needed will depend on the functionality's availability and the specificity of the obfuscation requirements",
        "selected_answer": "C"
      },
      {
        "author": "arvehisa",
        "date": "Sat 30 Mar 2024 14:13",
        "comment": "B is correct.\nC: glue data quality cannot obfuscate the PII\nD: need to write code but the question is the \"LEAST operational effort\"",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131710-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 22 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 22,
    "question_text": "A company maintains multiple extract, transform, and load (ETL) workflows that ingest data from the company's operational databases into an Amazon S3 based data lake. The ETL workflows use AWS Glue and Amazon EMR to process data.\nThe company wants to improve the existing architecture to provide automated orchestration and to require minimal manual effort.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Glue workflows"
      },
      {
        "letter": "B",
        "text": "AWS Step Functions tasks"
      },
      {
        "letter": "C",
        "text": "AWS Lambda functions"
      },
      {
        "letter": "D",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA) workflows"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "valuedate",
        "date": "Wed 22 May 2024 11:44",
        "comment": "Glue Workflow only orchestrate crawlers and glue jobs",
        "selected_answer": "B"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Fri 03 May 2024 12:18",
        "comment": "For me it's B because I did not found a possibility how Glue can trigger/orchestrate EMR processes OOTB.\nBut with StepFunction there is a way: https://aws.amazon.com/blogs/big-data/orchestrate-amazon-emr-serverless-jobs-with-aws-step-functions/",
        "selected_answer": "B"
      },
      {
        "author": "Rpathak4",
        "date": "Sun 23 Mar 2025 10:09",
        "comment": "Why Not the Other Options?\n\nB. AWS Step Functions\tMore flexible but requires manual setup of states and transitions for Glue & EMR. Higher operational overhead than Glue Workflows.\nC. AWS Lambda\tLambda is not ideal for long-running ETL workflows. Best suited for lightweight data transformations or event-driven tasks.\nD. Amazon MWAA (Apache Airflow)\tMore control but requires cluster management and custom DAGs. Higher maintenance than Glue Workflows.",
        "selected_answer": "A"
      },
      {
        "author": "Palee",
        "date": "Tue 18 Mar 2025 11:44",
        "comment": "The company wants to improve the existing architecture so A cannot be the right choice",
        "selected_answer": "B"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 13:30",
        "comment": "it is interesting to choose A for minimum effort but only step functions can trigger the work both on EMR and on GLUE jobs",
        "selected_answer": "B"
      },
      {
        "author": "ttpro1995",
        "date": "Tue 24 Dec 2024 02:07",
        "comment": "We have both Glue job and EMR job, so we need Step Functions to connect those. \nAirflow can do it, but required more dev work.",
        "selected_answer": "B"
      },
      {
        "author": "Adrifersilva",
        "date": "Wed 02 Oct 2024 06:59",
        "comment": "glue workflows is part of the glue ecosystem so its provides seamless integration with minimal changes",
        "selected_answer": "A"
      },
      {
        "author": "Shanmahi",
        "date": "Sat 24 Aug 2024 20:47",
        "comment": "Glue workflows are managed services and best for considering least operational overhead.",
        "selected_answer": "A"
      },
      {
        "author": "V0811",
        "date": "Mon 05 Aug 2024 07:22",
        "comment": "AWS Glue Workflows are specifically designed for orchestrating ETL jobs in AWS Glue. They allow you to define and manage complex workflows that include multiple jobs and triggers, all within the AWS Glue environment.Integration: AWS Glue workflows seamlessly integrate with other AWS Glue components, making it easier to manage ETL processes without the need for external orchestration tools.Minimal Operational Overhead: Since AWS Glue is a fully managed service, using Glue workflows will reduce the operational overhead compared to managing separate orchestrators or building custom solutions.While D. Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is also a good choice for more complex orchestration, it may involve more management overhead compared to the more straightforward AWS Glue workflows. Thus, AWS Glue workflows provide the least operational overhead given the context of this scenario.",
        "selected_answer": "A"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 04 Jul 2024 05:48",
        "comment": "B - because AWS Glue can't trigger EMR",
        "selected_answer": "B"
      },
      {
        "author": "FunkyFresco",
        "date": "Sun 26 May 2024 14:29",
        "comment": "EMR in workflows , i dont think so",
        "selected_answer": "B"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 08:00",
        "comment": "There is no way for Glue Workflow to trigger EMR",
        "selected_answer": "B"
      },
      {
        "author": "acoshi",
        "date": "Mon 29 Apr 2024 18:11",
        "comment": "https://aws.amazon.com/blogs/big-data/orchestrate-an-etl-pipeline-using-aws-glue-workflows-triggers-and-crawlers-with-custom-classifiers/",
        "selected_answer": "A"
      },
      {
        "author": "lucas_rfsb",
        "date": "Sun 31 Mar 2024 22:47",
        "comment": "Since it seems to me that this pipeline is complex, with multiple workflows, I would go for Glue workflows.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132654-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 23 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 23,
    "question_text": "A company currently stores all of its data in Amazon S3 by using the S3 Standard storage class.\nA data engineer examined data access patterns to identify trends. During the first 6 months, most data files are accessed several times each day. Between 6 months and 2 years, most data files are accessed once or twice each month. After 2 years, data files are accessed only once or twice each year.\nThe data engineer needs to use an S3 Lifecycle policy to develop new data storage rules. The new storage solution must continue to provide high availability.\nWhich solution will meet these requirements in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years."
      },
      {
        "letter": "B",
        "text": "Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Flexible Retrieval after 2 years."
      },
      {
        "letter": "C",
        "text": "Transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years."
      },
      {
        "letter": "D",
        "text": "Transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 6 months. Transfer objects to S3 Glacier Deep Archive after 2 years."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "helpaws",
        "date": "Fri 20 Sep 2024 22:07",
        "comment": "\"S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\"\n\nSource: https://aws.amazon.com/s3/storage-classes/glacier/",
        "selected_answer": "B"
      },
      {
        "author": "WarPig666",
        "date": "Fri 13 Dec 2024 02:14",
        "comment": "Flexible retrieval will be higher cost than deep archive. If records only need to be retrieved once or twice a year, this doesn't mean they need to be instantly available.",
        "selected_answer": "C"
      },
      {
        "author": "Nickalodeon99",
        "date": "Tue 16 Sep 2025 22:18",
        "comment": "Question says: \n1) After 2 years, data files are accessed only once or twice each year.\n2) The new storage solution must continue to provide high availability.\n3) MOST cost-effective way?\n\n\nS3 Glacier Flexible Retrieval \n-For archive data that is accessed 1—2 times per year and is retrieved asynchronously (not occurring at the same time).\n-Retrieval option from 1 minute to 12 hours.\n-$0.0036 per GB / Mo\n\n-S3 Glacier Deep Archive \n-Data that may be accessed once or twice in a year.\n-Designed to deliver 99.99% availability\n-Retrieval time within 12 hours\n-$0.00099 per GB / Mo\n\nI think its Option C, because question is not asking for exact retrieval time but asking most cost effective which is (S3 Glacier Deep Archive).\n\nhttps://aws.amazon.com/s3/storage-classes/?nc=sn&amp;loc=3#topic-5\nhttps://aws.amazon.com/s3/pricing/",
        "selected_answer": "C"
      },
      {
        "author": "Tani0908",
        "date": "Mon 23 Jun 2025 14:58",
        "comment": "As they mention high availability it will be B",
        "selected_answer": "B"
      },
      {
        "author": "AM027",
        "date": "Fri 18 Apr 2025 10:11",
        "comment": "\"S3 Glacier Flexible Retrieval delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously\"",
        "selected_answer": "B"
      },
      {
        "author": "Rpathak4",
        "date": "Sun 23 Mar 2025 10:15",
        "comment": "Why Not the Other Options?\n\nA. S3 One Zone-IA → Glacier Flexible Retrieval\t❌ One Zone-IA is risky (data loss if the AZ fails). Glacier Flexible Retrieval is more expensive than Deep Archive.\n\nB. S3 Standard-IA → Glacier Flexible Retrieval\t❌ Glacier Flexible Retrieval is not the cheapest long-term storage. Deep Archive costs much less.\n\nD. S3 One Zone-IA → Glacier Deep Archive\t❌ One Zone-IA lacks high availability (single AZ failure = data loss). S3 Standard-IA is safer.",
        "selected_answer": "C"
      },
      {
        "author": "anonymous_learner_2",
        "date": "Tue 18 Feb 2025 21:27",
        "comment": "Glacier deep archive has the same availability as flexible retrieval and there's no retrieval time requirement so C is the most cost effective that meets the requirements.",
        "selected_answer": "C"
      },
      {
        "author": "luigiDDD",
        "date": "Thu 23 Jan 2025 00:37",
        "comment": "C is the most cost effective",
        "selected_answer": "C"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 13:43",
        "comment": "\"data files are accessed only once or twice each year\", this is \"S3 Glacier Flexible Retrieval\" definition",
        "selected_answer": "B"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 15:28",
        "comment": "Is it mentioned in question that Retrieval time is constraint, no, so, if any engineer need to access data, say May and November, so he/she can wait for 2-3 days to get data, as in the long run, they have an year to analyze the data so, deep archive will save costs only.",
        "selected_answer": "C"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 15:22",
        "comment": "This question was in Stephen Maarek Udemy practice questions too, here concern not given for extraction time so, just see cost friendlyness, thus, C over B",
        "selected_answer": "C"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Thu 19 Dec 2024 16:01",
        "comment": "deep archive doesn't make sense",
        "selected_answer": "B"
      },
      {
        "author": "Eleftheriia",
        "date": "Fri 06 Dec 2024 08:27",
        "comment": "For once or twice a year it is flexible retrieval.",
        "selected_answer": "B"
      },
      {
        "author": "jk15997",
        "date": "Thu 05 Dec 2024 21:11",
        "comment": "There is no requirement for the retrieval time.",
        "selected_answer": "C"
      },
      {
        "author": "altonh",
        "date": "Wed 04 Dec 2024 06:22",
        "comment": "There is no requirement for the retrieval time. So this is more cost-effective.",
        "selected_answer": "C"
      },
      {
        "author": "iamwatchingyoualways",
        "date": "Tue 26 Nov 2024 18:33",
        "comment": "No instant access is mentioned. Most Cost effective.",
        "selected_answer": "C"
      },
      {
        "author": "truongnguyen86",
        "date": "Sat 16 Nov 2024 14:01",
        "comment": "Option B is the correct answer because it balances cost-effectiveness and availability:\n\n    S3 Standard-IA offers cost savings for infrequently accessed data while maintaining high availability across multiple zones.\n    S3 Glacier Flexible Retrieval is a good balance for archiving with occasional access needs.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131711-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 24 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 24,
    "question_text": "A company maintains an Amazon Redshift provisioned cluster that the company uses for extract, transform, and load (ETL) operations to support critical analysis tasks. A sales team within the company maintains a Redshift cluster that the sales team uses for business intelligence (BI) tasks.\nThe sales team recently requested access to the data that is in the ETL Redshift cluster so the team can perform weekly summary analysis tasks. The sales team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.\nThe company needs a solution that will share the ETL cluster data with the sales team without interrupting the critical analysis tasks. The solution must minimize usage of the computing resources of the ETL cluster.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Set up the sales team BI cluster as a consumer of the ETL cluster by using Redshift data sharing."
      },
      {
        "letter": "B",
        "text": "Create materialized views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster."
      },
      {
        "letter": "C",
        "text": "Create database views based on the sales team's requirements. Grant the sales team direct access to the ETL cluster."
      },
      {
        "letter": "D",
        "text": "Unload a copy of the data from the ETL cluster to an Amazon S3 bucket every week. Create an Amazon Redshift Spectrum table based on the content of the ETL cluster."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "arvehisa",
        "date": "Sat 30 Mar 2024 14:33",
        "comment": "A: redshift data sharing:\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\nWith data sharing, you can securely and easily share live data across Amazon Redshift clusters.\nB: materialized view is only within 1 redshift cluster, across different tables",
        "selected_answer": "A"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 07:41",
        "comment": "Initially I would go with B but that definitely will use more resource.",
        "selected_answer": "A"
      },
      {
        "author": "San_Juan",
        "date": "Wed 28 Aug 2024 08:17",
        "comment": "\"The solution must minimize usage of the computing resources of the ETL cluster.\" That is key. You shouldn't use ETL cluster, so unload data to S3 and run queries in a separate Redshift Spectrum database. ETL cluster do nothing meanwhile.",
        "selected_answer": "D"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 08:14",
        "comment": "Typetical Redshift data sharing use case",
        "selected_answer": "A"
      },
      {
        "author": "d8945a1",
        "date": "Tue 07 May 2024 05:25",
        "comment": "Typical usecase of datasharing in Redshift. \n\nThe question mentions that - 'team needs to join data from the ETL cluster with data that is in the sales team's BI cluster.' This is possible with datashare.",
        "selected_answer": "A"
      },
      {
        "author": "lucas_rfsb",
        "date": "Mon 01 Apr 2024 01:03",
        "comment": "In my opinion using Redshift Data Sharing will consume less resources. 'D' envolves using a S3 bucket.",
        "selected_answer": "D"
      },
      {
        "author": "jasango",
        "date": "Thu 28 Mar 2024 16:30",
        "comment": "The spectrum table is accessed from the sales cluster with zero impact on the ETL cluster.",
        "selected_answer": "D"
      },
      {
        "author": "[Removed]",
        "date": "Sun 21 Jan 2024 02:47",
        "comment": "To share data between Redshift clusters and meet the requirements of sharing ETL cluster data with the sales team without interrupting critical analysis tasks and minimizing the usage of the ETL cluster's computing resources, Redshift Data Sharing is the way to go\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html\n\n\"Supporting different kinds of business-critical workloads – Use a central extract, transform, and load (ETL) cluster that shares data with multiple business intelligence (BI) or analytic clusters. This approach provides read workload isolation and chargeback for individual workloads. You can size and scale your individual workload compute according to the workload-specific requirements of price and performance\"",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131712-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 25 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 25,
    "question_text": "A data engineer needs to join data from multiple sources to perform a one-time analysis job. The data is stored in Amazon DynamoDB, Amazon RDS, Amazon Redshift, and Amazon S3.\nWhich solution will meet this requirement MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an Amazon EMR provisioned cluster to read from all sources. Use Apache Spark to join the data and perform the analysis."
      },
      {
        "letter": "B",
        "text": "Copy the data from DynamoDB, Amazon RDS, and Amazon Redshift into Amazon S3. Run Amazon Athena queries directly on the S3 files."
      },
      {
        "letter": "C",
        "text": "Use Amazon Athena Federated Query to join the data from all data sources."
      },
      {
        "letter": "D",
        "text": "Use Redshift Spectrum to query data from DynamoDB, Amazon RDS, and Amazon S3 directly from Redshift."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "lucas_rfsb",
        "date": "Tue 01 Oct 2024 01:37",
        "comment": "I would go for C because Federated Query is typical for this porpouse. Besides, we don't need to add/duplicate resources in S3. But I see that, becasuse Athena is more optimized for S3, it can be considered a tricky question, since there can be more trade-offs to consider, such as data governance that are easier if data is centralized in S3 in my opinion.",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 09 Dec 2024 04:46",
        "comment": "Serverless Processing: Athena is a serverless query service, meaning you only pay for the queries you run. This eliminates the need to provision and manage compute resources like in EMR clusters,\nmaking it ideal for one-time jobs.\nFederated Query Capability: Athena Federated Query allows you to directly query data from various sources like DynamoDB, RDS, Redshift, and S3 without physically moving the data. This eliminates data movement costs and simplifies the analysis process.\nReduced Cost for Large Datasets: Compared to copying data to S3, which can be expensive for large datasets, Athena Federated Query avoids unnecessary data movement, reducing overall costs.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 26 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 26,
    "question_text": "A company is planning to use a provisioned Amazon EMR cluster that runs Apache Spark jobs to perform big data analysis. The company requires high reliability. A big data team must follow best practices for running cost-optimized and long-running workloads on Amazon EMR. The team must find a solution that will maintain the company's current level of performance.\nWhich combination of resources will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use Hadoop Distributed File System (HDFS) as a persistent data store."
      },
      {
        "letter": "B",
        "text": "Use Amazon S3 as a persistent data store."
      },
      {
        "letter": "C",
        "text": "Use x86-based instances for core nodes and task nodes."
      },
      {
        "letter": "D",
        "text": "Use Graviton instances for core nodes and task nodes."
      },
      {
        "letter": "E",
        "text": "Use Spot Instances for all primary nodes."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "[Removed]",
        "date": "Sun 21 Jul 2024 02:03",
        "comment": "HDFS is not recommended for persistent storage because once a cluster is terminated, all HDFS data is lost. Also, long-running workloads can fill the disk space quickly. Thus, S3 is the best option since it's highly available, durable, and scalable.\n\nAWS Graviton-based instances cost up to 20% less than comparable x86-based Amazon\nEC2 instances: https://aws.amazon.com/ec2/graviton/",
        "selected_answer": "BD"
      },
      {
        "author": "sam_pre",
        "date": "Fri 28 Mar 2025 14:23",
        "comment": "Cost effective + high reliability > S3\nGravitation > Low cost",
        "selected_answer": "BD"
      },
      {
        "author": "ttpro1995",
        "date": "Tue 24 Dec 2024 02:12",
        "comment": "Rule of thumb: pick the AWS in-house solution provided for that service.\nGraviton is aws processor, and also EMRFS on S3.",
        "selected_answer": "BD"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 09 Dec 2024 04:51",
        "comment": "s3 no question.\nGraviton=> Cost-Effectiveness: Graviton instances are ARM-based instances specifically designed for cloud workloads.\nThey offer significant cost savings compared to x86-based instances while delivering comparable or better performance for many Apache Spark workloads.\nPerformance: Graviton instances are optimized for Spark workloads and can deliver the same level of performance as x86-based instances in many cases. Additionally, EMR offers performance-optimized versions of Spark built for Graviton instances.",
        "selected_answer": "BD"
      },
      {
        "author": "GiorgioGss",
        "date": "Wed 11 Sep 2024 07:53",
        "comment": "B and D.",
        "selected_answer": "BD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/133048-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 27 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 27,
    "question_text": "A company wants to implement real-time analytics capabilities. The company wants to use Amazon Kinesis Data Streams and Amazon Redshift to ingest and process streaming data at the rate of several gigabytes per second. The company wants to derive near real-time insights by using existing business intelligence (BI) and analytics tools.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Kinesis Data Streams to stage data in Amazon S3. Use the COPY command to load data from Amazon S3 directly into Amazon Redshift to make the data immediately available for real-time analysis."
      },
      {
        "letter": "B",
        "text": "Access the data from Kinesis Data Streams by using SQL queries. Create materialized views directly on top of the stream. Refresh the materialized views regularly to query the most recent stream data."
      },
      {
        "letter": "C",
        "text": "Create an external schema in Amazon Redshift to map the data from Kinesis Data Streams to an Amazon Redshift object. Create a materialized view to read data from the stream. Set the materialized view to auto refresh."
      },
      {
        "letter": "D",
        "text": "Connect Kinesis Data Streams to Amazon Kinesis Data Firehose. Use Kinesis Data Firehose to stage the data in Amazon S3. Use the COPY command to load the data from Amazon S3 to a table in Amazon Redshift."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "blackgamer",
        "date": "Fri 29 Mar 2024 15:50",
        "comment": "The answer is C. It can provide near real-time insight analysis. Refer the article from AWS - https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/",
        "selected_answer": "C"
      },
      {
        "author": "helpaws",
        "date": "Sat 16 Mar 2024 03:15",
        "comment": "Key word here is near real-time. If it's involve S3 and COPY, it's not gonna be near real-time",
        "selected_answer": "C"
      },
      {
        "author": "melligeri",
        "date": "Thu 27 Mar 2025 09:33",
        "comment": "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/#:~:text=Before%20the%20launch,the%20data%20stream.",
        "selected_answer": "C"
      },
      {
        "author": "Rpathak4",
        "date": "Sun 23 Mar 2025 10:31",
        "comment": "✅ Use Kinesis Data Firehose to load data into Redshift via S3 for the simplest and most scalable solution.\n✅ Firehose automatically batches, transforms, and loads data with no manual intervention required.\n✅ Achieves near real-time analytics with minimal operational effort.",
        "selected_answer": "D"
      },
      {
        "author": "MephiboshethGumani",
        "date": "Sat 15 Mar 2025 15:46",
        "comment": "Creating an external schema and using materialized views directly on top of Kinesis Data Streams is also not an ideal choice because this approach can add complexity and doesn't leverage fully managed solutions like Kinesis Data Firehose. The manual management of data refresh rates adds operational overhead.",
        "selected_answer": "D"
      },
      {
        "author": "Eltanany",
        "date": "Thu 13 Mar 2025 11:21",
        "comment": "Refer to the article from AWS - https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/",
        "selected_answer": "C"
      },
      {
        "author": "jesusmoh",
        "date": "Tue 04 Mar 2025 04:25",
        "comment": "option D provides a streamlined, efficient, and low-overhead approach to achieving real-time analytics with the specified technologies.",
        "selected_answer": "D"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 14:04",
        "comment": "A: Kinesis Data Streams to stage data in Amazon S3. not really easy, \nB: sql directly to Kinesis Data Streams : functionality not exist\nC : external schema from redshift to Kinesis Data Streams : functionality not exist\nD :  near real-time = Kinesis Data Firehose",
        "selected_answer": "D"
      },
      {
        "author": "subbie",
        "date": "Wed 08 Jan 2025 07:13",
        "comment": "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/",
        "selected_answer": "C"
      },
      {
        "author": "subbie",
        "date": "Wed 08 Jan 2025 07:12",
        "comment": "https://aws.amazon.com/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/",
        "selected_answer": "B"
      },
      {
        "author": "haby",
        "date": "Sun 22 Dec 2024 15:53",
        "comment": "A for me\nC - Redshift does not natively support direct mapping to Kinesis Data Streams. Some extra configs are needed.\nD - There will be a 60s latency when using Firehose, so it's \"Near\" real time not real time.",
        "selected_answer": "A"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Thu 19 Dec 2024 16:12",
        "comment": "Redshift does not natively support direct mapping to Kinesis Data Streams. Materialized views cannot directly query streaming data from Kinesis.",
        "selected_answer": "D"
      },
      {
        "author": "altonh",
        "date": "Wed 04 Dec 2024 10:25",
        "comment": "See https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion-getting-started.html",
        "selected_answer": "C"
      },
      {
        "author": "Asen_Cat",
        "date": "Wed 06 Nov 2024 10:18",
        "comment": "D could be the most standard way to handle this case. How to use C to implement it is questionable for me.",
        "selected_answer": "D"
      },
      {
        "author": "heavenlypearl",
        "date": "Wed 06 Nov 2024 09:58",
        "comment": "Amazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created with or altered to have the autorefresh option. Amazon Redshift autorefreshes materialized views as soon as possible after base tables changes.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html",
        "selected_answer": "C"
      },
      {
        "author": "Shatheesh",
        "date": "Thu 24 Oct 2024 18:26",
        "comment": "Kinesis Data Streams , option D using Kinesis Data Firehose is a fully managed service that automatically handles the ingestion of data",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132734-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 28 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 28,
    "question_text": "A company uses an Amazon QuickSight dashboard to monitor usage of one of the company's applications. The company uses AWS Glue jobs to process data for the dashboard. The company stores the data in a single Amazon S3 bucket. The company adds new data every day.\nA data engineer discovers that dashboard queries are becoming slower over time. The data engineer determines that the root cause of the slowing queries is long-running AWS Glue jobs.\nWhich actions should the data engineer take to improve the performance of the AWS Glue jobs? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Partition the data that is in the S3 bucket. Organize the data by year, month, and day."
      },
      {
        "letter": "B",
        "text": "Increase the AWS Glue instance size by scaling up the worker type."
      },
      {
        "letter": "C",
        "text": "Convert the AWS Glue schema to the DynamicFrame schema class."
      },
      {
        "letter": "D",
        "text": "Adjust AWS Glue job scheduling frequency so the jobs run half as many times each day."
      },
      {
        "letter": "E",
        "text": "Modify the IAM role that grants access to AWS glue to grant access to all S3 features."
      }
    ],
    "correct_answer": "AB",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Sat 03 Aug 2024 09:14",
        "comment": "A. Partition the data that is in the S3 bucket. Organize the data by year, month, and day.\n\n\t•\tPartitioning data in Amazon S3 can significantly improve query performance. By organizing the data by year, month, and day, AWS Glue and Amazon QuickSight can scan only the relevant partitions of data, which reduces the amount of data read and processed. This approach is particularly effective for time-series data, where queries often target specific time ranges.\n\nB. Increase the AWS Glue instance size by scaling up the worker type.\n\n\t•\tScaling up the worker type can provide more computational resources to the AWS Glue jobs, enabling them to process data faster. This can be especially beneficial when dealing with large datasets or complex transformations. It’s important to monitor the performance improvements and cost implications of scaling up.",
        "selected_answer": "AB"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132773-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 29 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 29,
    "question_text": "A data engineer needs to use AWS Step Functions to design an orchestration workflow. The workflow must parallel process a large collection of data files and apply a specific transformation to each file.\nWhich Step Functions state should the data engineer use to meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Parallel state"
      },
      {
        "letter": "B",
        "text": "Choice state"
      },
      {
        "letter": "C",
        "text": "Map state"
      },
      {
        "letter": "D",
        "text": "Wait state"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GabrielSGoncalves",
        "date": "Thu 29 Aug 2024 03:40",
        "comment": "Clearly is mapping state",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 04:11",
        "comment": "The Map state allows you to define a single execution path for processing a collection of data items in parallel.\nThis aligns perfectly with the data engineer's requirement of parallel processing a large collection of data files",
        "selected_answer": "C"
      },
      {
        "author": "FunkyFresco",
        "date": "Wed 05 Jun 2024 02:59",
        "comment": "to execute in parallel",
        "selected_answer": "C"
      },
      {
        "author": "sveni1502",
        "date": "Thu 23 May 2024 02:34",
        "comment": "C is Correct\nTo meet the requirement of parallel processing a large collection of data files and applying a specific transformation to each file, the data engineer should use the Map state in AWS Step Functions.\nThe Map state is specifically designed to run a set of tasks in parallel for each element in a collection or array. Each element (in this case, each data file) is processed independently and in parallel, allowing the workflow to take advantage of parallel processing.",
        "selected_answer": "C"
      },
      {
        "author": "lucas_rfsb",
        "date": "Mon 01 Apr 2024 03:09",
        "comment": "C, Map state is correct",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Sun 04 Feb 2024 13:46",
        "comment": "The Map state is specifically designed for processing a collection of items (like data files) in parallel. It allows you to apply a transformation or a set of steps to each item in the input array independently.\nThe Map state automatically iterates over each item in the array and performs the defined steps. This makes it ideal for scenarios where you need to process a large number of files in a similar manner, as in your requirement.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132774-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 30 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 30,
    "question_text": "A company is migrating a legacy application to an Amazon S3 based data lake. A data engineer reviewed data that is associated with the legacy application. The data engineer found that the legacy data contained some duplicate information.\nThe data engineer must identify and remove duplicate information from the legacy application data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Write a custom extract, transform, and load (ETL) job in Python. Use the DataFrame.drop_duplicates() function by importing the Pandas library to perform data deduplication."
      },
      {
        "letter": "B",
        "text": "Write an AWS Glue extract, transform, and load (ETL) job. Use the FindMatches machine learning (ML) transform to transform the data to perform data deduplication."
      },
      {
        "letter": "C",
        "text": "Write a custom extract, transform, and load (ETL) job in Python. Import the Python dedupe library. Use the dedupe library to perform data deduplication."
      },
      {
        "letter": "D",
        "text": "Write an AWS Glue extract, transform, and load (ETL) job. Import the Python dedupe library. Use the dedupe library to perform data deduplication."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Sun 04 Feb 2024 13:56",
        "comment": "Option B, writing an AWS Glue ETL job with the FindMatches ML transform, is likely to meet the requirements with the least operational overhead. This solution leverages a managed service (AWS Glue) and incorporates a built-in ML transform specifically designed for deduplication, thus minimizing the need for manual setup, maintenance, and machine learning expertise.",
        "selected_answer": "B"
      },
      {
        "author": "_JP_",
        "date": "Wed 18 Dec 2024 00:56",
        "comment": "I disagree with B. That option requires additional effort just to train the ML model with labeled data. Option A is as simple as to use the robust pandas library",
        "selected_answer": "A"
      },
      {
        "author": "V0811",
        "date": "Mon 05 Aug 2024 07:39",
        "comment": "100 % B",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 09:08",
        "comment": "B. https://docs.aws.amazon.com/glue/latest/dg/machine-learning.html\n\"Find matches\nFinds duplicate records in the source data. You teach this machine learning transform by labeling example datasets to indicate which rows match. The machine learning transform learns which rows should be matches the more you teach it with example labeled data.\"",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132737-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 31 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 31,
    "question_text": "A company is building an analytics solution. The solution uses Amazon S3 for data lake storage and Amazon Redshift for a data warehouse. The company wants to use Amazon Redshift Spectrum to query the data that is in Amazon S3.\nWhich actions will provide the FASTEST queries? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use gzip compression to compress individual files to sizes that are between 1 GB and 5 GB."
      },
      {
        "letter": "B",
        "text": "Use a columnar storage file format."
      },
      {
        "letter": "C",
        "text": "Partition the data based on the most common query predicates."
      },
      {
        "letter": "D",
        "text": "Split the data into files that are less than 10 KB."
      },
      {
        "letter": "E",
        "text": "Use file formats that are not splittable."
      }
    ],
    "correct_answer": "BC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 09:12",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html",
        "selected_answer": "BC"
      },
      {
        "author": "rralucard_",
        "date": "Sat 03 Feb 2024 15:05",
        "comment": "B. Use a columnar storage file format: This is an excellent approach. Columnar storage formats like Parquet and ORC are highly recommended for use with Redshift Spectrum. They store data in columns, which allows Spectrum to scan only the needed columns for a query, significantly improving query performance and reducing the amount of data scanned.\n\nC. Partition the data based on the most common query predicates: Partitioning data in S3 based on commonly used query predicates (like date, region, etc.) allows Redshift Spectrum to skip large portions of data that are irrelevant to a particular query. This can lead to substantial performance improvements, especially for large datasets.",
        "selected_answer": "BC"
      },
      {
        "author": "andrologin",
        "date": "Wed 10 Jul 2024 05:48",
        "comment": "Partioning helps filter the data and columnar storage is optimised for analytical (OLAP) queries",
        "selected_answer": "BC"
      },
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 17:34",
        "comment": "Redshift Spectrum is optimized for querying data stored in columnar formats like Parquet or ORC.\n These formats store each data column separately, allowing Redshift Spectrum to only scan the relevant columns for a specific query, significantly improving performance compared to row-oriented formats\nPartitioning organizes data files in S3 based on specific column values (e.g., date,\n region). When your queries filter or join data based on these partitioning columns (common query predicates), Redshift Spectrum can quickly locate the relevant data files, minimizing the amount of data scanned and accelerating query execution",
        "selected_answer": "BC"
      },
      {
        "author": "d8945a1",
        "date": "Tue 07 May 2024 05:52",
        "comment": "https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/",
        "selected_answer": "BC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132738-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 32 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 32,
    "question_text": "A company uses Amazon RDS to store transactional data. The company runs an RDS DB instance in a private subnet. A developer wrote an AWS Lambda function with default settings to insert, update, or delete data in the DB instance.\nThe developer needs to give the Lambda function the ability to connect to the DB instance privately without using the public internet.\nWhich combination of steps will meet this requirement with the LEAST operational overhead? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Turn on the public access setting for the DB instance."
      },
      {
        "letter": "B",
        "text": "Update the security group of the DB instance to allow only Lambda function invocations on the database port."
      },
      {
        "letter": "C",
        "text": "Configure the Lambda function to run in the same subnet that the DB instance uses."
      },
      {
        "letter": "D",
        "text": "Attach the same security group to the Lambda function and the DB instance. Include a self-referencing rule that allows access through the database port."
      },
      {
        "letter": "E",
        "text": "Update the network ACL of the private subnet to include a self-referencing rule that allows access through the database port."
      }
    ],
    "correct_answer": "CD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Alagong",
        "date": "Fri 29 Mar 2024 03:40",
        "comment": "This solution only modifies the inbound rules of the security group of the DB instance, but it does not modify the outbound rules of the security group of the Lambda function. Additionally, this solution does not facilitate a private connection from the Lambda function to the DB instance, hence, the Lambda function would still need to use the public internet to access the DB instance. Therefore, this option does not fulfill the requirements.",
        "selected_answer": "CD"
      },
      {
        "author": "rr01",
        "date": "Wed 29 Jan 2025 15:42",
        "comment": "I would go with B & D. As C would have operational overhead in my opinion.",
        "selected_answer": "BD"
      },
      {
        "author": "altonh",
        "date": "Wed 04 Dec 2024 12:51",
        "comment": "D is wrong. It is a bad security practice for a DB to share SG with the client.\nC is correct compared to the other opinions (A & E).",
        "selected_answer": "BC"
      },
      {
        "author": "proserv",
        "date": "Mon 07 Oct 2024 10:55",
        "comment": "B & D \nC is wrong\nWhile you want the Lambda function to access the RDS instance privately, it does not need to run in the same subnet. As long as both are in the same VPC, the Lambda function can connect.",
        "selected_answer": "BD"
      },
      {
        "author": "tgv",
        "date": "Sun 02 Jun 2024 07:56",
        "comment": "I will go with C and D on this one, because in my opinion B is not correctly phrased. \n\n The correct way to phrase it would be something like:\n\nUpdate the security group of the RDS instance to allow inbound traffic on the database port (3306) only from the security group associated with the Lambda function.",
        "selected_answer": "CD"
      },
      {
        "author": "Snape",
        "date": "Mon 29 Apr 2024 02:23",
        "comment": "bbb ddd",
        "selected_answer": "BD"
      },
      {
        "author": "lucas_rfsb",
        "date": "Wed 03 Apr 2024 15:36",
        "comment": "I would go with CD, since it's less operational effort, in my opinion",
        "selected_answer": "CD"
      },
      {
        "author": "arvehisa",
        "date": "Sun 31 Mar 2024 03:04",
        "comment": "B: need update security group. and there there may be other application need to access db except for lambda function\nD: it works and reuse security group which has less operational overhead",
        "selected_answer": "CD"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 09:20",
        "comment": "When you want Lambda to \"privately\" connect to a resource (RDS in this case) that sits inside a VPC, then you deploy Lambda inside VPC. = C\nThen you attach a proper IAM role to lambda. \nThen, to be more secure you open the RDS security group only on the specific port:\nMySQL/Aurora MySQL: 3306\nSQL Server: 1433\nPostgreSQL: 5432\nOracle: 1521",
        "selected_answer": "BC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132630-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 33 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 33,
    "question_text": "A company has a frontend ReactJS website that uses Amazon API Gateway to invoke REST APIs. The APIs perform the functionality of the website. A data engineer needs to write a Python script that can be occasionally invoked through API Gateway. The code must return results to API Gateway.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Deploy a custom Python script on an Amazon Elastic Container Service (Amazon ECS) cluster."
      },
      {
        "letter": "B",
        "text": "Create an AWS Lambda Python function with provisioned concurrency."
      },
      {
        "letter": "C",
        "text": "Deploy a custom Python script that can integrate with API Gateway on Amazon Elastic Kubernetes Service (Amazon EKS)."
      },
      {
        "letter": "D",
        "text": "Create an AWS Lambda function. Ensure that the function is warm by scheduling an Amazon EventBridge rule to invoke the Lambda function every 5 minutes by using mock events."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Sun 09 Jun 2024 17:49",
        "comment": "B and D are both ok. Still, since it says LEAST operational overhead, then keep it simple. B then.",
        "selected_answer": "B"
      },
      {
        "author": "Mperu08",
        "date": "Sat 12 Apr 2025 22:18",
        "comment": "The solution with the least operational overhead is D. Create an AWS Lambda function with an EventBridge rule to keep it warm. Lambda handles the infrastructure management automatically, and the warm-up strategy addresses potential cold start issues while maintaining minimal operational requirements compared to container-based solutions.",
        "selected_answer": "D"
      },
      {
        "author": "MephiboshethGumani",
        "date": "Sat 15 Mar 2025 16:18",
        "comment": "D. Create an AWS Lambda function. Ensure that the function is warm by scheduling an Amazon EventBridge rule to invoke the Lambda function every 5 minutes by using mock events.",
        "selected_answer": "D"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 30 Apr 2024 11:23",
        "comment": "B - simple and clear",
        "selected_answer": "B"
      },
      {
        "author": "lucas_rfsb",
        "date": "Mon 01 Apr 2024 13:01",
        "comment": "I would go in B",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 09:30",
        "comment": "Although D seems a good choice but the questions asks for \"LEAST operational overhead\" will result in B",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Thu 01 Feb 2024 07:31",
        "comment": "B.\nAWS Lambda functions can be easily integrated with Amazon API Gateway to create RESTful APIs. This integration allows API Gateway to directly invoke the Lambda function when the API endpoint is hit.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/133056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 34 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 34,
    "question_text": "A company has a production AWS account that runs company workloads. The company's security team created a security AWS account to store and analyze security logs from the production AWS account. The security logs in the production AWS account are stored in Amazon CloudWatch Logs.\nThe company needs to use Amazon Kinesis Data Streams to deliver the security logs to the security AWS account.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a destination data stream in the production AWS account. In the security AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the production AWS account."
      },
      {
        "letter": "B",
        "text": "Create a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the security AWS account."
      },
      {
        "letter": "C",
        "text": "Create a destination data stream in the production AWS account. In the production AWS account, create an IAM role that has cross-account permissions to Kinesis Data Streams in the security AWS account."
      },
      {
        "letter": "D",
        "text": "Create a destination data stream in the security AWS account. Create an IAM role and a trust policy to grant CloudWatch Logs the permission to put data into the stream. Create a subscription filter in the production AWS account."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 00:15",
        "comment": "Cross-Account Delivery: Kinesis Data Streams in the security account ensures the logs reside in the designated security-focused environment.\nCloudWatch Logs Integration: Granting CloudWatch Logs permissions to put records into the Kinesis Data Stream directly establishes a streamlined and secure data flow from the production account.\nFiltering Controls: The subscription filter in the production account provides precise control over which log events are sent to the security account.",
        "selected_answer": "D"
      },
      {
        "author": "Salam9",
        "date": "Fri 24 Jan 2025 21:11",
        "comment": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters-AccountLevel.html#DestinationKinesisExample-AccountLevel",
        "selected_answer": "D"
      },
      {
        "author": "GiorgioGss",
        "date": "Wed 11 Sep 2024 08:36",
        "comment": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions-Kinesis.html",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131705-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 35 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 35,
    "question_text": "A company uses Amazon S3 to store semi-structured data in a transactional data lake. Some of the data files are small, but other data files are tens of terabytes.\nA data engineer must perform a change data capture (CDC) operation to identify changed data from the data source. The data source sends a full snapshot as a JSON file every day and ingests the changed data into the data lake.\nWhich solution will capture the changed data MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Lambda function to identify the changes between the previous data and the current data. Configure the Lambda function to ingest the changes into the data lake."
      },
      {
        "letter": "B",
        "text": "Ingest the data into Amazon RDS for MySQL. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake."
      },
      {
        "letter": "C",
        "text": "Use an open source data lake format to merge the data source with the S3 data lake to insert the new data and update the existing data."
      },
      {
        "letter": "D",
        "text": "Ingest the data into an Amazon Aurora MySQL DB instance that runs Aurora Serverless. Use AWS Database Migration Service (AWS DMS) to write the changed data to the data lake."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Mon 11 Mar 2024 09:42",
        "comment": "https://aws.amazon.com/blogs/big-data/implement-a-cdc-based-upsert-in-a-data-lake-using-apache-iceberg-and-aws-glue/",
        "selected_answer": "C"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 14:37",
        "comment": "Generally, AWS questions never give preference to the others solution than an AWS service so even if C could be better the answer is A",
        "selected_answer": "A"
      },
      {
        "author": "FunkyFresco",
        "date": "Sun 26 May 2024 14:50",
        "comment": "Ill go with Delta or something like that. is C",
        "selected_answer": "C"
      },
      {
        "author": "[Removed]",
        "date": "Sun 21 Jan 2024 02:13",
        "comment": "This is a tricky one. Although option A seems like the best choice since it uses an AWS service, I believe using Delta/Iceberg APIs would be easier than writing custom code on Lambda",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/131708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 36 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 36,
    "question_text": "A data engineer runs Amazon Athena queries on data that is in an Amazon S3 bucket. The Athena queries use AWS Glue Data Catalog as a metadata table.\nThe data engineer notices that the Athena query plans are experiencing a performance bottleneck. The data engineer determines that the cause of the performance bottleneck is the large number of partitions that are in the S3 bucket. The data engineer must resolve the performance bottleneck and reduce Athena query planning time.\nWhich solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Glue partition index. Enable partition filtering."
      },
      {
        "letter": "B",
        "text": "Bucket the data based on a column that the data have in common in a WHERE clause of the user query."
      },
      {
        "letter": "C",
        "text": "Use Athena partition projection based on the S3 bucket prefix."
      },
      {
        "letter": "D",
        "text": "Transform the data that is in the S3 bucket to Apache Parquet format."
      },
      {
        "letter": "E",
        "text": "Use the Amazon EMR S3DistCP utility to combine smaller objects in the S3 bucket into larger objects."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Thu 01 Aug 2024 06:55",
        "comment": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/\nOptimizing Partition Processing using partition projection\nProcessing partition information can be a bottleneck for Athena queries when you have a very large number of partitions and aren’t using AWS Glue partition indexing. You can use partition projection in Athena to speed up query processing of highly partitioned tables and automate partition management. Partition projection helps minimize this overhead by allowing you to query partitions by calculating partition information rather than retrieving it from a metastore. It eliminates the need to add partitions’ metadata to the AWS Glue table.",
        "selected_answer": "AC"
      },
      {
        "author": "Mahidbdwh",
        "date": "Wed 12 Feb 2025 06:16",
        "comment": "Bucketing not address the problem of having a large number of partitions in the metadata, which is the root cause of the query planning bottleneck.\nConverting to a columnar format like Apache Parquet will not directly reduce the overhead associated with managing a large number of partitions.\nCombining small objects will not mitigate the planning overhead that comes from a large number of partitions in the data catalog. Hence A and C",
        "selected_answer": "AC"
      },
      {
        "author": "SMALLAM",
        "date": "Sun 12 Jan 2025 21:48",
        "comment": "https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
        "selected_answer": "AE"
      },
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 02:13",
        "comment": "Creating an AWS Glue partition index and enabling partition filtering can significantly improve query performance when dealing with large datasets with many partitions. The partition index allows Athena to quickly identify the relevant partitions for a query, reducing the time spent scanning unnecessary data. Partition filtering further optimizes the query by only scanning the partitions that match the filter conditions.\nAthena partition projection based on the S3 bucket prefix is another effective technique to improve query performance. By leveraging the bucket prefix structure, Athena can prune partitions that are not relevant to the query, reducing the amount of data that needs to be scanned and processed. This approach is particularly useful when the data is organized in a hierarchical structure within the S3 bucket.",
        "selected_answer": "AC"
      },
      {
        "author": "VerRi",
        "date": "Tue 19 Nov 2024 14:40",
        "comment": "D is not correct because the issue is related to partitioning.",
        "selected_answer": "AC"
      },
      {
        "author": "HunkyBunky",
        "date": "Fri 01 Nov 2024 06:51",
        "comment": "I guess A / C, beucase we faced with - query plans performance bottleneck, so indexing should be improved",
        "selected_answer": "AC"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 00:27",
        "comment": "A. Create an AWS Glue partition index. Enable partition filtering.\nTargeted Optimization: Partition indexes within the Glue Data Catalog help Athena efficiently identify the relevant partitions, significantly reducing query planning time. Partition filtering further refines the search during query execution.\nD. Transform the data that is in the S3 bucket to Apache Parquet format.\nEfficient Columnar Format: Parquet's columnar storage and built-in metadata often allow Athena to skip over large portions of data irrelevant to the query, leading to faster query planning and execution.",
        "selected_answer": "AD"
      },
      {
        "author": "fceb2c1",
        "date": "Tue 24 Sep 2024 05:18",
        "comment": "Keyword: Athena query planning time\n\nSee explanation in the link:\nhttps://www.myexamcollection.com/Data-Engineer-Associate-vce-questions.htm\n\nB & D are related to analytical queries performance, not about \"query planning\" performance.",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132739-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 37 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 37,
    "question_text": "A data engineer must manage the ingestion of real-time streaming data into AWS. The data engineer wants to perform real-time analytics on the incoming streaming data by using time-based aggregations over a window of up to 30 minutes. The data engineer needs a solution that is highly fault tolerant.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Lambda function that includes both the business and the analytics logic to perform time-based aggregations over a window of up to 30 minutes for the data in Amazon Kinesis Data Streams."
      },
      {
        "letter": "B",
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data that might occasionally contain duplicates by using multiple types of aggregations."
      },
      {
        "letter": "C",
        "text": "Use an AWS Lambda function that includes both the business and the analytics logic to perform aggregations for a tumbling window of up to 30 minutes, based on the event timestamp."
      },
      {
        "letter": "D",
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to analyze the data by using multiple types of aggregations to perform time-based analytics over a window of up to 30 minutes."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Sat 03 Feb 2024 15:40",
        "comment": "D. Amazon Managed Service for Apache Flink for Time-Based Analytics over 30 Minutes: This option correctly identifies the use of Amazon Managed Service for Apache Flink for performing time-based analytics over a window of up to 30 minutes. Apache Flink is adept at handling such scenarios, providing capabilities for complex event processing, time-windowed aggregations, and maintaining state over time. This option would offer high fault tolerance and minimal operational overhead due to the managed nature of the service.",
        "selected_answer": "D"
      },
      {
        "author": "div_div",
        "date": "Wed 22 Jan 2025 07:47",
        "comment": "Lambda can not be used because it max processing limit of time is 15 min and remaining two option related to flink and using flink we can perfrom time-series and window size aggregation",
        "selected_answer": "D"
      },
      {
        "author": "Just_Ninja",
        "date": "Thu 16 May 2024 15:12",
        "comment": "Show the Docs",
        "selected_answer": "D"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Mon 06 May 2024 07:54",
        "comment": "https://docs.aws.amazon.com/managed-flink/latest/java/how-operators.html#how-operators-agg",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132762-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 38 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 38,
    "question_text": "A company is planning to upgrade its Amazon Elastic Block Store (Amazon EBS) General Purpose SSD storage from gp2 to gp3. The company wants to prevent any interruptions in its Amazon EC2 instances that will cause data loss during the migration to the upgraded storage.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create snapshots of the gp2 volumes. Create new gp3 volumes from the snapshots. Attach the new gp3 volumes to the EC2 instances."
      },
      {
        "letter": "B",
        "text": "Create new gp3 volumes. Gradually transfer the data to the new gp3 volumes. When the transfer is complete, mount the new gp3 volumes to the EC2 instances to replace the gp2 volumes."
      },
      {
        "letter": "C",
        "text": "Change the volume type of the existing gp2 volumes to gp3. Enter new values for volume size, IOPS, and throughput."
      },
      {
        "letter": "D",
        "text": "Use AWS DataSync to create new gp3 volumes. Transfer the data from the original gp2 volumes to the new gp3 volumes."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 07:47",
        "comment": "https://aws.amazon.com/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/",
        "selected_answer": "C"
      },
      {
        "author": "fceb2c1",
        "date": "Tue 24 Sep 2024 05:24",
        "comment": "Option C: Check section under \"To modify an Amazon EBS volume using the AWS Management Console“ in GiorgioGss's link\nAmazon EBS Elastic Volumes enable you to modify your volume type from gp2 to gp3 without detaching volumes or restarting instances (requirements for modification), which means that there are no interruptions to your applications during modification.",
        "selected_answer": "C"
      },
      {
        "author": "lcsantos99",
        "date": "Sun 26 Jan 2025 21:30",
        "comment": "the correct answer is C\n\nhttps://aws.amazon.com/pt/blogs/storage/migrate-your-amazon-ebs-volumes-from-gp2-to-gp3-and-save-up-to-20-on-costs/",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Sun 04 Aug 2024 04:51",
        "comment": "Option C is the most straightforward and efficient approach to upgrading from gp2 to gp3 EBS volumes, providing an in-place upgrade path with minimal operational overhead and no interruption in service.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132742-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 39 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 39,
    "question_text": "A company is migrating its database servers from Amazon EC2 instances that run Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. The company's analytics team must export large data elements every day until the migration is complete. The data elements are the result of SQL joins across multiple tables. The data must be in Apache Parquet format. The analytics team must store the data in Amazon S3.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day."
      },
      {
        "letter": "B",
        "text": "Schedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet."
      },
      {
        "letter": "C",
        "text": "Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day."
      },
      {
        "letter": "D",
        "text": "Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 05:33",
        "comment": "Leveraging SQL Views: Creating a view on the source database simplifies the data extraction process and keeps your SQL logic centralized.\nGlue Crawler Efficiency: Using a Glue crawler to automatically discover and catalog the view's metadata reduces manual setup.\nGlue Job for ETL: A dedicated Glue job is well-suited for the data transformation (to Parquet) and loading into S3. Glue jobs offer built-in scheduling capabilities.\nOperational Efficiency: This approach minimizes custom code and leverages native AWS services for data movement and cataloging.",
        "selected_answer": "C"
      },
      {
        "author": "taka5094",
        "date": "Tue 19 Mar 2024 02:35",
        "comment": "Choice A) is almost the same approach, but it doesn't use the AWS Glue crawler, so have to manage the view's metadata manually.",
        "selected_answer": "C"
      },
      {
        "author": "Tester_TKK",
        "date": "Sat 19 Apr 2025 18:01",
        "comment": "Option C in incorrect because it adds a Glue crawler to read the view, which is redundant if the schema is already defined in the view",
        "selected_answer": "A"
      },
      {
        "author": "Tester_TKK",
        "date": "Sat 19 Apr 2025 17:59",
        "comment": "Crawler not needed as the schema is already in the view",
        "selected_answer": "A"
      },
      {
        "author": "Mperu08",
        "date": "Sat 12 Apr 2025 21:44",
        "comment": "Uses AWS Glue, a serverless ETL service optimized for large-scale data processing and Parquet output. The view simplifies query logic, and scheduling is straightforward. No EC2 dependency, minimal maintenance, and distributed processing ensure efficiency.",
        "selected_answer": "A"
      },
      {
        "author": "Eltanany",
        "date": "Mon 24 Mar 2025 08:52",
        "comment": "I'll go with A",
        "selected_answer": "A"
      },
      {
        "author": "Certified101",
        "date": "Thu 13 Feb 2025 11:14",
        "comment": "A is correct - no need for crawler",
        "selected_answer": "A"
      },
      {
        "author": "plutonash",
        "date": "Sun 12 Jan 2025 14:56",
        "comment": "the scrawler is not necessary, use GLUE job to read data from sql server and transfert to S3 with Apache Parquet format is enough.",
        "selected_answer": "A"
      },
      {
        "author": "mtrianac",
        "date": "Wed 04 Dec 2024 15:22",
        "comment": "No, in this case, using an AWS Glue Crawler is not necessary. The schema is already defined in the SQL Server database, as the created view contains the required structure (columns and data types). AWS Glue can directly connect to the database via JDBC, extract the data, transform it into Parquet format, and store it in S3 without additional steps.\n\nA crawler is useful if you're working with data that doesn't have a predefined schema (e.g., files in S3) or if you need the data to be cataloged for services like Amazon Athena. However, for this ETL flow, using just a Glue Job simplifies the process and reduces operational complexity.",
        "selected_answer": "A"
      },
      {
        "author": "michele_scar",
        "date": "Tue 05 Nov 2024 11:32",
        "comment": "Glue crawler is useless because the schema is already in place with a SQL database",
        "selected_answer": "A"
      },
      {
        "author": "leonardoFelipe",
        "date": "Sun 03 Nov 2024 21:43",
        "comment": "Usually, views aren't true objects in a SGBD, they're just a \"nickname\" for a specific query string, different of Materialized Views. So, my questions is: can glue crawler understand their metadata?\nI'd go with A",
        "selected_answer": "A"
      },
      {
        "author": "bakarys",
        "date": "Mon 01 Jul 2024 15:08",
        "comment": "Option A involves creating a view in the EC2 instance-based SQL Server databases that contains the required data elements. An AWS Glue job is then created to select the data directly from the view and transfer the data in Parquet format to an S3 bucket. This job is scheduled to run every day. This approach is operationally efficient as it leverages managed services (AWS Glue) and does not require additional transformation steps.\n\nOption D involves creating an AWS Lambda function that queries the EC2 instance-based databases using JDBC. The Lambda function is configured to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. This approach could work, but managing and scheduling Lambda functions could add operational overhead compared to using managed services like AWS Glue.",
        "selected_answer": "A"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 08:56",
        "comment": "Just beacuse it decouples the whole architecture I will go with C",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Sun 04 Feb 2024 14:18",
        "comment": "Option A (Creating a view in the EC2 instance-based SQL Server databases and creating an AWS Glue job that selects data from the view, transfers it in Parquet format to S3, and schedules the job to run every day) seems to be the most operationally efficient solution. It leverages AWS Glue’s ETL capabilities for direct data extraction and transformation, minimizes manual steps, and effectively automates the process.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132660-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 40 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 40,
    "question_text": "A data engineering team is using an Amazon Redshift data warehouse for operational reporting. The team wants to prevent performance issues that might result from long- running queries. A data engineer must choose a system table in Amazon Redshift to record anomalies when a query optimizer identifies conditions that might indicate performance issues.\nWhich table views should the data engineer use to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "STL_USAGE_CONTROL"
      },
      {
        "letter": "B",
        "text": "STL_ALERT_EVENT_LOG"
      },
      {
        "letter": "C",
        "text": "STL_QUERY_METRICS"
      },
      {
        "letter": "D",
        "text": "STL_PLAN_INFO"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 07:58",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html",
        "selected_answer": "B"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Thu 19 Dec 2024 17:16",
        "comment": "Control table is related to usage control metrics and doesn't focus on performance issues or anomalies related to query optimization. It's more about usage limits and controls.",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 02:29",
        "comment": "this table records alerts that are generated by the Amazon Redshift system when it detects certain conditions that might indicate performance issues. These alerts are triggered by the query optimizer when it detects suboptimal query plans or other issues that could affect performance.",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 03:38",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/cm_chap_system-tables.html\nSTL_ALERT_EVENT_LOG table view to meet this requirement. This system table in Amazon Redshift is designed to record anomalies when a query optimizer identifies conditions that might indicate performance issues",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132349-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 41 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 41,
    "question_text": "A data engineer must ingest a source of structured data that is in .csv format into an Amazon S3 data lake. The .csv files contain 15 columns. Data analysts need to run Amazon Athena queries on one or two columns of the dataset. The data analysts rarely query the entire file.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Glue PySpark job to ingest the source data into the data lake in .csv format."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to ingest the data into the data lake in JSON format."
      },
      {
        "letter": "C",
        "text": "Use an AWS Glue PySpark job to ingest the source data into the data lake in Apache Avro format."
      },
      {
        "letter": "D",
        "text": "Create an AWS Glue extract, transform, and load (ETL) job to read from the .csv structured data source. Configure the job to write the data into the data lake in Apache Parquet format."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 01:34",
        "comment": "Athena is optimized for querying data stored in Parquet format. It can efficiently scan only the necessary columns for a specific query,\nreducing the amount of data processed. This translates to faster query execution times and lower query costs for data analysts who primarily focus on one or two columns",
        "selected_answer": "D"
      },
      {
        "author": "FunkyFresco",
        "date": "Sun 26 May 2024 15:02",
        "comment": "Cost effectively, and they are going to use only one or two columns, columnar.",
        "selected_answer": "D"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 09:00",
        "comment": "MOST cost-effectively = parquet",
        "selected_answer": "D"
      },
      {
        "author": "atu1789",
        "date": "Sun 28 Jan 2024 21:39",
        "comment": "Glue +  Parquet for cost efectiveness",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132348-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 42 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 42,
    "question_text": "A company has five offices in different AWS Regions. Each office has its own human resources (HR) department that uses a unique IAM role. The company stores employee records in a data lake that is based on Amazon S3 storage.\nA data engineering team needs to limit access to the records. Each HR department should be able to access records for only employees who are within the HR department's Region.\nWhich combination of steps should the data engineering team take to meet this requirement with the LEAST operational overhead? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use data filters for each Region to register the S3 paths as data locations."
      },
      {
        "letter": "B",
        "text": "Register the S3 path as an AWS Lake Formation location."
      },
      {
        "letter": "C",
        "text": "Modify the IAM roles of the HR departments to add a data filter for each department's Region."
      },
      {
        "letter": "D",
        "text": "Enable fine-grained access control in AWS Lake Formation. Add a data filter for each Region."
      },
      {
        "letter": "E",
        "text": "Create a separate S3 bucket for each Region. Configure an IAM policy to allow S3 access. Restrict access based on Region."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 05:06",
        "comment": "https://docs.aws.amazon.com/lake-formation/latest/dg/data-filters-about.html\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/access-control-fine-grained.html",
        "selected_answer": "BD"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 01:47",
        "comment": "Registering the S3 path as an AWS Lake Formation location is the first step in leveraging Lake Formation's data governance and access control capabilities. This allows the data engineering team to centrally manage and govern the data stored in the S3 data lake.\nEnabling fine-grained access control in AWS Lake Formation and adding a data filter for each Region is the key step to achieve the desired access control. Data filters in Lake Formation allow you to define row-level and column-level access policies based on specific conditions or attributes, such as the Region in this case",
        "selected_answer": "BD"
      },
      {
        "author": "atu1789",
        "date": "Sun 28 Jan 2024 21:38",
        "comment": "BD makes sense",
        "selected_answer": "BD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132353-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 43 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 43,
    "question_text": "A company uses AWS Step Functions to orchestrate a data pipeline. The pipeline consists of Amazon EMR jobs that ingest data from data sources and store the data in an Amazon S3 bucket. The pipeline also includes EMR jobs that load the data to Amazon Redshift.\nThe company's cloud infrastructure team manually built a Step Functions state machine. The cloud infrastructure team launched an EMR cluster into a VPC to support the EMR jobs. However, the deployed Step Functions state machine is not able to run the EMR jobs.\nWhich combination of steps should the company take to identify the reason the Step Functions state machine is not able to run the EMR jobs? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS CloudFormation to automate the Step Functions state machine deployment. Create a step to pause the state machine during the EMR jobs that fail. Configure the step to wait for a human user to send approval through an email message. Include details of the EMR task in the email message for further analysis."
      },
      {
        "letter": "B",
        "text": "Verify that the Step Functions state machine code has all IAM permissions that are necessary to create and run the EMR jobs. Verify that the Step Functions state machine code also includes IAM permissions to access the Amazon S3 buckets that the EMR jobs use. Use Access Analyzer for S3 to check the S3 access properties."
      },
      {
        "letter": "C",
        "text": "Check for entries in Amazon CloudWatch for the newly created EMR cluster. Change the AWS Step Functions state machine code to use Amazon EMR on EKS. Change the IAM access policies and the security group configuration for the Step Functions state machine code to reflect inclusion of Amazon Elastic Kubernetes Service (Amazon EKS)."
      },
      {
        "letter": "D",
        "text": "Query the flow logs for the VPC. Determine whether the traffic that originates from the EMR cluster can successfully reach the data providers. Determine whether any security group that might be attached to the Amazon EMR cluster allows connections to the data source servers on the informed ports."
      },
      {
        "letter": "E",
        "text": "Check the retry scenarios that the company configured for the EMR jobs. Increase the number of seconds in the interval between each EMR task. Validate that each fallback state has the appropriate catch for each decision state. Configure an Amazon Simple Notification Service (Amazon SNS) topic to store the error messages."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 04:31",
        "comment": "https://docs.aws.amazon.com/step-functions/latest/dg/procedure-create-iam-role.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/service-integration-iam-templates.html",
        "selected_answer": "BD"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 08:09",
        "comment": "Permissions of course and we need to see if the traffic is blocked at any hops because they mention that EMR is IN vpc so... flow-logs",
        "selected_answer": "BD"
      },
      {
        "author": "sam_pre",
        "date": "Thu 03 Apr 2025 09:49",
        "comment": "E> As par as I know, Step function does not require S3 access permission that EMR trying to access. so that eliminates E\nD and E make sense while E is bit less likely troubleshooting, but still valid",
        "selected_answer": "DE"
      },
      {
        "author": "lucas_rfsb",
        "date": "Thu 03 Oct 2024 16:26",
        "comment": "I'd go in BD",
        "selected_answer": "BD"
      },
      {
        "author": "atu1789",
        "date": "Sun 28 Jul 2024 21:39",
        "comment": "BE. In others are are redflag keywords",
        "selected_answer": "BE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132354-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 44 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 44,
    "question_text": "A company is developing an application that runs on Amazon EC2 instances. Currently, the data that the application generates is temporary. However, the company needs to persist the data, even if the EC2 instances are terminated.\nA data engineer must launch new EC2 instances from an Amazon Machine Image (AMI) and configure the instances to preserve the data.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume that contains the application data. Apply the default settings to the EC2 instances."
      },
      {
        "letter": "B",
        "text": "Launch new EC2 instances by using an AMI that is backed by a root Amazon Elastic Block Store (Amazon EBS) volume that contains the application data. Apply the default settings to the EC2 instances."
      },
      {
        "letter": "C",
        "text": "Launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume. Attach an Amazon Elastic Block Store (Amazon EBS) volume to contain the application data. Apply the default settings to the EC2 instances."
      },
      {
        "letter": "D",
        "text": "Launch new EC2 instances by using an AMI that is backed by an Amazon Elastic Block Store (Amazon EBS) volume. Attach an additional EC2 instance store volume to contain the application data. Apply the default settings to the EC2 instances."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "khchan123",
        "date": "Sat 27 Apr 2024 11:34",
        "comment": "CCCCCCC - you need to attach an extra EBS volume\n\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume.\nref: https://repost.aws/knowledge-center/deleteontermination-ebs",
        "selected_answer": "C"
      },
      {
        "author": "mohamedTR",
        "date": "Sun 20 Oct 2024 12:56",
        "comment": "B: by default, delete on termination is checked",
        "selected_answer": "C"
      },
      {
        "author": "Chanduchanti",
        "date": "Fri 21 Feb 2025 09:39",
        "comment": "When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume.",
        "selected_answer": "C"
      },
      {
        "author": "saransh_001",
        "date": "Sun 16 Feb 2025 09:11",
        "comment": "Check in the option B and C the default settings are mentioned. By default an EC2 instance whenever terminates, its root volume also gets terminated. So launch new EC2 instances by using an AMI that is backed by an EC2 instance store volume. Attach an Amazon Elastic Block Store (Amazon EBS) volume to contain the application data. Apply the default settings to the EC2 instances.",
        "selected_answer": "C"
      },
      {
        "author": "mohamedTR",
        "date": "Wed 09 Oct 2024 13:56",
        "comment": "By using an AMI backed by an Amazon EBS root volume, you ensure that the application data is preserved, even if the EC2 instances are terminated, because EBS volumes persist independently of the EC2 lifecycle.",
        "selected_answer": "B"
      },
      {
        "author": "ElFaramawi",
        "date": "Tue 01 Oct 2024 19:04",
        "comment": "This is because Amazon EBS volumes are persistent, meaning the data is preserved even if the EC2 instance is terminated, which meets the requirement to persist the data. C is incorrect because it suggests launching instances using an EC2 instance store volume, which is ephemeral. Even though it proposes attaching an Amazon EBS volume for data, the root volume remains an instance store.",
        "selected_answer": "B"
      },
      {
        "author": "portland",
        "date": "Tue 06 Aug 2024 21:37",
        "comment": "Using default setting means B won’t work.",
        "selected_answer": "C"
      },
      {
        "author": "GustonMari",
        "date": "Wed 10 Jul 2024 12:27",
        "comment": "its C!!! B with default setting will delete the EBS volume on termination",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 02:22",
        "comment": "Amazon EBS volumes provide persistent block storage for EC2 instances. Data written to an EBS volume is independent of the EC2 instance lifecycle. Even if the EC2 instance is terminated, ***the data on the EBS volume remains intact***. Launching new EC2 instances from an AMI backed by an EBS volume containing the application data ensures the data persists across instance restarts or terminations",
        "selected_answer": "B"
      },
      {
        "author": "hnk",
        "date": "Wed 22 May 2024 17:04",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 14:43",
        "comment": "launch EC2 using AMI with root EBS that contains data",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 07 May 2024 05:05",
        "comment": "C - Looks better, because it will save data in all cases",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sun 14 Apr 2024 06:53",
        "comment": "ccccccc",
        "selected_answer": "C"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 09:11",
        "comment": "This question is more for practitioner exam :)",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132667-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 45 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 45,
    "question_text": "A company uses Amazon Athena to run SQL queries for extract, transform, and load (ETL) tasks by using Create Table As Select (CTAS). The company must use Apache Spark instead of SQL to generate analytics.\nWhich solution will give the company the ability to use Spark to access Athena?",
    "choices": [
      {
        "letter": "A",
        "text": "Athena query settings"
      },
      {
        "letter": "B",
        "text": "Athena workgroup"
      },
      {
        "letter": "C",
        "text": "Athena data source"
      },
      {
        "letter": "D",
        "text": "Athena query editor"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 09:18",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\n\"To use Apache Spark in Amazon Athena, you create an Amazon Athena workgroup that uses a Spark engine.\"",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 02:30",
        "comment": "The Athena data source acts as a bridge between Athena and other analytics engines, such as Apache Spark. By using the Athena data source connector, you can access data stored in various formats (e.g., CSV, JSON, Parquet) and locations (e.g., Amazon S3, Apache Hive Metastore) through Spark applications",
        "selected_answer": "C"
      },
      {
        "author": "Tester_TKK",
        "date": "Sat 19 Apr 2025 23:40",
        "comment": "B makes sense",
        "selected_answer": "B"
      },
      {
        "author": "lsj900605",
        "date": "Sat 16 Nov 2024 13:16",
        "comment": "It is B, not C. \nThe workgroup is for organizing, controlling, and monitoring queries. \nThe Data source is the mechanism that enables Spark to query data via Athena. It allows Spark to interact with Athena. \nThe question focuses on enabling Apache Spark within Athena to generate analytics instead of using SQL. Thus, you must create a Spark-enabled workgroup",
        "selected_answer": "B"
      },
      {
        "author": "theloseralreadytaken",
        "date": "Sat 26 Oct 2024 09:19",
        "comment": "Athena datasource doesn't specifially enable Spark access",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Thu 11 Jul 2024 05:06",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html\nTo get started with Apache Spark on Amazon Athena, you must first create a Spark enabled workgroup. After you switch to the workgroup, you can create a notebook or open an existing notebook. When you open a notebook in Athena, a new session is started for it automatically and you can work with it directly in the Athena notebook editor.",
        "selected_answer": "B"
      },
      {
        "author": "blackgamer",
        "date": "Sat 30 Mar 2024 14:02",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-getting-started.html",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 09:33",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132364-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 46 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 46,
    "question_text": "A company needs to partition the Amazon S3 storage that the company uses for a data lake. The partitioning will use a path of the S3 object keys in the following format: s3://bucket/prefix/year=2023/month=01/day=01.\nA data engineer must ensure that the AWS Glue Data Catalog synchronizes with the S3 storage when the company adds new partitions to the bucket.\nWhich solution will meet these requirements with the LEAST latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Schedule an AWS Glue crawler to run every morning."
      },
      {
        "letter": "B",
        "text": "Manually run the AWS Glue CreatePartition API twice each day."
      },
      {
        "letter": "C",
        "text": "Use code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call."
      },
      {
        "letter": "D",
        "text": "Run the MSCK REPAIR TABLE command from the AWS Glue console."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 08:40",
        "comment": "Use code that writes data to Amazon S3 to invoke the Boto3 AWS Glue create_partition API call. This approach ensures that the Data Catalog is updated as soon as new data is written to S3, providing the least latency in reflecting new partitions.",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 03:35",
        "comment": "By embedding the Boto3 create_partition API call within the code that writes data to S3, you achieve near real-time synchronization. The Data Catalog is updated immediately after a new partition is created in S3.",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sun 01 Dec 2024 07:34",
        "comment": "The explanation could be more precise regarding the interaction with Amazon S3 and AWS Glue. The key point is that the process should be triggered immediately when new data is added to S3. This can be achieved through event-driven architecture, which indeed makes the solution intuitive and efficient.",
        "selected_answer": "C"
      },
      {
        "author": "valuedate",
        "date": "Sun 24 Nov 2024 16:10",
        "comment": "add partition after writing the data in s3",
        "selected_answer": "C"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Thu 14 Nov 2024 11:07",
        "comment": "It's about \"synchronizing AWS Glue Data Catalog with S3\". So for me it's D - using MSCK REPAIR TABLE for existing S3 partitions (https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html)",
        "selected_answer": "D"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 08:23",
        "comment": "It's pure event-driven so... C",
        "selected_answer": "C"
      },
      {
        "author": "atu1789",
        "date": "Sun 28 Jul 2024 23:30",
        "comment": "C. Least latency",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132669-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 47 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 47,
    "question_text": "A media company uses software as a service (SaaS) applications to gather data by using third-party tools. The company needs to store the data in an Amazon S3 bucket. The company will use Amazon Redshift to perform analytics based on the data.\nWhich AWS service or feature will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon Managed Streaming for Apache Kafka (Amazon MSK)"
      },
      {
        "letter": "B",
        "text": "Amazon AppFlow"
      },
      {
        "letter": "C",
        "text": "AWS Glue Data Catalog"
      },
      {
        "letter": "D",
        "text": "Amazon Kinesis"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sun 01 Dec 2024 07:39",
        "comment": "That's exactly the purpose of AppFlow: \"fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift. For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.\"\n\nhttps://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 03:39",
        "comment": "the media company can leverage a fully managed service that simplifies the process of ingesting data from their third-party SaaS applications into an Amazon S3 bucket, with minimal operational overhead. Additionally, AppFlow can integrate with Amazon Redshift, allowing the company to load the ingested data directly into their analytics environment for further processing and analysis",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 08:27",
        "comment": "https://docs.aws.amazon.com/appflow/latest/userguide/flow-tutorial.html",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 08:49",
        "comment": "https://d1.awsstatic.com/solutions/guidance/architecture-diagrams/integrating-third-party-saas-data-using-amazon-appflow.pdf\nAmazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software as a Service (SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks. It can store the raw data pulled from SaaS applications in Amazon S3, and integrates with AWS Glue Data Catalog to catalog and store metadata",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132672-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 48 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 48,
    "question_text": "A data engineer is using Amazon Athena to analyze sales data that is in Amazon S3. The data engineer writes a query to retrieve sales amounts for 2023 for several products from a table named sales_data. However, the query does not return results for all of the products that are in the sales_data table. The data engineer needs to troubleshoot the query to resolve the issue.\nThe data engineer's original query is as follows:\nSELECT product_name, sum(sales_amount)\nFROM sales_data -\nWHERE year = 2023 -\nGROUP BY product_name -\nHow should the data engineer modify the Athena query to meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Replace sum(sales_amount) with count(*) for the aggregation."
      },
      {
        "letter": "B",
        "text": "Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023."
      },
      {
        "letter": "C",
        "text": "Add HAVING sum(sales_amount) > 0 after the GROUP BY clause."
      },
      {
        "letter": "D",
        "text": "Remove the GROUP BY clause."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 09:32",
        "comment": "\"SELECT product_name, sum(sales_amount)\nFROM sales_data\nWHERE extract(year FROM sales_date) = 2023\nGROUP BY product_name;\"\nA. This would change the query to count the number of rows instead of summing sales.\nC. This would filter out products with zero sales amounts.\nD. Removing the GROUP BY clause would result in a single sum of all sales amounts without grouping by product_name.",
        "selected_answer": "B"
      },
      {
        "author": "rebasheer",
        "date": "Tue 12 Aug 2025 17:20",
        "comment": "c is the correct one",
        "selected_answer": "C"
      },
      {
        "author": "YUICH",
        "date": "Tue 28 Jan 2025 07:59",
        "comment": "hy Option (B) Works\nIf the underlying table field is a date or timestamp (rather than a numeric year column), using WHERE year = 2023 filters out all rows that do not literally match year = 2023.\nBy using extract(year FROM sales_data) = 2023, you are correctly filtering rows whose date (or timestamp) in the sales_data column corresponds to the year 2023.\nHence, (B) resolves the problem by filtering on the correct year value from the actual date/timestamp column, ensuring all qualifying products are included in the results.",
        "selected_answer": "B"
      },
      {
        "author": "Udyan",
        "date": "Sat 11 Jan 2025 17:10",
        "comment": "The issue might be that some products have sales amounts of 0 or NULL, and those records are being excluded from the results because Athena may not include them in the final output when performing aggregation. By using the HAVING clause, you can filter the groups based on the aggregated sales amount (sum). This ensures that only products with a non-zero sum of sales are returned in the results. The HAVING clause is used to filter results after the aggregation.",
        "selected_answer": "C"
      },
      {
        "author": "MLOPS_eng",
        "date": "Sun 29 Dec 2024 00:40",
        "comment": "The HAVING clause filters the results to include only products with an aggregated sales amount greater than zero.",
        "selected_answer": "C"
      },
      {
        "author": "Assassin27",
        "date": "Fri 27 Dec 2024 17:07",
        "comment": "SELECT product_name, sum(sales_amount)\nFROM sales_data\nWHERE year = 2023\nGROUP BY product_name\nHAVING sum(sales_amount) > 0\n\nExplanation:\nThe HAVING clause ensures that only products with a non-zero aggregated sales amount are included in the results. This will address cases where products exist in the table but have no sales data for 2023.",
        "selected_answer": "C"
      },
      {
        "author": "kailu",
        "date": "Mon 23 Dec 2024 21:42",
        "comment": "There is no issue with the WHERE clause from the original query, so B is not the right option IMO.",
        "selected_answer": "C"
      },
      {
        "author": "valuedate",
        "date": "Fri 24 May 2024 15:17",
        "comment": "year should be the partition in s3 so its necessary to extract. its not a column",
        "selected_answer": "B"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 15:25",
        "comment": "No need to extract the year again",
        "selected_answer": "C"
      },
      {
        "author": "Just_Ninja",
        "date": "Thu 16 May 2024 16:04",
        "comment": "https://docs.aws.amazon.com/kinesisanalytics/latest/sqlref/sql-reference-having-clause.html",
        "selected_answer": "C"
      },
      {
        "author": "Snape",
        "date": "Mon 29 Apr 2024 07:04",
        "comment": "Wrong answers \n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This option will return the count of records for each product, not the sum of sales amounts, which is the desired result.\n\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. The year column likely stores the year value directly, so there's no need to extract it from a date or timestamp column.\n\nD. Remove the GROUP BY clause. Removing the GROUP BY clause will cause an error because the sum(sales_amount) aggregation function requires a GROUP BY clause to specify the grouping column (product_name in this case).",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 06:08",
        "comment": "Gemini: C. Add HAVING sum(sales_amount) > 0 after the GROUP BY clause.\n\nZero Sales Products: The original query is likely missing products that had zero sales amount in 2023. This modification filters the grouped results, ensuring only products with positive sales are displayed.\nWhy Other Options Don't Address the Core Issue:\n\nA. Replace sum(sales_amount) with count(*) for the aggregation. This would show how many sales transactions a product had, but not if it generated any revenue. It wouldn't solve the issue of missing products.\nB. Change WHERE year = 2023 to WHERE extract(year FROM sales_data) = 2023. This is functionally equivalent to the original WHERE clause if the year column is already an integer type. It wouldn't fix missing products.\nD. Remove the GROUP BY clause. This would aggregate all sales for 2023 with no product breakdown, losing the granularity needed.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132673-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 49 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 49,
    "question_text": "A data engineer has a one-time task to read data from objects that are in Apache Parquet format in an Amazon S3 bucket. The data engineer needs to query only one column of the data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure an AWS Lambda function to load data from the S3 bucket into a pandas dataframe. Write a SQL SELECT statement on the dataframe to query the required column."
      },
      {
        "letter": "B",
        "text": "Use S3 Select to write a SQL SELECT statement to retrieve the required column from the S3 objects."
      },
      {
        "letter": "C",
        "text": "Prepare an AWS Glue DataBrew project to consume the S3 objects and to query the required column."
      },
      {
        "letter": "D",
        "text": "Run an AWS Glue crawler on the S3 objects. Use a SQL SELECT statement in Amazon Athena to query the required column."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "XP_2600",
        "date": "Mon 09 Jun 2025 21:49",
        "comment": "B is no longer valid:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-select.html\n\nImportant\n\nAmazon S3 Select is no longer available to new customers. Existing customers of Amazon S3 Select can continue to use the feature as usual.",
        "selected_answer": "D"
      },
      {
        "author": "imymoco",
        "date": "Wed 25 Dec 2024 10:08",
        "comment": "only one column -> S3 select",
        "selected_answer": "B"
      },
      {
        "author": "JoeAWSOCM",
        "date": "Fri 06 Dec 2024 16:40",
        "comment": "S3 select is for querying one object. Here the requirement is to query one column from multiple objects.  Also S3 select is discontinued for new users. So answer could be D",
        "selected_answer": "D"
      },
      {
        "author": "hogs",
        "date": "Sat 15 Jun 2024 00:08",
        "comment": "omly once",
        "selected_answer": "B"
      },
      {
        "author": "FunkyFresco",
        "date": "Fri 24 May 2024 02:59",
        "comment": "if is one-time task",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 09:37",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-select.html",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 10:29",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory-athena-query.html\nS3 Select allows you to retrieve a subset of data from an object stored in S3 using simple SQL expressions. It is capable of working directly with objects in Parquet format.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132674-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 50 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 50,
    "question_text": "A company uses Amazon Redshift for its data warehouse. The company must automate refresh schedules for Amazon Redshift materialized views.\nWhich solution will meet this requirement with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Apache Airflow to refresh the materialized views."
      },
      {
        "letter": "B",
        "text": "Use an AWS Lambda user-defined function (UDF) within Amazon Redshift to refresh the materialized views."
      },
      {
        "letter": "C",
        "text": "Use the query editor v2 in Amazon Redshift to refresh the materialized views."
      },
      {
        "letter": "D",
        "text": "Use an AWS Glue workflow to refresh the materialized views."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "magnorm",
        "date": "Sun 29 Dec 2024 13:28",
        "comment": "https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 03:55",
        "comment": "the company can automate the refresh schedules for materialized views with minimal effort. This approach leverages the built-in capabilities of Amazon Redshift, reducing the need for additional services, configurations, or custom code. It aligns with the principle of using the simplest and most straightforward solution that meets the requirements, minimizing operational overhead and complexity",
        "selected_answer": "C"
      },
      {
        "author": "d8945a1",
        "date": "Fri 08 Nov 2024 06:32",
        "comment": "We can schedule the refresh using query scheduler from Query Editor V2.",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 06:14",
        "comment": "Amazon Redshift can automatically refresh materialized views with up-to-date data from its base tables when materialized views are created with or altered to have the autorefresh option. For more details, refer to the documentation here, https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html.",
        "selected_answer": "C"
      },
      {
        "author": "FuriouZ",
        "date": "Mon 23 Sep 2024 15:38",
        "comment": "You can set autorefresh for materialized views using CREATE MATERIALIZED VIEW. You can also use the AUTO REFRESH clause to refresh materialized views automatically.",
        "selected_answer": "C"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 08:40",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html",
        "selected_answer": "C"
      },
      {
        "author": "confusedyeti69",
        "date": "Fri 13 Sep 2024 13:22",
        "comment": "Lambda requires code and configuring permissions. A and D are additional overheads as well. Vote C",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 09:39",
        "comment": "AWS Lambda allows running code in response to triggers without needing to provision or manage servers. However, creating a UDF within Amazon Redshift to call a Lambda function for this purpose involves writing custom code and managing permissions between Lambda and Redshift.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132676-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 51 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 51,
    "question_text": "A data engineer must orchestrate a data pipeline that consists of one AWS Lambda function and one AWS Glue job. The solution must integrate with AWS services.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Step Functions workflow that includes a state machine. Configure the state machine to run the Lambda function and then the AWS Glue job."
      },
      {
        "letter": "B",
        "text": "Use an Apache Airflow workflow that is deployed on an Amazon EC2 instance. Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job."
      },
      {
        "letter": "C",
        "text": "Use an AWS Glue workflow to run the Lambda function and then the AWS Glue job."
      },
      {
        "letter": "D",
        "text": "Use an Apache Airflow workflow that is deployed on Amazon Elastic Kubernetes Service (Amazon EKS). Define a directed acyclic graph (DAG) in which the first task is to call the Lambda function and the second task is to call the AWS Glue job."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 03:17",
        "comment": "Step Functions is a managed service for building serverless workflows. You define a state machine that orchestrates the execution sequence.\nThis eliminates the need to manage and maintain your own workflow orchestration server like Airflow.",
        "selected_answer": "A"
      },
      {
        "author": "hcong",
        "date": "Tue 20 Aug 2024 11:58",
        "comment": "AWS Glue is a fully managed ETL (extract, transform, load) service that makes it easy to orchestrate data pipelines. Using the AWS Glue workflow to run Lambda functions and glue jobs is the easiest and least expensive option because it's a fully managed service that requires no additional workflow tools or infrastructure to configure and manage. Other options require additional tools or resources to configure and manage, and are therefore more expensive to manage.",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 01 Jun 2024 06:46",
        "comment": "Step Functions can handle both Lambda and Glue in this scenario, making it the best choice.",
        "selected_answer": "A"
      },
      {
        "author": "hnk",
        "date": "Mon 13 May 2024 08:10",
        "comment": "B and D require additional effort\nC Glue workflows do not have a direct integration with lambda\nhence the best choice is A",
        "selected_answer": "A"
      },
      {
        "author": "FuriouZ",
        "date": "Sun 24 Mar 2024 09:44",
        "comment": "Key word orchestrating is most likely step functions",
        "selected_answer": "A"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 10:46",
        "comment": "Option A, using AWS Step Functions, is the best solution to meet the requirement with the least management overhead. Step Functions is designed for easy integration with AWS services like Lambda and Glue, providing a managed, low-code approach to orchestrate workflows. This allows for a more straightforward setup and less ongoing management compared to the other options.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132677-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 52 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 52,
    "question_text": "A company needs to set up a data catalog and metadata management for data sources that run in the AWS Cloud. The company will use the data catalog to maintain the metadata of all the objects that are in a set of data stores. The data stores include structured sources such as Amazon RDS and Amazon Redshift. The data stores also include semistructured sources such as JSON files and .xml files that are stored in Amazon S3.\nThe company needs a solution that will update the data catalog on a regular basis. The solution also must detect changes to the source metadata.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Aurora as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the Aurora data catalog. Schedule the Lambda functions to run periodically."
      },
      {
        "letter": "B",
        "text": "Use the AWS Glue Data Catalog as the central metadata repository. Use AWS Glue crawlers to connect to multiple data stores and to update the Data Catalog with metadata changes. Schedule the crawlers to run periodically to update the metadata catalog."
      },
      {
        "letter": "C",
        "text": "Use Amazon DynamoDB as the data catalog. Create AWS Lambda functions that will connect to the data catalog. Configure the Lambda functions to gather the metadata information from multiple sources and to update the DynamoDB data catalog. Schedule the Lambda functions to run periodically."
      },
      {
        "letter": "D",
        "text": "Use the AWS Glue Data Catalog as the central metadata repository. Extract the schema for Amazon RDS and Amazon Redshift sources, and build the Data Catalog. Use AWS Glue crawlers for data that is in Amazon S3 to infer the schema and to automatically update the Data Catalog."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Tue 10 Dec 2024 04:25",
        "comment": "The AWS Glue Data Catalog is a purpose-built, fully managed service designed to serve as a central metadata repository for your data sources. It provides a unified view of your data across various sources, including structured databases (like Amazon RDS and Amazon Redshift) and semi-structured data formats (like JSON and XML files in Amazon S3).",
        "selected_answer": "B"
      },
      {
        "author": "valuedate",
        "date": "Sun 24 Nov 2024 17:01",
        "comment": "glue data catalog with crawlers",
        "selected_answer": "B"
      },
      {
        "author": "hnk",
        "date": "Wed 13 Nov 2024 10:40",
        "comment": "B is the obvious answer",
        "selected_answer": "A"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 09:23",
        "comment": "A,C out for obvious reason \nD out because it involves manual schema extract",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 09:51",
        "comment": "Option B, using the AWS Glue Data Catalog with AWS Glue Crawlers, is the best solution to meet the requirements with the least operational overhead. It provides a fully managed, integrated solution for cataloging both structured and semistructured data across various AWS data stores without the need for extensive manual configuration or custom coding.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132678-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 53 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 53,
    "question_text": "A company stores data from an application in an Amazon DynamoDB table that operates in provisioned capacity mode. The workloads of the application have predictable throughput load on a regular schedule. Every Monday, there is an immediate increase in activity early in the morning. The application has very low usage during weekends.\nThe company must ensure that the application performs consistently during peak usage times.\nWhich solution will meet these requirements in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Increase the provisioned capacity to the maximum capacity that is currently present during peak load times."
      },
      {
        "letter": "B",
        "text": "Divide the table into two tables. Provision each table with half of the provisioned capacity of the original table. Spread queries evenly across both tables."
      },
      {
        "letter": "C",
        "text": "Use AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times. Schedule lower capacity during off-peak times."
      },
      {
        "letter": "D",
        "text": "Change the capacity mode from provisioned to on-demand. Configure the table to scale up and scale down based on the load on the table."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 10:54",
        "comment": "Option C, using AWS Application Auto Scaling to schedule higher provisioned capacity for peak usage times and lower capacity during off-peak times, is the most cost-effective solution for the described scenario. It allows the company to align their DynamoDB capacity costs with actual usage patterns, scaling up only when needed and scaling down during low-usage periods.",
        "selected_answer": "C"
      },
      {
        "author": "Rakiko",
        "date": "Tue 04 Mar 2025 10:19",
        "comment": "My guess is C as it stands for Cat",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Mon 10 Jun 2024 03:29",
        "comment": "app autoscalling allows you to dynamically adjust provisioned capacity based on usage patterns. You only pay for the capacity you utilize, reducing costs compared to keeping a high, fixed capacity throughout the week",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 06:23",
        "comment": "D. Change the capacity mode from provisioned to on-demand... On-demand mode is great for unpredictable workloads. In your case, with predictable patterns, you'd likely pay more with on-demand than with a well-managed, scheduled, provisioned mode.",
        "selected_answer": "C"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 02:26",
        "comment": "As I understand, should be D",
        "selected_answer": "D"
      },
      {
        "author": "FuriouZ",
        "date": "Sun 24 Mar 2024 09:47",
        "comment": "Obviously better than B because of peak scaling",
        "selected_answer": "C"
      },
      {
        "author": "jpmadan",
        "date": "Fri 22 Mar 2024 08:43",
        "comment": "D\nExcerpts from documentation: \nThis means that provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.\nWhereas on-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use. The on-demand pricing model is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when under-provisioned capacity would impact the user experience.\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132680-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 54 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 54,
    "question_text": "A company is planning to migrate on-premises Apache Hadoop clusters to Amazon EMR. The company also needs to migrate a data catalog into a persistent storage solution.\nThe company currently stores the data catalog in an on-premises Apache Hive metastore on the Hadoop clusters. The company requires a serverless solution to migrate the data catalog.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the Hive metastore into Amazon S3. Configure AWS Glue Data Catalog to scan Amazon S3 to produce the data catalog."
      },
      {
        "letter": "B",
        "text": "Configure a Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use AWS Glue Data Catalog to store the company's data catalog as an external data catalog."
      },
      {
        "letter": "C",
        "text": "Configure an external Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use Amazon Aurora MySQL to store the company's data catalog."
      },
      {
        "letter": "D",
        "text": "Configure a new Hive metastore in Amazon EMR. Migrate the existing on-premises Hive metastore into Amazon EMR. Use the new metastore as the company's data catalog."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Asmunk",
        "date": "Sun 10 Nov 2024 12:39",
        "comment": "A and D can be discarded because of added steps. This link provides documentation for this exact use case : https://aws.amazon.com/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/\nC is also discarded because of the serverless key word, although Aurora can be serverless it is not specified in the choice.",
        "selected_answer": "B"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 06:25",
        "comment": "Serverless and Cost-Efficient: AWS Glue Data Catalog offers a serverless metadata repository, reducing operational overhead and making it cost-effective. Using it as an external data catalog means you don't have to manage additional database infrastructure.\nSeamless Migration: Migrating your existing Hive metastore to Amazon EMR ensures compatibility with your current Hadoop setup. EMR is designed to run Hadoop workloads, facilitating this process.\nFlexibility: An external data catalog in AWS Glue offers flexibility and separation of concerns. Your metastore remains managed by EMR for your Hadoop workloads, while Glue provides a centralized catalog for broader AWS data sources.",
        "selected_answer": "B"
      },
      {
        "author": "arvehisa",
        "date": "Sat 06 Apr 2024 07:35",
        "comment": "B.  https://aws.amazon.com/jp/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/",
        "selected_answer": "B"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 02:42",
        "comment": "I will go with A. Besides DMS is typical for migration, it's the only choice which explicitly concerns about how the migration itself will be made. Other choices would demand a script or GLUE ETL job if you will. But this logic of migration was never put",
        "selected_answer": "A"
      },
      {
        "author": "jpmadan",
        "date": "Fri 22 Mar 2024 08:42",
        "comment": "serverless catalog in AWS == glue",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 11:00",
        "comment": "https://aws.amazon.com/blogs/big-data/migrate-and-deploy-your-apache-hive-metastore-on-amazon-emr/           Option B is likely the most suitable. Migrating the Hive metastore into Amazon EMR and using AWS Glue Data Catalog as an external catalog provides a balance between leveraging the scalable and managed services of AWS (like EMR and Glue Data Catalog) and ensuring a smooth transition from the on-premises setup. This approach leverages the serverless nature of AWS Glue Data Catalog, minimizing operational overhead and potentially reducing costs compared to managing database servers.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132681-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 55 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 55,
    "question_text": "A company uses an Amazon Redshift provisioned cluster as its database. The Redshift cluster has five reserved ra3.4xlarge nodes and uses key distribution.\nA data engineer notices that one of the nodes frequently has a CPU load over 90%. SQL Queries that run on the node are queued. The other four nodes usually have a CPU load under 15% during daily operations.\nThe data engineer wants to maintain the current number of compute nodes. The data engineer also wants to balance the load more evenly across all five compute nodes.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Change the sort key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement."
      },
      {
        "letter": "B",
        "text": "Change the distribution key to the table column that has the largest dimension."
      },
      {
        "letter": "C",
        "text": "Upgrade the reserved node from ra3.4xlarge to ra3.16xlarge."
      },
      {
        "letter": "D",
        "text": "Change the primary key to be the data column that is most often used in a WHERE clause of the SQL SELECT statement."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 10:07",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html\nOption B, changing the distribution key, is the most effective solution to balance the load more evenly across all five compute nodes. Selecting an appropriate distribution key that aligns with the query patterns and data characteristics can result in a more uniform distribution of data and workloads, thus reducing the likelihood of one node being overutilized while others are underutilized.",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 11 Dec 2024 05:35",
        "comment": "In a Redshift cluster with key distribution, data is distributed across compute nodes based on the values of the distribution key. An uneven distribution can lead to skewed workloads on specific nodes.\nBy choosing the table column with the largest dimension (most distinct values) as the distribution key, you ensure a more even spread of data across all nodes. This balances the processing load on each node when queries access that column.",
        "selected_answer": "B"
      },
      {
        "author": "khchan123",
        "date": "Mon 28 Oct 2024 09:00",
        "comment": "The correct solution is B. Change the distribution key to the table column that has the largest dimension. This will help to distribute the data more evenly across the nodes, reducing the load on the heavily utilized node.",
        "selected_answer": "B"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 06:33",
        "comment": "Gemini result:\nUnderstanding the Problem:\n\nThe scenario describes a Redshift cluster with uneven load distribution. This indicates potential issues with either the distribution style or the sort key.\n\nKey Distribution:\n\nThe problem states that the cluster uses key distribution, meaning a specific column is designated as the distribution key. Data rows with matching distribution key values are placed on the same node.\n\nSort Key:\n\nA sort key determines the order in which data is physically stored within a table's blocks on a node.  A well-chosen sort key can significantly optimize query performance, especially when queries often filter by that column.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132683-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 56 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 56,
    "question_text": "A security company stores IoT data that is in JSON format in an Amazon S3 bucket. The data structure can change when the company upgrades the IoT devices. The company wants to create a data catalog that includes the IoT data. The company's analytics department will use the data catalog to index the data.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Redshift provisioned cluster. Create an Amazon Redshift Spectrum database for the analytics department to explore the data that is in Amazon S3. Create Redshift stored procedures to load the data into Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Create an Amazon Athena workgroup. Explore the data that is in Amazon S3 by using Apache Spark through Athena. Provide the Athena workgroup schema and tables to the analytics department."
      },
      {
        "letter": "D",
        "text": "Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create AWS Lambda user defined functions (UDFs) by using the Amazon Redshift Data API. Create an AWS Step Functions job to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 11:43",
        "comment": "Option A, creating an AWS Glue Data Catalog with Glue Schema Registry and orchestrating data ingestion into Amazon Redshift Serverless using AWS Glue, appears to be the most cost-effective and suitable solution. It offers a serverless approach to manage the evolving data schema of the IoT data and efficiently supports data analytics needs without the overhead of managing a provisioned database cluster or complex orchestration setups.",
        "selected_answer": "A"
      },
      {
        "author": "VerRi",
        "date": "Sun 19 May 2024 18:17",
        "comment": "Athena is not able to create new data catalog",
        "selected_answer": "A"
      },
      {
        "author": "khchan123",
        "date": "Sun 28 Apr 2024 08:03",
        "comment": "The correct solution is A. Create an AWS Glue Data Catalog. Configure an AWS Glue Schema Registry. Create a new AWS Glue workload to orchestrate the ingestion of the data that the analytics department will use into Amazon Redshift Serverless.\n\nOption C (Amazon Athena and Apache Spark) is suitable for ad-hoc querying and exploration but may not be the best choice for the analytics department's ongoing data analysis needs, as Athena is designed for interactive querying rather than complex data transformations.",
        "selected_answer": "A"
      },
      {
        "author": "chris_spencer",
        "date": "Wed 17 Apr 2024 12:19",
        "comment": "The objective is to create a data catalog that includes the IoT data and AWS Glue Data Catalog is the best option for this requirement.\nhttps://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\n\nC is incorrect. While Athena makes it easy to read from S3 using SQL, it does not crawl the data source and create a data catalog.",
        "selected_answer": "A"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 09:04",
        "comment": "Why Option C is the Most Cost-Effective\n\nServerless and Pay-as-you-go: Athena is a serverless query service, meaning you only pay for the queries the analytics department runs. No need to provision and manage always-running clusters.\nFlexible Schema Handling: Athena works well with semi-structured data like JSON and can handle schema evolution on the fly. This is perfect for the scenario where IoT data structures might change.\nSpark Integration: Integrating Apache Spark with Athena provides rich capabilities for data processing and transformation.\nEase of Use for Analytics: Athena's familiar SQL-like interface and ability to directly query S3 data make it convenient for the analytics department.",
        "selected_answer": "C"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 02:58",
        "comment": "Options A, B, and D involve setting up additional infrastructure (e.g., AWS Glue, Redshift clusters, Lambda functions) which may incur unnecessary costs and complexity for the given requirements. Option C, on the other hand, utilizes a serverless and scalable solution directly querying data in S3, making it the most cost-effective choice.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132684-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 57 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 57,
    "question_text": "A company stores details about transactions in an Amazon S3 bucket. The company wants to log all writes to the S3 bucket into another S3 bucket that is in the same AWS Region.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the event to Amazon Kinesis Data Firehose. Configure Kinesis Data Firehose to write the event to the logs S3 bucket."
      },
      {
        "letter": "B",
        "text": "Create a trail of management events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket."
      },
      {
        "letter": "C",
        "text": "Configure an S3 Event Notifications rule for all activities on the transactions S3 bucket to invoke an AWS Lambda function. Program the Lambda function to write the events to the logs S3 bucket."
      },
      {
        "letter": "D",
        "text": "Create a trail of data events in AWS CloudTraiL. Configure the trail to receive data from the transactions S3 bucket. Specify an empty prefix and write-only events. Specify the logs S3 bucket as the destination bucket."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 10:48",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html\nOption D, creating a trail of data events in AWS CloudTrail, is the best solution to meet the requirement with the least operational effort. It directly logs the desired activities to another S3 bucket and does not involve the development and maintenance of additional resources like Lambda functions or Kinesis Data Firehose streams.",
        "selected_answer": "D"
      },
      {
        "author": "VerRi",
        "date": "Tue 19 Nov 2024 19:32",
        "comment": "A:  Don't need all activities on the S3 bucket\nB:  Management events include not only the data log but also the admin log\nC:  Don't need all activities on the S3 bucket\nOption D with the LEAST operational effort",
        "selected_answer": "D"
      },
      {
        "author": "khchan123",
        "date": "Mon 28 Oct 2024 09:07",
        "comment": "Correct answer is D.\n\nOption A or C require writing custom Lambda code to handle the events and write them to the Kinesis or S3 bucket so they are not the LEAST operational effort.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132685-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 58 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 58,
    "question_text": "A data engineer needs to maintain a central metadata repository that users access through Amazon EMR and Amazon Athena queries. The repository needs to provide the schema and properties of many tables. Some of the metadata is stored in Apache Hive. The data engineer needs to import the metadata from Hive into the central metadata repository.\nWhich solution will meet these requirements with the LEAST development effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon EMR and Apache Ranger."
      },
      {
        "letter": "B",
        "text": "Use a Hive metastore on an EMR cluster."
      },
      {
        "letter": "C",
        "text": "Use the AWS Glue Data Catalog."
      },
      {
        "letter": "D",
        "text": "Use a metastore on an Amazon RDS for MySQL DB instance."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 12:06",
        "comment": "https://aws.amazon.com/blogs/big-data/metadata-classification-lineage-and-discovery-using-apache-atlas-on-amazon-emr/\nOption C, using the AWS Glue Data Catalog, is the best solution to meet the requirements with the least development effort. The AWS Glue Data Catalog is designed to be a central metadata repository that can integrate with various AWS services including EMR and Athena, providing a managed and scalable solution for metadata management with built-in Hive compatibility.",
        "selected_answer": "C"
      },
      {
        "author": "vic614",
        "date": "Thu 20 Jun 2024 20:42",
        "comment": "Data Catalog.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132686-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 59 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 59,
    "question_text": "A company needs to build a data lake in AWS. The company must provide row-level data access and column-level data access to specific teams. The teams will access the data by using Amazon Athena, Amazon Redshift Spectrum, and Apache Hive from Amazon EMR.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon S3 for data lake storage. Use S3 access policies to restrict data access by rows and columns. Provide data access through Amazon S3."
      },
      {
        "letter": "B",
        "text": "Use Amazon S3 for data lake storage. Use Apache Ranger through Amazon EMR to restrict data access by rows and columns. Provide data access by using Apache Pig."
      },
      {
        "letter": "C",
        "text": "Use Amazon Redshift for data lake storage. Use Redshift security policies to restrict data access by rows and columns. Provide data access by using Apache Spark and Amazon Athena federated queries."
      },
      {
        "letter": "D",
        "text": "Use Amazon S3 for data lake storage. Use AWS Lake Formation to restrict data access by rows and columns. Provide data access through AWS Lake Formation."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Shanmahi",
        "date": "Wed 28 Aug 2024 03:31",
        "comment": "Using Amazon S3 for storage and AWS Lake Formation for fine-grained access control like row-level or column-level access.",
        "selected_answer": "D"
      },
      {
        "author": "cas_tori",
        "date": "Fri 16 Aug 2024 12:29",
        "comment": "this id D",
        "selected_answer": "D"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 12:18",
        "comment": "https://docs.aws.amazon.com/lake-formation/latest/dg/cbac-tutorial.html\nOption D, using Amazon S3 for data lake storage and AWS Lake Formation for access control, is the most suitable solution. It meets the requirements for row-level and column-level access control and integrates well with Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on EMR, all with lower operational overhead compared to the other options.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132687-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 60 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 60,
    "question_text": "An airline company is collecting metrics about flight activities for analytics. The company is conducting a proof of concept (POC) test to show how analytics can provide insights that the company can use to increase on-time departures.\nThe POC test uses objects in Amazon S3 that contain the metrics in .csv format. The POC test uses Amazon Athena to query the data. The data is partitioned in the S3 bucket by date.\nAs the amount of data increases, the company wants to optimize the storage solution to improve query performance.\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Add a randomized string to the beginning of the keys in Amazon S3 to get more throughput across partitions."
      },
      {
        "letter": "B",
        "text": "Use an S3 bucket that is in the same account that uses Athena to query the data."
      },
      {
        "letter": "C",
        "text": "Use an S3 bucket that is in the same AWS Region where the company runs Athena queries."
      },
      {
        "letter": "D",
        "text": "Preprocess the .csv data to JSON format by fetching only the document keys that the query requires."
      },
      {
        "letter": "E",
        "text": "Preprocess the .csv data to Apache Parquet format by fetching only the data blocks that are needed for predicates."
      }
    ],
    "correct_answer": "CE",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 11:36",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html",
        "selected_answer": "CE"
      },
      {
        "author": "Ramdi1",
        "date": "Sun 09 Mar 2025 12:18",
        "comment": "C - Reduces latency and network costs → When Athena queries S3 data in the same AWS Region, data does not cross AWS Regions, improving performance.\nLower query execution time → No inter-region data transfer delays.\nCost-Effective → AWS charges for cross-region data transfers, but querying within the same region avoids these costs.\n\n\nE - Parquet is a columnar storage format → Queries can fetch only needed columns, reducing scanning costs.",
        "selected_answer": "CE"
      },
      {
        "author": "tgv",
        "date": "Thu 28 Nov 2024 10:48",
        "comment": "I will go with C and E.",
        "selected_answer": "CE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/135451-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 61 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 61,
    "question_text": "A company uses Amazon RDS for MySQL as the database for a critical application. The database workload is mostly writes, with a small number of reads.\nA data engineer notices that the CPU utilization of the DB instance is very high. The high CPU utilization is slowing down the application. The data engineer must reduce the CPU utilization of the DB Instance.\nWhich actions should the data engineer take to meet this requirement? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use the Performance Insights feature of Amazon RDS to identify queries that have high CPU utilization. Optimize the problematic queries."
      },
      {
        "letter": "B",
        "text": "Modify the database schema to include additional tables and indexes."
      },
      {
        "letter": "C",
        "text": "Reboot the RDS DB instance once each week."
      },
      {
        "letter": "D",
        "text": "Upgrade to a larger instance size."
      },
      {
        "letter": "E",
        "text": "Implement caching to reduce the database query load."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 03:27",
        "comment": "I will go for A and D, since other options are more likely to improve read performance issues.",
        "selected_answer": "AD"
      },
      {
        "author": "michele_scar",
        "date": "Wed 06 Nov 2024 15:20",
        "comment": "With A you should understand why the CPU is in high loading. B is mentioned in the last phrase of A (optimizing). Remain valid only D",
        "selected_answer": "AD"
      },
      {
        "author": "fceb2c1",
        "date": "Sun 24 Mar 2024 11:48",
        "comment": "A and D.\n\nFor A it is mentioned here https://repost.aws/knowledge-center/rds-instance-high-cpu",
        "selected_answer": "AD"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 11:13",
        "comment": "Since the questions states that \"the database workload is mostly writes\" let's eliminate the options that improves the reads.",
        "selected_answer": "BD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/135091-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 62 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 62,
    "question_text": "A company has used an Amazon Redshift table that is named Orders for 6 months. The company performs weekly updates and deletes on the table. The table has an interleaved sort key on a column that contains AWS Regions.\nThe company wants to reclaim disk space so that the company will not run out of storage space. The company also wants to analyze the sort key column.\nWhich Amazon Redshift command will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "VACUUM FULL Orders"
      },
      {
        "letter": "B",
        "text": "VACUUM DELETE ONLY Orders"
      },
      {
        "letter": "C",
        "text": "VACUUM REINDEX Orders"
      },
      {
        "letter": "D",
        "text": "VACUUM SORT ONLY Orders"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 11:21",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html\n\"A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\"\nA - \"A full vacuum doesn't perform a reindex for interleaved tables.\"- from the docs above\nB- \"A DELETE ONLY vacuum operation doesn't sort table data.\" - from the docs above\nD - \"without reclaiming space freed by deleted rows. \" - from the docs above",
        "selected_answer": "C"
      },
      {
        "author": "d8945a1",
        "date": "Wed 08 May 2024 16:05",
        "comment": "VACUUM REINDEX makes an additional pass to analyze the interleaved sort keys. \n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#r_VACUUM_command-parameters",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 09:16",
        "comment": "Reclaiming Space: After updates and deletes, Redshift tables can retain deleted data blocks, taking up space. The VACUUM REINDEX command:\n\nReclaims the space taken up by the deleted rows.\nRebuilds indexes on the sort key columns.\nAnalyzing the Sort Key: Since the sort key column contains AWS Regions, rebuilding the indexes on this column will help cluster data according to region.  This clustering can improve performance for queries that filter or group by region.",
        "selected_answer": "C"
      },
      {
        "author": "arvehisa",
        "date": "Sat 06 Apr 2024 10:44",
        "comment": "Correct Answer: C\n\nRequirements:\n1. relcaim the disk space\n2. analyze the sork key column\n\nDocument: https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#vacuum-reindex\nVACUUM FULL: A full vacuum doesn't perform a reindex for interleaved tables. To reindex interleaved tables followed by a full vacuum, use the VACUUM REINDEX option.\nVACUUM REINDEX: Analyzes the distribution of the values in interleaved sort key columns, then performs a full VACUUM operation.",
        "selected_answer": "C"
      },
      {
        "author": "lucas_rfsb",
        "date": "Wed 03 Apr 2024 19:59",
        "comment": "FULL is the only one which claims space and sorts. \nFULL\nSorts the specified table (or all tables in the current database) and reclaims disk space occupied by rows that were marked for deletion by previous UPDATE and DELETE operations. VACUUM FULL is the default.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132688-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 63 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 63,
    "question_text": "A manufacturing company wants to collect data from sensors. A data engineer needs to implement a solution that ingests sensor data in near real time.\nThe solution must store the data to a persistent data store. The solution must store the data in nested JSON format. The company must have the ability to query from the data store with a latency of less than 10 milliseconds.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use a self-hosted Apache Kafka cluster to capture the sensor data. Store the data in Amazon S3 for querying."
      },
      {
        "letter": "B",
        "text": "Use AWS Lambda to process the sensor data. Store the data in Amazon S3 for querying."
      },
      {
        "letter": "C",
        "text": "Use Amazon Kinesis Data Streams to capture the sensor data. Store the data in Amazon DynamoDB for querying."
      },
      {
        "letter": "D",
        "text": "Use Amazon Simple Queue Service (Amazon SQS) to buffer incoming sensor data. Use AWS Glue to store the data in Amazon RDS for querying."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 00:53",
        "comment": "Amazon Kinesis Data Streams is a fully managed service that allows for seamless integration of diverse data sources, including IoT sensors. By using Kinesis Data Streams as the ingestion mechanism, the company can avoid the overhead of setting up and managing an Apache Kafka cluster or other data ingestion pipelines.",
        "selected_answer": "C"
      },
      {
        "author": "Snape",
        "date": "Thu 02 May 2024 02:20",
        "comment": "near real time = Kinesis Data streams",
        "selected_answer": "C"
      },
      {
        "author": "Ousseyni",
        "date": "Tue 16 Apr 2024 23:37",
        "comment": "Option C is the best solution to meet the requirements",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 13:00",
        "comment": "Option C, using Amazon Kinesis Data Streams to capture the sensor data and storing it in Amazon DynamoDB for querying, is the best solution to meet the requirements with the least operational overhead. This solution is well-optimized for real-time data ingestion, supports the desired data format, and provides the necessary query performance.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132689-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 64 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 64,
    "question_text": "A company stores data in a data lake that is in Amazon S3. Some data that the company stores in the data lake contains personally identifiable information (PII). Multiple user groups need to access the raw data. The company must ensure that user groups can access only the PII that they require.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Athena to query the data. Set up AWS Lake Formation and create data filters to establish levels of access for the company's IAM roles. Assign each user to the IAM role that matches the user's PII access requirements."
      },
      {
        "letter": "B",
        "text": "Use Amazon QuickSight to access the data. Use column-level security features in QuickSight to limit the PII that users can retrieve from Amazon S3 by using Amazon Athena. Define QuickSight access levels based on the PII access requirements of the users."
      },
      {
        "letter": "C",
        "text": "Build a custom query builder UI that will run Athena queries in the background to access the data. Create user groups in Amazon Cognito. Assign access levels to the user groups based on the PII access requirements of the users."
      },
      {
        "letter": "D",
        "text": "Create IAM roles that have different levels of granular access. Assign the IAM roles to IAM user groups. Use an identity-based policy to assign access levels to user groups at the column level."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "lucas_rfsb",
        "date": "Wed 02 Oct 2024 03:42",
        "comment": "Amazon Athena to query the data and setting up AWS Lake Formation with data filters, the company can ensure that user groups can access only the personally identifiable information (PII) that they require. The combination of Athena for querying and Lake Formation for access control provides a comprehensive solution for managing PII access requirements effectively and securely",
        "selected_answer": "A"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 12:03",
        "comment": "Option A, using Amazon Athena with AWS Lake Formation, is the most suitable solution. Lake Formation is designed to provide fine-grained access control to data lakes stored in S3 and integrates well with Athena, thereby meeting the requirements with the least effort.\nhttps://aws.amazon.com/blogs/big-data/anonymize-and-manage-data-in-your-data-lake-with-amazon-athena-and-aws-lake-formation/",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132744-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 65 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 65,
    "question_text": "A data engineer must build an extract, transform, and load (ETL) pipeline to process and load data from 10 source systems into 10 tables that are in an Amazon Redshift database. All the source systems generate .csv, JSON, or Apache Parquet files every 15 minutes. The source systems all deliver files into one Amazon S3 bucket. The file sizes range from 10 MB to 20 GB. The ETL pipeline must function correctly despite changes to the data schema.\nWhich data pipeline solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use an Amazon EventBridge rule to run an AWS Glue job every 15 minutes. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables."
      },
      {
        "letter": "B",
        "text": "Use an Amazon EventBridge rule to invoke an AWS Glue workflow job every 15 minutes. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables."
      },
      {
        "letter": "C",
        "text": "Configure an AWS Lambda function to invoke an AWS Glue crawler when a file is loaded into the S3 bucket. Configure an AWS Glue job to process and load the data into the Amazon Redshift tables. Create a second Lambda function to run the AWS Glue job. Create an Amazon EventBridge rule to invoke the second Lambda function when the AWS Glue crawler finishes running successfully."
      },
      {
        "letter": "D",
        "text": "Configure an AWS Lambda function to invoke an AWS Glue workflow when a file is loaded into the S3 bucket. Configure the AWS Glue workflow to have an on-demand trigger that runs an AWS Glue crawler and then runs an AWS Glue job when the crawler finishes running successfully. Configure the AWS Glue job to process and load the data into the Amazon Redshift tables."
      },
      {
        "letter": "E",
        "text": "Configure an AWS Lambda function to invoke an AWS Glue job when a file is loaded into the S3 bucket. Configure the AWS Glue job to read the files from the S3 bucket into an Apache Spark DataFrame. Configure the AWS Glue job to also put smaller partitions of the DataFrame into an Amazon Kinesis Data Firehose delivery stream. Configure the delivery stream to load data into the Amazon Redshift tables."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Sun 04 Aug 2024 15:16",
        "comment": "Option B: Amazon EventBridge Rule with AWS Glue Workflow Job Every 15 Minutes - for its streamlined process, automated scheduling, and ability to handle schema changes.\n\nOption D: AWS Lambda to Invoke AWS Glue Workflow When a File is Loaded - for its responsiveness to file arrival and adaptability to schema changes, though it is slightly more complex than option B.",
        "selected_answer": "BD"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Thu 19 Dec 2024 18:03",
        "comment": "change od schema is the key",
        "selected_answer": "BD"
      },
      {
        "author": "valuedate",
        "date": "Mon 25 Nov 2024 16:27",
        "comment": "eventbridge rule or event trigger",
        "selected_answer": "BD"
      },
      {
        "author": "Ousseyni",
        "date": "Wed 16 Oct 2024 23:47",
        "comment": "ChatCGT sid A and E",
        "selected_answer": "AE"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 09:24",
        "comment": "eventbridge rule or event trigger",
        "selected_answer": "BD"
      },
      {
        "author": "lucas_rfsb",
        "date": "Wed 02 Oct 2024 16:49",
        "comment": "I will go with BD",
        "selected_answer": "BD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132694-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 66 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 66,
    "question_text": "A financial company wants to use Amazon Athena to run on-demand SQL queries on a petabyte-scale dataset to support a business intelligence (BI) application. An AWS Glue job that runs during non-business hours updates the dataset once every day. The BI application has a standard data refresh frequency of 1 hour to comply with company policies.\nA data engineer wants to cost optimize the company's use of Amazon Athena without adding any additional infrastructure costs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure an Amazon S3 Lifecycle policy to move data to the S3 Glacier Deep Archive storage class after 1 day."
      },
      {
        "letter": "B",
        "text": "Use the query result reuse feature of Amazon Athena for the SQL queries."
      },
      {
        "letter": "C",
        "text": "Add an Amazon ElastiCache cluster between the BI application and Athena."
      },
      {
        "letter": "D",
        "text": "Change the format of the files that are in the dataset to Apache Parquet."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 13:31",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html\nUse the Query Result Reuse Feature of Amazon Athena. This leverages Athena's built-in feature to reduce redundant data scans and thus lowers query costs.",
        "selected_answer": "B"
      },
      {
        "author": "TheWheelsofSteel",
        "date": "Thu 31 Jul 2025 18:36",
        "comment": "Correct answer is B!\nA -> glacier is not queryable from Athena\nC -> Adding Elasticache means adding \"infrastructure\"\nD -> Transforming the data format could work but adding operation overhead",
        "selected_answer": "B"
      },
      {
        "author": "Ell89",
        "date": "Tue 25 Feb 2025 16:07",
        "comment": "D\nquery result reuse will benefit the same queries that are being re-run, it wont benefit new queries. parquet will benefit all queries.",
        "selected_answer": "D"
      },
      {
        "author": "rsmf",
        "date": "Tue 22 Oct 2024 13:03",
        "comment": "Why not D? The question specifies the option with the least overhead, and it clearly states that the Glue job runs once a day. Since the data for that day will not change, there’s no need for additional overhead.",
        "selected_answer": "B"
      },
      {
        "author": "Ousseyni",
        "date": "Tue 16 Apr 2024 23:52",
        "comment": "B. Use the query result reuse feature of Amazon Athena for the SQL queries.",
        "selected_answer": "B"
      },
      {
        "author": "FuriouZ",
        "date": "Wed 27 Mar 2024 14:12",
        "comment": "It's B: Glacier adds more retrieval time and the other options cost some money",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132695-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 67 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 67,
    "question_text": "A company's data engineer needs to optimize the performance of table SQL queries. The company stores data in an Amazon Redshift cluster. The data engineer cannot increase the size of the cluster because of budget constraints.\nThe company stores the data in multiple tables and loads the data by using the EVEN distribution style. Some tables are hundreds of gigabytes in size. Other tables are less than 10 MB in size.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Keep using the EVEN distribution style for all tables. Specify primary and foreign keys for all tables."
      },
      {
        "letter": "B",
        "text": "Use the ALL distribution style for large tables. Specify primary and foreign keys for all tables."
      },
      {
        "letter": "C",
        "text": "Use the ALL distribution style for rarely updated small tables. Specify primary and foreign keys for all tables."
      },
      {
        "letter": "D",
        "text": "Specify a combination of distribution, sort, and partition keys for all tables."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 13:39",
        "comment": "Use the ALL Distribution Style for Rarely Updated Small Tables. This approach optimizes the performance of joins involving these smaller tables and is a common best practice in Redshift data warehousing. For the larger tables, maintaining the EVEN distribution style or considering a KEY-based distribution (if there are common join columns) could be more appropriate.",
        "selected_answer": "C"
      },
      {
        "author": "Tester_TKK",
        "date": "Sun 20 Apr 2025 22:34",
        "comment": "D is wrong. There is no partition key in Redshift",
        "selected_answer": "C"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 01:23",
        "comment": "For small tables (less than 10 MB in size) that are rarely updated, using the ALL distribution style can provide better query performance. With the ALL distribution style, each compute node stores a copy of the entire table, eliminating the need for data redistribution or shuffling during certain queries. This can significantly improve query performance, especially for joins and aggregations involving small tables.",
        "selected_answer": "C"
      },
      {
        "author": "DevoteamAnalytix",
        "date": "Mon 06 May 2024 13:44",
        "comment": "\"ALL distribution is appropriate only for relatively slow moving tables; that is, tables that are not updated frequently or extensively.\" (https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html)",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/135424-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 68 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 68,
    "question_text": "A company receives .csv files that contain physical address data. The data is in columns that have the following names: Door_No, Street_Name, City, and Zip_Code. The company wants to create a single column to store these values in the following format:\nWhich solution will meet this requirement with the LEAST coding effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue DataBrew to read the files. Use the NEST_TO_ARRAY transformation to create the new column."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue DataBrew to read the files. Use the NEST_TO_MAP transformation to create the new column."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue DataBrew to read the files. Use the PIVOT transformation to create the new column."
      },
      {
        "letter": "D",
        "text": "Write a Lambda function in Python to read the files. Use the Python data dictionary type to create the new column."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "FuriouZ",
        "date": "Sun 24 Mar 2024 10:29",
        "comment": "NEST_TO_ARRAY would result in:\n[  {\"key\": \"key1\", \"value\": \"value1\"},  {\"key\": \"key2\", \"value\": \"value2\"},  {\"key\": \"key3\", \"value\": \"value3\"}]\n\nwhile NEST_TO_MAP results: {\n  \"key1\": \"value1\",\n  \"key2\": \"value2\",\n  \"key3\": \"value3\"\n}\nTherefore go with B",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 01:26",
        "comment": "The NEST_TO_MAP transformation is specifically designed to convert data from nested structures (like rows in a CSV) into key-value pairs,\nperfectly matching the requirement of creating a new column with address components as key-value pairs",
        "selected_answer": "B"
      },
      {
        "author": "Ousseyni",
        "date": "Tue 16 Apr 2024 23:58",
        "comment": "AWS Glue DataBrew is a visual data preparation tool that allows for easy transformation of data without requiring extensive coding. The NEST_TO_MAP transformation in DataBrew allows you to convert columns into a JSON map, which aligns with the desired JSON format for the address data.",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 12:16",
        "comment": "Come on guys. That's and array there so...",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132696-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 69 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 69,
    "question_text": "A company receives call logs as Amazon S3 objects that contain sensitive customer information. The company must protect the S3 objects by using encryption. The company must also use encryption keys that only specific employees can access.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS CloudHSM cluster to store the encryption keys. Configure the process that writes to Amazon S3 to make calls to CloudHSM to encrypt and decrypt the objects. Deploy an IAM policy that restricts access to the CloudHSM cluster."
      },
      {
        "letter": "B",
        "text": "Use server-side encryption with customer-provided keys (SSE-C) to encrypt the objects that contain customer information. Restrict access to the keys that encrypt the objects."
      },
      {
        "letter": "C",
        "text": "Use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects."
      },
      {
        "letter": "D",
        "text": "Use server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the Amazon S3 managed keys that encrypt the objects."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ousseyni",
        "date": "Thu 17 Oct 2024 00:00",
        "comment": "C. Use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the objects that contain customer information. Configure an IAM policy that restricts access to the KMS keys that encrypt the objects.\n\nServer-side encryption with AWS KMS (SSE-KMS) provides strong encryption for S3 objects while allowing fine-grained access control through AWS Key Management Service (KMS). With SSE-KMS, you can control access to encryption keys using IAM policies, ensuring that only specific employees can access them.\n\nThis solution requires minimal effort as it leverages AWS's managed encryption service (SSE-KMS) and integrates seamlessly with S3. Additionally, IAM policies can be easily configured to restrict access to the KMS keys, providing granular control over who can access the encryption keys.",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 09:56",
        "comment": "Encryption at Rest: SSE-KMS provides robust encryption of the sensitive call log data while it's stored in S3.\nKey Management and Access Control: AWS KMS offers centralized key management. You can easily create and manage KMS keys (Customer Master Keys - CMKs) and use fine-grained IAM policies to restrict access to specific employees.\nMinimal Effort: SSE-KMS is a built-in S3 feature. Enabling it requires minimal configuration and no custom code for encryption/decryption.",
        "selected_answer": "C"
      },
      {
        "author": "FuriouZ",
        "date": "Fri 27 Sep 2024 15:23",
        "comment": "KMS because you can restrict access and of course for pricing ;)",
        "selected_answer": "C"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 11:25",
        "comment": "Least effort = C",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 12:47",
        "comment": "Option D does not provide the ability to restrict access to the encryption keys",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132697-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 70 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 70,
    "question_text": "A company stores petabytes of data in thousands of Amazon S3 buckets in the S3 Standard storage class. The data supports analytics workloads that have unpredictable and variable data access patterns.\nThe company does not access some data for months. However, the company must be able to retrieve all data within milliseconds. The company needs to optimize S3 storage costs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use S3 Storage Lens standard metrics to determine when to move objects to more cost-optimized storage classes. Create S3 Lifecycle policies for the S3 buckets to move objects to cost-optimized storage classes. Continue to refine the S3 Lifecycle policies in the future to optimize storage costs."
      },
      {
        "letter": "B",
        "text": "Use S3 Storage Lens activity metrics to identify S3 buckets that the company accesses infrequently. Configure S3 Lifecycle rules to move objects from S3 Standard to the S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier storage classes based on the age of the data."
      },
      {
        "letter": "C",
        "text": "Use S3 Intelligent-Tiering. Activate the Deep Archive Access tier."
      },
      {
        "letter": "D",
        "text": "Use S3 Intelligent-Tiering. Use the default access tier."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 12:35",
        "comment": "Although C is more cost-effective, because of \"must be able to retrieve all data within milliseconds\" will go with D",
        "selected_answer": "D"
      },
      {
        "author": "andrologin",
        "date": "Sun 14 Jul 2024 10:24",
        "comment": "Based on this docs https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html\nD will be appropriate as it allows for instant retrieval",
        "selected_answer": "D"
      },
      {
        "author": "rpwags",
        "date": "Sat 22 Jun 2024 19:52",
        "comment": "Staying with \"D\"... The Amazon S3 Glacier Deep Archive storage class is designed for long-term data archiving where data retrieval times are flexible. It does not offer millisecond retrieval times. Instead, data retrieval from S3 Glacier Deep Archive typically takes 12 hours or more. For millisecond retrieval times, you would use the S3 Standard, S3 Standard-IA, or S3 One Zone-IA storage classes, which are designed for frequent or infrequent access with low latency.",
        "selected_answer": "D"
      },
      {
        "author": "raghumvj",
        "date": "Mon 22 Apr 2024 17:05",
        "comment": "I am confused with C or D",
        "selected_answer": "D"
      },
      {
        "author": "chris_spencer",
        "date": "Thu 18 Apr 2024 14:04",
        "comment": "C is correct.\n\n\"Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds.\"\nhttps://aws.amazon.com/s3/storage-classes/glacier/instant-retrieval/",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 09:58",
        "comment": "least operation overhead, D",
        "selected_answer": "D"
      },
      {
        "author": "helpaws",
        "date": "Sat 16 Mar 2024 05:58",
        "comment": "Amazon S3 Glacier Instant Retrieval is an archive storage class that delivers the lowest-cost storage for long-lived data that is rarely accessed and requires retrieval in milliseconds",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 13:56",
        "comment": "Option D, using S3 Intelligent-Tiering with the default access tier, will meet the requirements best. It provides a hands-off approach to storage cost optimization while ensuring that data is available for analytics workloads within the required timeframe.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132698-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 71 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 71,
    "question_text": "During a security review, a company identified a vulnerability in an AWS Glue job. The company discovered that credentials to access an Amazon Redshift cluster were hard coded in the job script.\nA data engineer must remediate the security vulnerability in the AWS Glue job. The solution must securely store the credentials.\nWhich combination of steps should the data engineer take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Store the credentials in the AWS Glue job parameters."
      },
      {
        "letter": "B",
        "text": "Store the credentials in a configuration file that is in an Amazon S3 bucket."
      },
      {
        "letter": "C",
        "text": "Access the credentials from a configuration file that is in an Amazon S3 bucket by using the AWS Glue job."
      },
      {
        "letter": "D",
        "text": "Store the credentials in AWS Secrets Manager."
      },
      {
        "letter": "E",
        "text": "Grant the AWS Glue job IAM role access to the stored credentials."
      }
    ],
    "correct_answer": "DE",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 11:37",
        "comment": "D because it's AWS best practice for securing creds and E because after you put cred in secrets you will need permissions for accesing",
        "selected_answer": "DE"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 13:01",
        "comment": "D. Store the credentials in AWS Secrets Manager: AWS Secrets Manager is a service that helps you protect access to your applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure. It's specifically designed for storing and retrieving credentials securely, and therefore, it is an appropriate choice for handling the Redshift cluster credentials.\n\nE. Grant the AWS Glue job IAM role access to the stored credentials: IAM roles for AWS Glue will allow the job to assume a role with the necessary permissions to access the credentials in AWS Secrets Manager. This method avoids embedding credentials directly in the script or a configuration file and allows for centralized management of the credentials.",
        "selected_answer": "DE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132699-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 72 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 72,
    "question_text": "A data engineer uses Amazon Redshift to run resource-intensive analytics processes once every month. Every month, the data engineer creates a new Redshift provisioned cluster. The data engineer deletes the Redshift provisioned cluster after the analytics processes are complete every month. Before the data engineer deletes the cluster each month, the data engineer unloads backup data from the cluster to an Amazon S3 bucket.\nThe data engineer needs a solution to run the monthly analytics processes that does not require the data engineer to manage the infrastructure manually.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Step Functions to pause the Redshift cluster when the analytics processes are complete and to resume the cluster to run new processes every month."
      },
      {
        "letter": "B",
        "text": "Use Amazon Redshift Serverless to automatically process the analytics workload."
      },
      {
        "letter": "C",
        "text": "Use the AWS CLI to automatically process the analytics workload."
      },
      {
        "letter": "D",
        "text": "Use AWS CloudFormation templates to automatically process the analytics workload."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 10:04",
        "comment": "Fully Managed, Serverless: Redshift Serverless eliminates the need to manually create, manage, or delete clusters. It automatically scales resources based on the workload, reducing operational overhead significantly.\nCost-Effective for Infrequent Workloads: Since the analytics processes run only once a month, Redshift Serverless's pay-per-use model is ideal for minimizing costs during downtime.\nSeamless S3 Integration: Redshift Serverless natively integrates with S3 for backup and restore operations, ensuring compatibility with the existing process.",
        "selected_answer": "B"
      },
      {
        "author": "4c78df0",
        "date": "Wed 27 Nov 2024 15:00",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 11:38",
        "comment": "\"does not require to manage the infrastructure manually\" = Serverless",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 13:04",
        "comment": "Use Amazon Redshift Serverless. This option allows the data engineer to focus on the analytics processes themselves without worrying about cluster provisioning, scaling, or management. It provides an on-demand, serverless solution that can handle variable workloads and is cost-effective for intermittent and irregular processing needs like those described.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132700-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 73 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 73,
    "question_text": "A company receives a daily file that contains customer data in .xls format. The company stores the file in Amazon S3. The daily file is approximately 2 GB in size.\nA data engineer concatenates the column in the file that contains customer first names and the column that contains customer last names. The data engineer needs to determine the number of distinct customers in the file.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Create and run an Apache Spark job in an AWS Glue notebook. Configure the job to read the S3 file and calculate the number of distinct customers."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue crawler to create an AWS Glue Data Catalog of the S3 file. Run SQL queries from Amazon Athena to calculate the number of distinct customers."
      },
      {
        "letter": "C",
        "text": "Create and run an Apache Spark job in Amazon EMR Serverless to calculate the number of distinct customers."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue DataBrew to create a recipe that uses the COUNT_DISTINCT aggregate function to calculate the number of distinct customers."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:07",
        "comment": "AWS Glue DataBrew: AWS Glue DataBrew is a visual data preparation tool that allows data engineers and data analysts to clean and normalize data without writing code. Using DataBrew, a data engineer could create a recipe that includes the concatenation of the customer first and last names and then use the COUNT_DISTINCT function. This would not require complex code and could be performed through the DataBrew user interface, representing a lower operational effort.",
        "selected_answer": "D"
      },
      {
        "author": "Juan_pc",
        "date": "Wed 07 May 2025 14:15",
        "comment": "According to the official DataBrew documentation, it does not natively support files in .xls format (it does support .xlsx).\nThe correct option is A.",
        "selected_answer": "A"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 01:42",
        "comment": "DataBrew supports various transformations,\nincluding the COUNT_DISTINCT function, which is ideal for calculating the number of unique values in a column (combined first and last names in this case).",
        "selected_answer": "D"
      },
      {
        "author": "Ousseyni",
        "date": "Wed 17 Apr 2024 13:30",
        "comment": "go in D",
        "selected_answer": "D"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 20:50",
        "comment": "since it's less operational effort, I would go in D",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132765-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 74 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 74,
    "question_text": "A healthcare company uses Amazon Kinesis Data Streams to stream real-time health data from wearable devices, hospital equipment, and patient records.\nA data engineer needs to find a solution to process the streaming data. The data engineer needs to store the data in an Amazon Redshift Serverless warehouse. The solution must support near real-time analytics of the streaming data and the previous day's data.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Load data into Amazon Kinesis Data Firehose. Load the data into Amazon Redshift."
      },
      {
        "letter": "B",
        "text": "Use the streaming ingestion feature of Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Load the data into Amazon S3. Use the COPY command to load the data into Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Use the Amazon Aurora zero-ETL integration with Amazon Redshift."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rralucard_",
        "date": "Sun 04 Feb 2024 09:05",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html\nUse the Streaming Ingestion Feature of Amazon Redshift: Amazon Redshift recently introduced streaming data ingestion, allowing Redshift to consume data directly from Kinesis Data Streams in near real-time. This feature simplifies the architecture by eliminating the need for intermediate steps or services, and it is specifically designed to support near real-time analytics. The operational overhead is minimal since the feature is integrated within Redshift.",
        "selected_answer": "B"
      },
      {
        "author": "4c78df0",
        "date": "Mon 27 May 2024 14:01",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 20:53",
        "comment": "I'd go in B",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132701-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 75 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 75,
    "question_text": "A data engineer needs to use an Amazon QuickSight dashboard that is based on Amazon Athena queries on data that is stored in an Amazon S3 bucket. When the data engineer connects to the QuickSight dashboard, the data engineer receives an error message that indicates insufficient permissions.\nWhich factors could cause to the permissions-related errors? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "There is no connection between QuickSight and Athena."
      },
      {
        "letter": "B",
        "text": "The Athena tables are not cataloged."
      },
      {
        "letter": "C",
        "text": "QuickSight does not have access to the S3 bucket."
      },
      {
        "letter": "D",
        "text": "QuickSight does not have access to decrypt S3 data."
      },
      {
        "letter": "E",
        "text": "There is no IAM role assigned to QuickSight."
      }
    ],
    "correct_answer": "CD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "fceb2c1",
        "date": "Sun 24 Mar 2024 12:32",
        "comment": "C and D\nhttps://docs.aws.amazon.com/quicksight/latest/user/troubleshoot-athena-insufficient-permissions.html\n\nE is incorrect because it will result in authentication/authorization error, not insufficient permission error.",
        "selected_answer": "CD"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:16",
        "comment": "C. QuickSight does not have access to the S3 bucket: Amazon QuickSight needs to have the necessary permissions to access the S3 bucket where the data resides. If QuickSight lacks the permissions to read the data from the S3 bucket, it would result in an error indicating insufficient permissions.\n\nD. QuickSight does not have access to decrypt S3 data: If the data in S3 is encrypted, QuickSight needs permissions to use the necessary keys to decrypt the data. Without access to the decryption keys, typically managed by AWS Key Management Service (KMS), QuickSight cannot read the encrypted data and would give an error.",
        "selected_answer": "CD"
      },
      {
        "author": "bonds",
        "date": "Sat 19 Apr 2025 23:03",
        "comment": "QuickSight requires explicit permission to connect to Athena\nThis connection must be established during QuickSight setup\nWithout this connection, QuickSight cannot execute Athena queries\nResults in permissions-related errors",
        "selected_answer": "AC"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 09:59",
        "comment": "C. QuickSight does not have access to the S3 bucket. Amazon QuickSight needs to have the necessary permissions to access the Amazon S3 bucket where the data is stored. If these permissions are not correctly configured, QuickSight will not be able to access the data, resulting in an error.\n\nE. There is no IAM role assigned to QuickSight. Amazon QuickSight uses AWS Identity and Access Management (IAM) roles to access AWS resources. If QuickSight is not assigned an IAM role, or if the assigned role does not have the necessary permissions, QuickSight will not be able to access the resources it needs, leading to an error.",
        "selected_answer": "CE"
      },
      {
        "author": "Ousseyni",
        "date": "Wed 17 Apr 2024 13:34",
        "comment": "C and D",
        "selected_answer": "CD"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 10:11",
        "comment": "The two most likely factors causing the permissions-related errors are:\n\nC. QuickSight does not have access to the S3 bucket. To access data from an S3 bucket, QuickSight needs explicit S3 permissions. This is typically handled through an IAM role associated with the QuickSight service.\nD. QuickSight does not have access to decrypt S3 data. If the data in S3 is encrypted (e.g., using KMS), QuickSight must have the necessary permissions to decrypt the data using the relevant KMS key.\nLet's analyze why the other options are less likely the primary culprits:\n\n\nE. There is no IAM role assigned to QuickSight. QuickSight needs an IAM role for overall functionality. A missing role would likely cause broader service failures, not specific data access errors.",
        "selected_answer": "CD"
      },
      {
        "author": "taka5094",
        "date": "Sun 17 Mar 2024 23:16",
        "comment": "I think the assumptions in the problem are insufficient. If the data is encrypted, then D can be the correct answer, but if not, then E is the correct answer.",
        "selected_answer": "CE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132702-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 76 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 76,
    "question_text": "A company stores datasets in JSON format and .csv format in an Amazon S3 bucket. The company has Amazon RDS for Microsoft SQL Server databases, Amazon DynamoDB tables that are in provisioned capacity mode, and an Amazon Redshift cluster. A data engineering team must develop a solution that will give data scientists the ability to query all data sources by using syntax similar to SQL.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Amazon Athena to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use Redshift Spectrum to query the data. Use SQL for structured data sources. Use PartiQL for data that is stored in JSON format."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue to crawl the data sources. Store metadata in the AWS Glue Data Catalog. Use AWS Glue jobs to transform data that is in JSON format to Apache Parquet or .csv format. Store the transformed data in an S3 bucket. Use Amazon Athena to query the original and transformed data from the S3 bucket."
      },
      {
        "letter": "D",
        "text": "Use AWS Lake Formation to create a data lake. Use Lake Formation jobs to transform the data from all data sources to Apache Parquet format. Store the transformed data in an S3 bucket. Use Amazon Athena or Redshift Spectrum to query the data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 14:19",
        "comment": "LEAST operational overhead? query straight with Athena without any intermediate actions or services",
        "selected_answer": "A"
      },
      {
        "author": "ToyaG",
        "date": "Wed 08 Oct 2025 22:58",
        "comment": "AWS Glue Data Catalog: This provides a centralized metadata repository for all data sources, including S3, RDS, DynamoDB, and Redshift. AWS Glue crawlers can automatically infer schemas and populate the Data Catalog, minimizing manual effort.\nAmazon Athena: Athena allows data scientists to query data directly from S3 using standard SQL. For structured data in RDS and Redshift, SQL can be used. For JSON data in S3 and DynamoDB, Athena supports PartiQL, which is a SQL-compatible query language for semi-structured data.\nLeast Operational Overhead: This approach leverages serverless services like AWS Glue and Amazon Athena, which eliminates the need to provision, manage, or scale servers, significantly reducing operational overhead compared to solutions involving managed clusters or data transformations.",
        "selected_answer": "A"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 01:54",
        "comment": "thena natively supports querying JSON data stored in S3 using standard SQL functions.\nThis eliminates the need for additional data transformation steps using Glue jobs (as required in Option C or D).",
        "selected_answer": "A"
      },
      {
        "author": "VerRi",
        "date": "Wed 22 May 2024 16:10",
        "comment": "B requires Redshift Spectrum, so A",
        "selected_answer": "A"
      },
      {
        "author": "chris_spencer",
        "date": "Thu 18 Apr 2024 14:14",
        "comment": "Answer should be C.\n\nAmazon Athena does not support querying with PartiQL until 16.04.2024, https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-athena-federated-query-pass-through/\n\nThe DEA01 exam should not have include the latest feature",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 10:16",
        "comment": "A. Unified Querying with Athena: Athena provides a SQL-like interface for querying various data sources, including JSON and CSV in S3, as well as traditional databases.\nPartiQL Support: Athena's PartiQL extension allows querying semi-structured JSON data directly, eliminating the need for a separate query engine.\nServerless and Managed: Both AWS Glue and Athena are serverless, minimizing infrastructure management for the data engineers.\nNo Unnecessary Transformations: Avoiding transformations for JSON data simplifies the pipeline and reduces operational overhead.\nB. Redshift Spectrum: While Spectrum can query external data, it's primarily intended for Redshift data warehouse extensions. It adds complexity for the RDS and DynamoDB data sources.",
        "selected_answer": "A"
      },
      {
        "author": "lucas_rfsb",
        "date": "Wed 03 Apr 2024 21:50",
        "comment": "I will go with B",
        "selected_answer": "B"
      },
      {
        "author": "halogi",
        "date": "Thu 28 Mar 2024 04:09",
        "comment": "AWS Athena can only query in SQL, not PartiQL, so both A and B are incorrect. LakeFormation can not work directly with DynamoDB, so D is incorrect. \nThe only acceptable answer is C",
        "selected_answer": "C"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:20",
        "comment": "Option A, using AWS Glue and Amazon Athena, would meet the requirements with the least operational overhead. This solution allows data scientists to directly query data in its original format without the need for additional data transformation steps, making it easier to implement and manage.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132703-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 77 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 77,
    "question_text": "A data engineer is configuring Amazon SageMaker Studio to use AWS Glue interactive sessions to prepare data for machine learning (ML) models.\nThe data engineer receives an access denied error when the data engineer tries to prepare the data by using SageMaker Studio.\nWhich change should the engineer make to gain access to SageMaker Studio?",
    "choices": [
      {
        "letter": "A",
        "text": "Add the AWSGlueServiceRole managed policy to the data engineer's IAM user."
      },
      {
        "letter": "B",
        "text": "Add a policy to the data engineer's IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy."
      },
      {
        "letter": "C",
        "text": "Add the AmazonSageMakerFullAccess managed policy to the data engineer's IAM user."
      },
      {
        "letter": "D",
        "text": "Add a policy to the data engineer's IAM user that allows the sts:AddAssociation action for the AWS Glue and SageMaker service principals in the trust policy."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Tue 28 May 2024 12:24",
        "comment": "I don't believe you're supposed to assign a FullAccess policy, so I will go with B.",
        "selected_answer": "B"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 14:34",
        "comment": "I will go with B since you can get access denied even with the AmazonSageMakerFullAccess.\n See here: https://stackoverflow.com/questions/64709871/aws-sagemaker-studio-createdomain-access-error",
        "selected_answer": "B"
      },
      {
        "author": "mohamedTR",
        "date": "Mon 07 Oct 2024 10:08",
        "comment": "B. the engineer needs to assume specific roles to allow interaction between these services. The sts:AssumeRole action is necessary for this purpose",
        "selected_answer": "B"
      },
      {
        "author": "junrun3",
        "date": "Sat 24 Aug 2024 22:56",
        "comment": "B, this approach involves setting up the trust relationship for roles. It is not a typical requirement for resolving access issues with SageMaker Studio directly.",
        "selected_answer": "C"
      },
      {
        "author": "Christina666",
        "date": "Sat 13 Apr 2024 10:22",
        "comment": "SageMaker Permissions: The AmazonSageMakerFullAccess managed policy provides broad permissions for using Amazon SageMaker features, including SageMaker Studio and the ability to interact with other AWS services like AWS Glue.\nLeast Privilege: While this policy is quite permissive, it's the most direct solution to the immediate access issue. After resolving the error, you can refine permissions for a more granular approach.",
        "selected_answer": "C"
      },
      {
        "author": "lucas_rfsb",
        "date": "Wed 03 Apr 2024 21:56",
        "comment": "I will go with C",
        "selected_answer": "C"
      },
      {
        "author": "atu1789",
        "date": "Sat 17 Feb 2024 06:43",
        "comment": "B. Add a policy to the data engineer’s IAM user that includes the sts:AssumeRole action for the AWS Glue and SageMaker service principals in the trust policy.\n\n\t•\tThis is the most appropriate solution. The sts:AssumeRole action allows the data engineer’s IAM user to assume a role that has the necessary permissions for both AWS Glue and SageMaker. This is a common approach for granting cross-service access in AWS.",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:23",
        "comment": "Amazon SageMaker requires permissions to perform actions on your behalf. By attaching the AmazonSageMakerFullAccess managed policy to the data engineer’s IAM user, you grant the necessary permissions for SageMaker Studio to access AWS Glue and other related services.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132706-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 78 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 78,
    "question_text": "A company extracts approximately 1 TB of data every day from data sources such as SAP HANA, Microsoft SQL Server, MongoDB, Apache Kafka, and Amazon DynamoDB. Some of the data sources have undefined data schemas or data schemas that change.\nA data engineer must implement a solution that can detect the schema for these data sources. The solution must extract, transform, and load the data to an Amazon S3 bucket. The company has a service level agreement (SLA) to load the data into the S3 bucket within 15 minutes of data creation.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon EMR to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark."
      },
      {
        "letter": "C",
        "text": "Create a PySpark program in AWS Lambda to extract, transform, and load the data into the S3 bucket."
      },
      {
        "letter": "D",
        "text": "Create a stored procedure in Amazon Redshift to detect the schema and to extract, transform, and load the data into a Redshift Spectrum table. Access the table from Amazon S3."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "GiorgioGss",
        "date": "Thu 19 Sep 2024 13:35",
        "comment": "Least effort = B",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Aug 2024 13:28",
        "comment": "B. Use AWS Glue to detect the schema and to extract, transform, and load the data into the S3 bucket. Create a pipeline in Apache Spark.",
        "selected_answer": "B"
      },
      {
        "author": "Christina666",
        "date": "Sun 13 Oct 2024 10:26",
        "comment": "Glue ETL",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132707-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 79 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 79,
    "question_text": "A company has multiple applications that use datasets that are stored in an Amazon S3 bucket. The company has an ecommerce application that generates a dataset that contains personally identifiable information (PII). The company has an internal analytics application that does not require access to the PII.\nTo comply with regulations, the company must not share PII unnecessarily. A data engineer needs to implement a solution that with redact PII dynamically, based on the needs of each application that accesses the dataset.\nWhich solution will meet the requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an S3 bucket policy to limit the access each application has. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy."
      },
      {
        "letter": "B",
        "text": "Create an S3 Object Lambda endpoint. Use the S3 Object Lambda endpoint to read data from the S3 bucket. Implement redaction logic within an S3 Object Lambda function to dynamically redact PII based on the needs of each application that accesses the data."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue to transform the data for each application. Create multiple copies of the dataset. Give each dataset copy the appropriate level of redaction for the needs of the application that accesses the copy."
      },
      {
        "letter": "D",
        "text": "Create an API Gateway endpoint that has custom authorizers. Use the API Gateway endpoint to read data from the S3 bucket. Initiate a REST API call to dynamically redact PII based on the needs of each application that accesses the data."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "teo2157",
        "date": "Mon 19 Aug 2024 06:36",
        "comment": "It's B based on AWS documentation\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/transforming-objects.html",
        "selected_answer": "B"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 02:09",
        "comment": "S3 Object Lambda automatically triggers the Lambda function only when there's a request to access data in the S3 bucket. This eliminates the need for pre-processing or creating multiple data copies with varying levels of redaction (Options A and C).",
        "selected_answer": "B"
      },
      {
        "author": "4c78df0",
        "date": "Mon 27 May 2024 14:02",
        "comment": "B is correct",
        "selected_answer": "B"
      },
      {
        "author": "atu1789",
        "date": "Sat 17 Feb 2024 06:56",
        "comment": "S3 Object Lambda allows you to add custom processing, such as redaction of PII, to data retrieved from S3. This is done dynamically, meaning you don’t need to store multiple copies of the data. It’s a more efficient and operationally simpler approach compared to managing multiple dataset versions.",
        "selected_answer": "B"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:31",
        "comment": "Amazon S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it is returned to an application. For example, you could use an S3 Object Lambda to dynamically redact personally identifiable information (PII) from data retrieved from S3. This would allow you to control access to sensitive information based on the needs of different applications, without having to create and manage multiple copies of your data.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/132708-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 80 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 80,
    "question_text": "A data engineer needs to build an extract, transform, and load (ETL) job. The ETL job will process daily incoming .csv files that users upload to an Amazon S3 bucket. The size of each S3 object is less than 100 MB.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Write a custom Python application. Host the application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster."
      },
      {
        "letter": "B",
        "text": "Write a PySpark ETL script. Host the script on an Amazon EMR cluster."
      },
      {
        "letter": "C",
        "text": "Write an AWS Glue PySpark job. Use Apache Spark to transform the data."
      },
      {
        "letter": "D",
        "text": "Write an AWS Glue Python shell job. Use pandas to transform the data."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "halogi",
        "date": "Thu 28 Mar 2024 04:26",
        "comment": "AWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/",
        "selected_answer": "C"
      },
      {
        "author": "atu1789",
        "date": "Sat 17 Feb 2024 07:03",
        "comment": "Option D: Write an AWS Glue Python shell job and use pandas to transform the data, is the most cost-effective solution for the described scenario.\n\nAWS Glue’s Python shell jobs are a good fit for smaller-scale ETL tasks, especially when dealing with .csv files that are less than 100 MB each. The use of pandas, a powerful and efficient data manipulation library in Python, makes it an ideal tool for processing and transforming these types of files. This approach avoids the overhead and additional costs associated with more complex solutions like Amazon EKS or EMR, which are generally more suited for larger-scale, more complex data processing tasks.\n\nGiven the requirements – processing daily incoming small-sized .csv files – this solution provides the necessary functionality with minimal resources, aligning well with the goal of cost-effectiveness.",
        "selected_answer": "D"
      },
      {
        "author": "AminTriton",
        "date": "Sat 09 Aug 2025 12:55",
        "comment": "Spark, EKS, and EMR all cost more significantly than running Python!",
        "selected_answer": "D"
      },
      {
        "author": "YUICH",
        "date": "Mon 27 Jan 2025 12:28",
        "comment": "It is important not to compare just the “price per DPU hour,” but to consider the total cost by factoring in overhead for job startup, minimum DPU count, execution time, and data volume. For a relatively lightweight workload—such as processing approximately 100 MB of CSV files on a daily basis—option (D), using an AWS Glue Python shell job, is the most cost-effective choice.",
        "selected_answer": "D"
      },
      {
        "author": "LR2023",
        "date": "Tue 16 Jul 2024 03:13",
        "comment": "going with D https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html",
        "selected_answer": "D"
      },
      {
        "author": "pypelyncar",
        "date": "Wed 12 Jun 2024 02:19",
        "comment": "good candidate to be (2 options) for real, either spark and py have similar approaches. I would go with Pandas, although... 50/50.. it could be Spark. I hope not to find this question in the exam",
        "selected_answer": "D"
      },
      {
        "author": "VerRi",
        "date": "Tue 21 May 2024 12:18",
        "comment": "PySpark with Spark(Flexible Execution): $0.29/hr for 1 DPU\nPySpark with Spark(Standard Execution): $0.44/hr for 1 DPU\nPython Shell with Pandas: $0.44/hr for 1 DPU",
        "selected_answer": "C"
      },
      {
        "author": "cloudata",
        "date": "Sun 05 May 2024 02:50",
        "comment": "Python Shell is cheaper and can handle small to medium tasks.\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-performant-data-pipeline/additional-considerations.html",
        "selected_answer": "D"
      },
      {
        "author": "khchan123",
        "date": "Sun 28 Apr 2024 10:19",
        "comment": "D.\n\nWhile AWS Glue PySpark jobs are scalable and suitable for large workloads, C may be overkill for processing small .csv files (less than 100 MB each). The overhead of using Apache Spark may not be cost-effective for this specific use case.",
        "selected_answer": "D"
      },
      {
        "author": "Leo87656789",
        "date": "Sat 27 Apr 2024 09:32",
        "comment": "Option D:\n\nEven though the Python Shell Job is more expensive on a DPU-Hour basis, you can select the option \"1/16 DPU\" in the Job details for a Python Shell Job, which is definetly cheaper than a Pyspark job.",
        "selected_answer": "D"
      },
      {
        "author": "lucas_rfsb",
        "date": "Tue 02 Apr 2024 21:42",
        "comment": "AWS Glue Python Shell Job is billed $0.44 per DPU-Hour for each job\nAWS Glue PySpark is billed $0.29 per DPU-Hour for each job with flexible execution and $0.44 per DPU-Hour for each job with standard execution\nSource: https://aws.amazon.com/glue/pricing/",
        "selected_answer": "C"
      },
      {
        "author": "GiorgioGss",
        "date": "Tue 19 Mar 2024 14:55",
        "comment": "D is more cheaper than C. Not so scalable but is cheaper...",
        "selected_answer": "D"
      },
      {
        "author": "rralucard_",
        "date": "Fri 02 Feb 2024 14:34",
        "comment": "AWS Glue is a fully managed ETL service, which means you don't need to manage infrastructure, and it automatically scales to handle your data processing needs. This reduces operational overhead and cost.\n\nPySpark, as a part of AWS Glue, is a powerful and widely-used framework for distributed data processing, and it's well-suited for handling data transformations on a large scale.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142527-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 81 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 81,
    "question_text": "A data engineer creates an AWS Glue Data Catalog table by using an AWS Glue crawler that is named Orders. The data engineer wants to add the following new partitions:\ns3://transactions/orders/order_date=2023-01-01\ns3://transactions/orders/order_date=2023-01-02\nThe data engineer must edit the metadata to include the new partitions in the table without scanning all the folders and files in the location of the table.\nWhich data definition language (DDL) statement should the data engineer use in Amazon Athena?",
    "choices": [
      {
        "letter": "A",
        "text": "ALTER TABLE Orders ADD PARTITION(order_date=’2023-01-01’) LOCATION ‘s3://transactions/orders/order_date=2023-01-01’;ALTER TABLE Orders ADD PARTITION(order_date=’2023-01-02’) LOCATION ‘s3://transactions/orders/order_date=2023-01-02’;"
      },
      {
        "letter": "B",
        "text": "MSCK REPAIR TABLE Orders;"
      },
      {
        "letter": "C",
        "text": "REPAIR TABLE Orders;"
      },
      {
        "letter": "D",
        "text": "ALTER TABLE Orders MODIFY PARTITION(order_date=’2023-01-01’) LOCATION ‘s3://transactions/orders/2023-01-01’;ALTER TABLE Orders MODIFY PARTITION(order_date=’2023-01-02’) LOCATION ‘s3://transactions/orders/2023-01-02’;"
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ja13",
        "date": "Sun 07 Jul 2024 20:41",
        "comment": "Why the Other Options Are Incorrect:\nOption B: MSCK REPAIR TABLE Orders: This command is used to repair the partitions of a table by scanning all the files in the specified location. This is not efficient if you know the specific partitions you want to add, as it will scan the entire table location.\nOption C: REPAIR TABLE Orders: This is not a valid Athena DDL command.\nOption D: ALTER TABLE Orders MODIFY PARTITION: This command is used to modify the location of existing partitions, not to add new partitions. It would not work for adding new partitions.",
        "selected_answer": "A"
      },
      {
        "author": "artworkad",
        "date": "Fri 14 Jun 2024 18:40",
        "comment": "A is correct as per https://docs.aws.amazon.com/athena/latest/ug/alter-table-add-partition.html",
        "selected_answer": "A"
      },
      {
        "author": "tgv",
        "date": "Fri 14 Jun 2024 14:11",
        "comment": "A is correct because it uses the appropriate DDL statements to add the new partitions directly without scanning all folders and files, meeting the requirements stated in the question.\nB is incorrect because while it would update the partitions, it would involve scanning all files and folders.\nC is incorrect because REPAIR TABLE is not a valid command.\nD is incorrect because it modifies partitions instead of adding new ones.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142529-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 82 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 82,
    "question_text": "A company stores 10 to 15 TB of uncompressed .csv files in Amazon S3. The company is evaluating Amazon Athena as a one-time query engine.\nThe company wants to transform the data to optimize query runtime and storage costs.\nWhich file format and compression solution will meet these requirements for Athena queries?",
    "choices": [
      {
        "letter": "A",
        "text": ".csv format compressed with zip"
      },
      {
        "letter": "B",
        "text": "JSON format compressed with bzip2"
      },
      {
        "letter": "C",
        "text": "Apache Parquet format compressed with Snappy"
      },
      {
        "letter": "D",
        "text": "Apache Avro format compressed with LZO"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Fri 14 Jun 2024 16:25",
        "comment": "Parquet provides efficient columnar storage, enabling Athena to read only the necessary data for queries, which reduces scan times and speeds up query performance.\nSnappy compression offers a good balance between compression speed and efficiency, reducing storage costs without significantly impacting query times.",
        "selected_answer": "C"
      },
      {
        "author": "artworkad",
        "date": "Fri 14 Jun 2024 18:40",
        "comment": "Parquet + Snappy",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142558-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 83 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 83,
    "question_text": "A company uses Apache Airflow to orchestrate the company's current on-premises data pipelines. The company runs SQL data quality check tasks as part of the pipelines. The company wants to migrate the pipelines to AWS and to use AWS managed services.\nWhich solution will meet these requirements with the LEAST amount of refactoring?",
    "choices": [
      {
        "letter": "A",
        "text": "Setup AWS Outposts in the AWS Region that is nearest to the location where the company uses Airflow. Migrate the servers into Outposts hosted Amazon EC2 instances. Update the pipelines to interact with the Outposts hosted EC2 instances instead of the on-premises pipelines."
      },
      {
        "letter": "B",
        "text": "Create a custom Amazon Machine Image (AMI) that contains the Airflow application and the code that the company needs to migrate. Use the custom AMI to deploy Amazon EC2 instances. Update the network connections to interact with the newly deployed EC2 instances."
      },
      {
        "letter": "C",
        "text": "Migrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow."
      },
      {
        "letter": "D",
        "text": "Convert the pipelines to AWS Step Functions workflows. Recreate the data quality checks in SQL as Python based AWS Lambda functions."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 12:32",
        "comment": "The solution that will meet these requirements with the least amount of refactoring is Option C: Migrate the existing Airflow orchestration configuration into Amazon Managed Workflows for Apache Airflow (Amazon MWAA). Create the data quality checks during the ingestion to validate the data quality by using SQL tasks in Airflow.\n\nAmazon Managed Workflows for Apache Airflow (MWAA) is a fully managed service that makes it easy to run open-source versions of Apache Airflow on AWS. It allows you to build workflows to design and visualize pipelines, automate complex tasks, and monitor executions. Since the company is already using Apache Airflow for orchestration, migrating to Amazon MWAA would require minimal refactoring.",
        "selected_answer": "C"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 20 Jun 2024 18:14",
        "comment": "Amazon MWAA - becuase we already uses Apache Airflow",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:12",
        "comment": "Amazon MWAA is a managed service for running Apache Airflow. It allows migrating existing Airflow configurations with minimal changes. Data quality checks can continue to be implemented as SQL tasks in Airflow, similar to the current setup.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142535-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 84 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 84,
    "question_text": "A company uses Amazon EMR as an extract, transform, and load (ETL) pipeline to transform data that comes from multiple sources. A data engineer must orchestrate the pipeline to maximize performance.\nWhich AWS service will meet this requirement MOST cost effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon EventBridge"
      },
      {
        "letter": "B",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)"
      },
      {
        "letter": "C",
        "text": "AWS Step Functions"
      },
      {
        "letter": "D",
        "text": "AWS Glue Workflows"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "artworkad",
        "date": "Fri 14 Jun 2024 18:44",
        "comment": "Glue Workflows is for Glue job orchestration. C is for orchestration with different AWS services.",
        "selected_answer": "C"
      },
      {
        "author": "chrispchrisp",
        "date": "Tue 23 Jul 2024 15:31",
        "comment": "B is not cost effective, D is only to orchestrate Glue Jobs and Crawlers within AWS Glue itself. Hence C is correct, Step functions is cost effective and can link together your different AWS services.",
        "selected_answer": "C"
      },
      {
        "author": "hcong",
        "date": "Tue 20 Aug 2024 13:23",
        "comment": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is the best service for orchestrating complex data pipelines, especially for workloads already using Amazon EMR. Airflow is a powerful workflow orchestration tool that can be integrated with various AWS services, including EMR, to provide flexible scheduling, task dependency management, and monitoring capabilities. Using a hosted Airflow service (MWAA) can reduce administrative overhead while maintaining a familiar workflow orchestration environment.",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:23",
        "comment": "This is EMR not Glue workflows hence step functions\nEventBridge is best for event driven architecture",
        "selected_answer": "C"
      },
      {
        "author": "LR2023",
        "date": "Tue 16 Jul 2024 19:59",
        "comment": "https://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 12:37",
        "comment": "The most cost-effective AWS service for orchestrating an ETL pipeline that maximizes performance is D. AWS Glue Workflows.\n\nAWS Glue is a fully managed ETL service that makes it easy to move data between your data stores. AWS Glue simplifies and automates the difficult and time-consuming tasks of data discovery, conversion mapping, and job scheduling. AWS Glue Workflows allows you to orchestrate complex ETL jobs involving multiple crawlers, jobs, and triggers.\n\nWhile the other services mentioned (Amazon EventBridge, Amazon MWAA, and AWS Step Functions) can be used for workflow orchestration, they are not specifically designed for ETL workloads and may not be as cost-effective for this use case. AWS Glue is designed for ETL workloads, and its workflows feature is specifically designed for orchestrating ETL jobs, making it the most suitable and cost-effective choice.",
        "selected_answer": "D"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 20 Jun 2024 18:16",
        "comment": "C - becuase AWS Glue can be used only for glue based ETL jobs",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:18",
        "comment": "While AWS Glue Workflows are excellent for orchestrating Glue-specific ETL tasks, AWS Step Functions is more suitable for orchestrating an Amazon EMR-based ETL pipeline due to its greater flexibility, broader integration capabilities, and effective cost management. Therefore, the correct choice remains [C]",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142559-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 85 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 85,
    "question_text": "An online retail company stores Application Load Balancer (ALB) access logs in an Amazon S3 bucket. The company wants to use Amazon Athena to query the logs to analyze traffic patterns.\nA data engineer creates an unpartitioned table in Athena. As the amount of the data gradually increases, the response time for queries also increases. The data engineer wants to improve the query performance in Athena.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Glue job that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue crawler that includes a classifier that determines the schema of all ALB access logs and writes the partition metadata to AWS Glue Data Catalog."
      },
      {
        "letter": "C",
        "text": "Create an AWS Lambda function to transform all ALB access logs. Save the results to Amazon S3 in Apache Parquet format. Partition the metadata. Use Athena to query the transformed data."
      },
      {
        "letter": "D",
        "text": "Use Apache Hive to create bucketed tables. Use an AWS Lambda function to transform all ALB access logs."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "PGGuy",
        "date": "Fri 21 Jun 2024 20:03",
        "comment": "Creating an AWS Glue crawler (Option B) is the most straightforward and least operationally intensive approach to automatically determine the schema, partition the data, and keep the AWS Glue Data Catalog updated. This ensures Athena queries are optimized without requiring extensive manual management or additional processing steps.",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:26",
        "comment": "AWS Crawler with classifiers allow you to determine the schema pattern on files/data that can then be used to partition the data for Athena query optimization",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:20",
        "comment": "An AWS Glue crawler can automatically determine the schema of the logs, infer partitions, and update the Glue Data Catalog. Crawlers can be scheduled to run at intervals, minimizing manual intervention.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142560-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 86 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 86,
    "question_text": "A company has a business intelligence platform on AWS. The company uses an AWS Storage Gateway Amazon S3 File Gateway to transfer files from the company's on-premises environment to an Amazon S3 bucket.\nA data engineer needs to setup a process that will automatically launch an AWS Glue workflow to run a series of AWS Glue jobs when each file transfer finishes successfully.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Determine when the file transfers usually finish based on previous successful file transfers. Set up an Amazon EventBridge scheduled event to initiate the AWS Glue jobs at that time of day."
      },
      {
        "letter": "B",
        "text": "Set up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event."
      },
      {
        "letter": "C",
        "text": "Set up an on-demand AWS Glue workflow so that the data engineer can start the AWS Glue workflow when each file transfer is complete."
      },
      {
        "letter": "D",
        "text": "Set up an AWS Lambda function that will invoke the AWS Glue Workflow. Set up an event for the creation of an S3 object as a trigger for the Lambda function."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:25",
        "comment": "Using EventBridge directly to trigger the AWS Glue workflow upon S3 events is straightforward and leverages AWS's event-driven architecture, requiring minimal maintenance.",
        "selected_answer": "B"
      },
      {
        "author": "ToyaG",
        "date": "Wed 08 Oct 2025 23:10",
        "comment": "AWS Glue is specifically designed for ETL tasks. It provides a serverless and managed environment for building and running complex ETL jobs. Its workflows feature allows for seamless orchestration of multiple Glue jobs, crawlers, and triggers, making it the ideal choice for managing complex data pipelines.",
        "selected_answer": "B"
      },
      {
        "author": "Tester_TKK",
        "date": "Mon 21 Apr 2025 10:29",
        "comment": "EventBridge is a \"bridge\" for almost all AWS services",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:30",
        "comment": "Event driven architecture with S3 file creation can only be EventBridge",
        "selected_answer": "C"
      },
      {
        "author": "bakarys",
        "date": "Thu 04 Jul 2024 23:46",
        "comment": "Setting up an Amazon EventBridge event that initiates the AWS Glue workflow after every successful S3 File Gateway file transfer event would meet these requirements with the least operational overhead.\n\nThis solution is event-driven and does not require manual intervention or reliance on a schedule that might not align with the actual completion time of the file transfers. The AWS Glue workflow is triggered automatically when a new file is added to the S3 bucket, ensuring that the AWS Glue workflow starts processing the new data as soon as it’s available.",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 13:01",
        "comment": "The solution that will meet these requirements with the least operational overhead is Option D.\n\nSetting up an AWS Lambda function that will invoke the AWS Glue Workflow, and setting up an event for the creation of an S3 object as a trigger for the Lambda function, will ensure that the workflow is automatically initiated each time a file transfer is successfully completed. This approach requires minimal operational overhead as it automates the process and does not require manual intervention or scheduling based on estimated completion times.\n\nOptions A and C involve manual intervention or assumptions about transfer times, which could lead to inefficiencies or inaccuracies. Option B is not feasible because Amazon EventBridge does not directly support triggering events based on S3 File Gateway file transfer events. Therefore, Option D is the most suitable solution.",
        "selected_answer": "D"
      },
      {
        "author": "PGGuy",
        "date": "Fri 21 Jun 2024 20:04",
        "comment": "Setting up an Amazon EventBridge event (Option B) to initiate the AWS Glue workflow after every successful S3 File Gateway file transfer event is the most efficient solution. It provides real-time automation with minimal operational overhead, ensuring that the Glue workflow starts immediately after the file transfer is complete.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142537-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 87 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 87,
    "question_text": "A retail company uses Amazon Aurora PostgreSQL to process and store live transactional data. The company uses an Amazon Redshift cluster for a data warehouse.\nAn extract, transform, and load (ETL) job runs every morning to update the Redshift cluster with new data from the PostgreSQL database. The company has grown rapidly and needs to cost optimize the Redshift cluster.\nA data engineer needs to create a solution to archive historical data. The data engineer must be able to run analytics queries that effectively combine data from live transactional data in PostgreSQL, current data in Redshift, and archived historical data. The solution must keep only the most recent 15 months of data in Amazon Redshift to reduce costs.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the Amazon Redshift Federated Query feature to query live transactional data that is in the PostgreSQL database."
      },
      {
        "letter": "B",
        "text": "Configure Amazon Redshift Spectrum to query live transactional data that is in the PostgreSQL database."
      },
      {
        "letter": "C",
        "text": "Schedule a monthly job to copy data that is older than 15 months to Amazon S3 by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Amazon Redshift Spectrum to access historical data in Amazon S3."
      },
      {
        "letter": "D",
        "text": "Schedule a monthly job to copy data that is older than 15 months to Amazon S3 Glacier Flexible Retrieval by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Redshift Spectrum to access historical data from S3 Glacier Flexible Retrieval."
      },
      {
        "letter": "E",
        "text": "Create a materialized view in Amazon Redshift that combines live, current, and historical data from different sources."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "ToyaG",
        "date": "Wed 08 Oct 2025 23:14",
        "comment": "A and C\nThis step addresses the cost optimization requirement by moving older data out of the Redshift cluster and into a more cost-effective storage solution like Amazon S3. Redshift Spectrum then allows analytical queries to be run directly against this data in S3, effectively combining it with data still residing in Redshift.\n\nThis step addresses the requirement to combine live transactional data from Aurora PostgreSQL with current and historical data for analytics. Redshift Federated Query enables Redshift to directly query data in external databases like Aurora PostgreSQL, eliminating the need for complex ETL processes to move this live data into Redshift for analysis.",
        "selected_answer": "C"
      },
      {
        "author": "rebasheer",
        "date": "Wed 13 Aug 2025 21:59",
        "comment": "A and C",
        "selected_answer": "A"
      },
      {
        "author": "XP_2600",
        "date": "Thu 12 Jun 2025 11:41",
        "comment": "Question requires two answers \nA and C",
        "selected_answer": "A"
      },
      {
        "author": "AWSMM",
        "date": "Wed 23 Apr 2025 04:43",
        "comment": "A&C\nD is incorrect and here is why: Redshift Spectrum cannot directly query data stored in Amazon S3 Glacier Flexible Retrieval (or S3 Glacier Deep Archive). \nRedshift Spectrum can only query data stored in Amazon S3 Standard, S3 Intelligent-Tiering, S3 One Zone-IA, or S3 Glacier Instant Retrieval. Glacier Flexible Retrieval (previously just \"Glacier\") and Deep Archive are meant for long-term archival, and objects stored in those tiers aren't immediately accessible.\n\nWhen an object is in Glacier Flexible Retrieval:\n\nIt must first be restored, which can take minutes to hours depending on the retrieval tier (Expedited, Standard, or Bulk).\nDuring that time, the object remains unavailable for queries.",
        "selected_answer": "C"
      },
      {
        "author": "Palee",
        "date": "Sat 15 Mar 2025 16:38",
        "comment": "Option A and D. \nOption C doesn't talk about archiving Historical data",
        "selected_answer": "D"
      },
      {
        "author": "Vidhi212",
        "date": "Mon 09 Dec 2024 10:08",
        "comment": "The correct combination of steps is:\n\nA. Configure the Amazon Redshift Federated Query feature to query live transactional data that is in the PostgreSQL database.\n\nThis feature allows Amazon Redshift to directly query live transactional data in the PostgreSQL database without moving the data, enabling seamless integration with the data warehouse.\nC. Schedule a monthly job to copy data that is older than 15 months to Amazon S3 by using the UNLOAD command. Delete the old data from the Redshift cluster. Configure Amazon Redshift Spectrum to access historical data in Amazon S3.\n\nThis step archives older data to Amazon S3, which is more cost-effective than storing it in Redshift. Redshift Spectrum allows querying this archived data directly from S3, ensuring analytics queries can still access historical data.",
        "selected_answer": "A"
      },
      {
        "author": "SambitParida",
        "date": "Fri 06 Dec 2024 05:50",
        "comment": "A & C. Redshift spectrum cant read from glacier",
        "selected_answer": "A"
      },
      {
        "author": "rsmf",
        "date": "Tue 22 Oct 2024 14:17",
        "comment": "A & C is the best choice",
        "selected_answer": "A"
      },
      {
        "author": "mohamedTR",
        "date": "Mon 07 Oct 2024 10:48",
        "comment": "A & C: allows exporting Redshift data to Amazon S3 and ability to frequent access",
        "selected_answer": "A"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 20 Jun 2024 18:23",
        "comment": "A / C is a best choice",
        "selected_answer": "A"
      },
      {
        "author": "artworkad",
        "date": "Sat 15 Jun 2024 12:03",
        "comment": "AC is correct. D is not correct, because Redshift Spectrum cannot read from S3 Glacier Flexible Retrieval.",
        "selected_answer": "A"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:41",
        "comment": "Choice A ensures that live transactional data from PostgreSQL can be accessed directly within Redshift queries.\n\nChoice C archives historical data in Amazon S3, reducing storage costs in Redshift while still making the data accessible via Redshift Spectrum.\n\n(to Admin: I can't select multiple answers on the voting comment)",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142562-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 88 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 88,
    "question_text": "A manufacturing company has many IoT devices in facilities around the world. The company uses Amazon Kinesis Data Streams to collect data from the devices. The data includes device ID, capture date, measurement type, measurement value, and facility ID. The company uses facility ID as the partition key.\nThe company's operations team recently observed many WriteThroughputExceeded exceptions. The operations team found that some shards were heavily used but other shards were generally idle.\nHow should the company resolve the issues that the operations team observed?",
    "choices": [
      {
        "letter": "A",
        "text": "Change the partition key from facility ID to a randomly generated key."
      },
      {
        "letter": "B",
        "text": "Increase the number of shards."
      },
      {
        "letter": "C",
        "text": "Archive the data on the producer's side."
      },
      {
        "letter": "D",
        "text": "Change the partition key from facility ID to capture date."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:43",
        "comment": "The best solution to resolve the issue of uneven shard usage and WriteThroughputExceeded exceptions is to balance the load more evenly across the shards. This can be effectively achieved by changing the partition key to something that ensures a more uniform distribution of data across the shards.",
        "selected_answer": "A"
      },
      {
        "author": "Tester_TKK",
        "date": "Mon 21 Apr 2025 10:45",
        "comment": "https://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/",
        "selected_answer": "A"
      },
      {
        "author": "bakarys",
        "date": "Thu 04 Jul 2024 23:53",
        "comment": "The correct answer is **A. Change the partition key from facility ID to a randomly generated key.**\n\nAmazon Kinesis Data Streams uses the partition key that you specify to segregate the data records in the stream into shards. If the company uses the facility ID as the partition key, and if some facilities produce more data than others, then the data will be unevenly distributed across the shards. This can lead to some shards being heavily used while others are idle, and can cause `WriteThroughputExceeded` exceptions.\n\nBy changing the partition key to a randomly generated key, the data records are more likely to be evenly distributed across all the shards, which can help to avoid the issue of some shards being heavily used and others being idle. This solution requires the least operational overhead and does not involve increasing costs (as in option B), archiving data (which might not be desirable or feasible, as in option C), or changing to a partition key that might also lead to uneven distribution (as in option D).",
        "selected_answer": "A"
      },
      {
        "author": "didorins",
        "date": "Wed 03 Jul 2024 13:29",
        "comment": "D is not good, because you're effectively making things worse by partitioning by date. My answer is A",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142563-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 89 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 89,
    "question_text": "A data engineer wants to improve the performance of SQL queries in Amazon Athena that run against a sales data table.\nThe data engineer wants to understand the execution plan of a specific SQL statement. The data engineer also wants to see the computational cost of each operation in a SQL query.\nWhich statement does the data engineer need to run to meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "EXPLAIN SELECT * FROM sales;"
      },
      {
        "letter": "B",
        "text": "EXPLAIN ANALYZE FROM sales;"
      },
      {
        "letter": "C",
        "text": "EXPLAIN ANALYZE SELECT * FROM sales;"
      },
      {
        "letter": "D",
        "text": "EXPLAIN FROM sales;"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "FunkyFresco",
        "date": "Thu 20 Jun 2024 18:08",
        "comment": "use EXPLAIN ANALIZE \nhttps://docs.aws.amazon.com/athena/latest/ug/athena-explain-statement.html",
        "selected_answer": "C"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 04 Jul 2024 05:53",
        "comment": "explain analyze  + select * from table",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:46",
        "comment": "A - Only partially meets the requirements as it does not include computational costs.\nB - Incorrect syntax and does not meet the requirements.\nC - Fully meets the requirements by providing both the execution plan and the computational costs.\nD - Incorrect syntax and does not meet the requirements.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142564-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 90 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 90,
    "question_text": "A company plans to provision a log delivery stream within a VPC. The company configured the VPC flow logs to publish to Amazon CloudWatch Logs. The company needs to send the flow logs to Splunk in near real time for further analysis.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the data stream."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create a CloudWatch Logs subscription filter to send log events to the delivery stream."
      },
      {
        "letter": "C",
        "text": "Create an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the delivery stream."
      },
      {
        "letter": "D",
        "text": "Configure an Amazon Kinesis Data Streams data stream to use Splunk as the destination. Create an AWS Lambda function to send the flow logs from CloudWatch Logs to the data stream."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:48",
        "comment": "Kinesis Data Firehose has built-in support for Splunk as a destination, making the integration straightforward. Using a CloudWatch Logs subscription filter directly to Firehose simplifies the data flow, eliminating the need for additional Lambda functions or custom integrations.",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Fri 05 Jul 2024 00:00",
        "comment": "Creating an Amazon Kinesis Data Firehose delivery stream to use Splunk as the destination and creating a CloudWatch Logs subscription filter to send log events to the delivery stream would meet these requirements with the least operational overhead.\n\nAmazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Splunk.\n\nCloudWatch Logs subscription filters allow you to send real-time log events to Kinesis Data Firehose and are ideal for scenarios where you want to forward the logs to other services for further analysis.\n\nOptions A and D involve Kinesis Data Streams, which would require additional management and operational overhead. Option C involves creating a Lambda function, which also adds operational overhead. Therefore, option B is the best choice.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142565-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 91 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 91,
    "question_text": "A company has a data lake on AWS. The data lake ingests sources of data from business units. The company uses Amazon Athena for queries. The storage layer is Amazon S3 with an AWS Glue Data Catalog as a metadata repository.\nThe company wants to make the data available to data scientists and business analysts. However, the company first needs to manage fine-grained, column-level data access for Athena based on the user roles and responsibilities.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Set up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation."
      },
      {
        "letter": "B",
        "text": "Define an IAM resource-based policy for AWS Glue tables. Attach the same policy to IAM user groups."
      },
      {
        "letter": "C",
        "text": "Define an IAM identity-based policy for AWS Glue tables. Attach the same policy to IAM roles. Associate the IAM roles with IAM groups that contain the users."
      },
      {
        "letter": "D",
        "text": "Create a resource share in AWS Resource Access Manager (AWS RAM) to grant access to IAM users."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ja13",
        "date": "Sun 07 Jul 2024 23:12",
        "comment": "Correct Solution:\nA. Set up AWS Lake Formation. Define security policy-based rules for the users and applications by IAM role in Lake Formation.\n\nExplanation:\nAWS Lake Formation: This service simplifies and automates the process of securing and managing data lakes. It allows you to define fine-grained access control policies at the database, table, and column levels.\nSecurity Policy-Based Rules: Lake Formation allows you to create policies that specify which users or roles have access to specific data, including column-level access controls. This makes it easier to manage access based on roles and responsibilities.",
        "selected_answer": "A"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Fri 20 Dec 2024 13:21",
        "comment": "A lake formation for any  fine-grained access",
        "selected_answer": "A"
      },
      {
        "author": "HunkyBunky",
        "date": "Fri 21 Jun 2024 03:18",
        "comment": "A - Lake formation",
        "selected_answer": "A"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:51",
        "comment": "Lake Formation supports fine-grained access control, including column-level permissions.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142566-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 92 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 92,
    "question_text": "A company has developed several AWS Glue extract, transform, and load (ETL) jobs to validate and transform data from Amazon S3. The ETL jobs load the data into Amazon RDS for MySQL in batches once every day. The ETL jobs use a DynamicFrame to read the S3 data.\nThe ETL jobs currently process all the data that is in the S3 bucket. However, the company wants the jobs to process only the daily incremental data.\nWhich solution will meet this requirement with the LEAST coding effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an ETL job that reads the S3 file status and logs the status in Amazon DynamoDB."
      },
      {
        "letter": "B",
        "text": "Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data."
      },
      {
        "letter": "C",
        "text": "Enable job metrics for the ETL jobs to help keep track of processed objects in Amazon CloudWatch."
      },
      {
        "letter": "D",
        "text": "Configure the ETL jobs to delete processed objects from Amazon S3 after each run."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:54",
        "comment": "AWS Glue job bookmarks are designed to handle incremental data processing by automatically tracking the state.",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:47",
        "comment": "AWS Glue Bookmarks can be used to pin where the data processing last stopped hence help with incremental processing.",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 04 Jul 2024 05:52",
        "comment": "B - bookmarks is a key",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 13:27",
        "comment": "The solution that will meet this requirement with the least coding effort is Option B: Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.\n\nAWS Glue job bookmarks help ETL jobs to keep track of data that has already been processed during previous runs. By enabling job bookmarks, the ETL jobs can skip the processed data and only process the new, incremental data. This feature is designed specifically for this use case and requires minimal coding effort.\n\nOptions A, C, and D would require additional coding and operational effort. Option A would require creating a new ETL job and managing a DynamoDB table. Option C would involve setting up job metrics and CloudWatch, which doesn’t directly address processing incremental data. Option D would involve deleting data from S3 after processing, which might not be desirable if the original data needs to be retained. Therefore, Option B is the most suitable solution.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142567-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 93 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 93,
    "question_text": "An online retail company has an application that runs on Amazon EC2 instances that are in a VPC. The company wants to collect flow logs for the VPC and analyze network traffic.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Publish flow logs to Amazon CloudWatch Logs. Use Amazon Athena for analytics."
      },
      {
        "letter": "B",
        "text": "Publish flow logs to Amazon CloudWatch Logs. Use an Amazon OpenSearch Service cluster for analytics."
      },
      {
        "letter": "C",
        "text": "Publish flow logs to Amazon S3 in text format. Use Amazon Athena for analytics."
      },
      {
        "letter": "D",
        "text": "Publish flow logs to Amazon S3 in Apache Parquet format. Use Amazon Athena for analytics."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 09:59",
        "comment": "Flow Logs can be published to S3 in Parquet format: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html#flow-logs-s3-path",
        "selected_answer": "D"
      },
      {
        "author": "PGGuy",
        "date": "Fri 21 Jun 2024 20:06",
        "comment": "Publishing flow logs to Amazon S3 in Apache Parquet format and using Amazon Athena for analytics (D) is the most cost-effective solution. This approach minimizes storage costs due to the efficient compression of Parquet, and optimizes query performance and cost in Athena due to the reduced data size and optimized columnar storage.",
        "selected_answer": "D"
      },
      {
        "author": "LR2023",
        "date": "Wed 17 Jul 2024 16:08",
        "comment": "Flow logs acn be published to S3 but then option D sas in Parquet format - it is not automatically converted into parquet....\nhttps://aws.amazon.com/solutions/implementations/centralized-logging-with-opensearch/",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Mon 24 Jun 2024 13:12",
        "comment": "Apache parquet and S3 = most cost-effective solution",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142568-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 94 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 94,
    "question_text": "A retail company stores transactions, store locations, and customer information tables in four reserved ra3.4xlarge Amazon Redshift cluster nodes. All three tables use even table distribution.\nThe company updates the store location table only once or twice every few years.\nA data engineer notices that Redshift queues are slowing down because the whole store location table is constantly being broadcast to all four compute nodes for most queries. The data engineer wants to speed up the query performance by minimizing the broadcasting of the store location table.\nWhich solution will meet these requirements in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Change the distribution style of the store location table from EVEN distribution to ALL distribution."
      },
      {
        "letter": "B",
        "text": "Change the distribution style of the store location table to KEY distribution based on the column that has the highest dimension."
      },
      {
        "letter": "C",
        "text": "Add a join column named store_id into the sort key for all the tables."
      },
      {
        "letter": "D",
        "text": "Upgrade the Redshift reserved node to a larger instance size in the same instance family."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:52",
        "comment": "ALL distribution is optimal for slowly changing dimension tables and generally small in size to allow for optimal joins.",
        "selected_answer": "A"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 13:40",
        "comment": "The most cost-effective solution to speed up the query performance by minimizing the broadcasting of the store location table would be:\n\nA. Change the distribution style of the store location table from EVEN distribution to ALL distribution.\n\nIn Amazon Redshift, the ALL distribution style replicates the entire table to all nodes in the cluster, which eliminates the need to redistribute the data when executing a query. This can significantly improve query performance. Given that the store location table is updated only once or twice every few years, the overhead of maintaining the replicated data would be minimal. This makes it a cost-effective solution for improving the query performance.",
        "selected_answer": "A"
      },
      {
        "author": "PGGuy",
        "date": "Fri 21 Jun 2024 20:07",
        "comment": "Changing the distribution style of the store location table to ALL distribution (A) is the most cost-effective solution. It directly addresses the issue of broadcasting by ensuring the entire table is available on each node, significantly improving join performance without incurring substantial additional costs.",
        "selected_answer": "A"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:01",
        "comment": "Using ALL distribution means the table is replicated to all nodes, eliminating the need for broadcasting during queries. Since the store location table is updated infrequently, this will significantly speed up queries without incurring frequent update costs.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142569-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 95 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 95,
    "question_text": "A company has a data warehouse that contains a table that is named Sales. The company stores the table in Amazon Redshift. The table includes a column that is named city_name. The company wants to query the table to find all rows that have a city_name that starts with \"San\" or \"El\".\nWhich SQL query will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Select * from Sales where city_name ~ ‘$(San|El)*’;"
      },
      {
        "letter": "B",
        "text": "Select * from Sales where city_name ~ ‘^(San|El)*’;"
      },
      {
        "letter": "C",
        "text": "Select * from Sales where city_name ~’$(San&El)*’;"
      },
      {
        "letter": "D",
        "text": "Select * from Sales where city_name ~ ‘^(San&El)*’;"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "chrispchrisp",
        "date": "Tue 23 Jul 2024 16:00",
        "comment": "Regex Patterns for everyone's reference\n\n. : Matches any single character.\n* : Matches zero or more of the preceding element.\n+ : Matches one or more of the preceding element.\n[abc] : Matches any of the enclosed characters.\n[^abc] : Matches any character not enclosed.\n^ : Matches the start of a string.\n$ : Matches the end of a string.\n| : Logical OR operator.\n(abc) : Matches 'abc' and remembers the match.\n\nAnswer is B",
        "selected_answer": "B"
      },
      {
        "author": "andrologin",
        "date": "Tue 16 Jul 2024 21:53",
        "comment": "Regex patterns:\n^ - used to capture the start of the text/string\n| - used as an OR operator",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 13:42",
        "comment": "B. Select * from Sales where city_name ~ ‘^(San|El)*’;\n\nThis query uses a regular expression pattern with the ~ operator. The caret ^ at the beginning of the pattern indicates that the match must start at the beginning of the string. (San|El) matches either “San” or “El”, and * means zero or more of the preceding element. So this query will return all rows where city_name starts with either “San” or “El”.",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Fri 21 Jun 2024 03:23",
        "comment": "B - becuase of regexp",
        "selected_answer": "B"
      },
      {
        "author": "JohnYang",
        "date": "Wed 19 Jun 2024 00:02",
        "comment": "^ asserts the position at the start of the string.\n(San|El) matches either \"San\" or \"El\".",
        "selected_answer": "B"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:02",
        "comment": "~: This operator indicates the use of a regular expression.\n^: This symbol signifies the start of the string.\n(San|El): This pattern matches strings that start with either \"San\" or \"El\".",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142571-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 96 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 96,
    "question_text": "A company needs to send customer call data from its on-premises PostgreSQL database to AWS to generate near real-time insights. The solution must capture and load updates from operational data stores that run in the PostgreSQL database. The data changes continuously.\nA data engineer configures an AWS Database Migration Service (AWS DMS) ongoing replication task. The task reads changes in near real time from the PostgreSQL source database transaction logs for each table. The task then sends the data to an Amazon Redshift cluster for processing.\nThe data engineer discovers latency issues during the change data capture (CDC) of the task. The data engineer thinks that the PostgreSQL source database is causing the high latency.\nWhich solution will confirm that the PostgreSQL database is the source of the high latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon CloudWatch to monitor the DMS task. Examine the CDCIncomingChanges metric to identify delays in the CDC from the source database."
      },
      {
        "letter": "B",
        "text": "Verify that logical replication of the source database is configured in the postgresql.conf configuration file."
      },
      {
        "letter": "C",
        "text": "Enable Amazon CloudWatch Logs for the DMS endpoint of the source database. Check for error messages."
      },
      {
        "letter": "D",
        "text": "Use Amazon CloudWatch to monitor the DMS task. Examine the CDCLatencySource metric to identify delays in the CDC from the source database."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:11",
        "comment": "CDCLatencySource Metric: This metric measures the latency between the source database and the DMS task. It shows how long it takes for changes to be read from the source database's transaction logs.\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html#CHAP_Monitoring.Metrics",
        "selected_answer": "D"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 04 Jul 2024 07:12",
        "comment": "only D makes sense",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142561-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 97 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 97,
    "question_text": "A lab uses IoT sensors to monitor humidity, temperature, and pressure for a project. The sensors send 100 KB of data every 10 seconds. A downstream process will read the data from an Amazon S3 bucket every 30 seconds.\nWhich solution will deliver the data to the S3 bucket with the LEAST latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use the default buffer interval for Kinesis Data Firehose."
      },
      {
        "letter": "B",
        "text": "Use Amazon Kinesis Data Streams to deliver the data to the S3 bucket. Configure the stream to use 5 provisioned shards."
      },
      {
        "letter": "C",
        "text": "Use Amazon Kinesis Data Streams and call the Kinesis Client Library to deliver the data to the S3 bucket. Use a 5 second buffer interval from an application."
      },
      {
        "letter": "D",
        "text": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use a 5 second buffer interval for Kinesis Data Firehose."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:36",
        "comment": "C - This option ensures low latency by using a short buffer interval (5 seconds). The use of KCL allows for customized processing logic and timely delivery of data to S3. This makes it a strong candidate for minimal latency.\n\nD - While this option provides low latency with a 5-second buffer interval, it introduces unnecessary complexity by using Apache Flink for what seems to be a straightforward data ingestion task. This option is overkill for the given use case and may add more operational overhead than necessary.",
        "selected_answer": "C"
      },
      {
        "author": "artworkad",
        "date": "Sat 15 Jun 2024 12:28",
        "comment": "Kinesis Data Streams cannot deliver directly to S3. Data has to go through Firehose. A is correct but is not lowest latency. I would go with D, as we can set the buffer interval to a low value. We do not need Flink, tho. That's a bit confusing.",
        "selected_answer": "D"
      },
      {
        "author": "rebasheer",
        "date": "Wed 13 Aug 2025 22:53",
        "comment": "ANSWER IS D",
        "selected_answer": "D"
      },
      {
        "author": "Eleftheriia",
        "date": "Fri 22 Nov 2024 11:13",
        "comment": "Why could not be A?\nhttps://aws.amazon.com/blogs/big-data/optimize-downstream-data-processing-with-amazon-data-firehose-and-amazon-emr-running-apache-spark/\nIt uses Data Firehose + Kinesis Data Streams",
        "selected_answer": "A"
      },
      {
        "author": "andrologin",
        "date": "Wed 24 Jul 2024 05:42",
        "comment": "Use data streams and KCL, option A would be right but the default buffer for Firehose does not allow it to be correct. D adds extra components that are not needed for delivery of data.",
        "selected_answer": "C"
      },
      {
        "author": "LR2023",
        "date": "Wed 17 Jul 2024 17:02",
        "comment": "https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/",
        "selected_answer": "A"
      },
      {
        "author": "GHill1982",
        "date": "Sat 15 Jun 2024 09:31",
        "comment": "I think the answer is C. Kinesis Data Firehose has a minimum buffer interval of 60 seconds (1 minute) or 1 MB of data.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142573-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 98 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 98,
    "question_text": "A company wants to use machine learning (ML) to perform analytics on data that is in an Amazon S3 data lake. The company has two data transformation requirements that will give consumers within the company the ability to create reports.\nThe company must perform daily transformations on 300 GB of data that is in a variety format that must arrive in Amazon S3 at a scheduled time. The company must perform one-time transformations of terabytes of archived data that is in the S3 data lake. The company uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) Directed Acyclic Graphs (DAGs) to orchestrate processing.\nWhich combination of tasks should the company schedule in the Amazon MWAA DAGs to meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "For daily incoming data, use AWS Glue crawlers to scan and identify the schema."
      },
      {
        "letter": "B",
        "text": "For daily incoming data, use Amazon Athena to scan and identify the schema."
      },
      {
        "letter": "C",
        "text": "For daily incoming data, use Amazon Redshift to perform transformations."
      },
      {
        "letter": "D",
        "text": "For daily and archived data, use Amazon EMR to perform data transformations."
      },
      {
        "letter": "E",
        "text": "For archived data, use Amazon SageMaker to perform data transformations."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Wed 24 Jul 2024 05:45",
        "comment": "Glue crawlers for identifying the schema, EMR to run batch processing on the data",
        "selected_answer": "AD"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 15:56",
        "comment": "According to ChatGPT",
        "selected_answer": "AD"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:46",
        "comment": "A. For daily incoming data, use AWS Glue crawlers to scan and identify the schema. This is cost-effective and simplifies the process of managing metadata.\n\nD. For daily and archived data, use Amazon EMR to perform data transformations. EMR is suitable for both large-scale and regular transformations, offering flexibility and cost efficiency.",
        "selected_answer": "AD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142574-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 99 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 99,
    "question_text": "A retail company uses AWS Glue for extract, transform, and load (ETL) operations on a dataset that contains information about customer orders. The company wants to implement specific validation rules to ensure data accuracy and consistency.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue job bookmarks to track the data for accuracy and consistency."
      },
      {
        "letter": "B",
        "text": "Create custom AWS Glue Data Quality rulesets to define specific data quality checks."
      },
      {
        "letter": "C",
        "text": "Use the built-in AWS Glue Data Quality transforms for standard data quality validations."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue Data Catalog to maintain a centralized data schema and metadata repository."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HunkyBunky",
        "date": "Thu 04 Jul 2024 06:18",
        "comment": "Only B - makes sense",
        "selected_answer": "B"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 15:58",
        "comment": "B. Create custom AWS Glue Data Quality rulesets to define specific data quality checks.\n\nCustom AWS Glue Data Quality rulesets allow you to define precise data quality checks tailored to your specific needs, ensuring that the data meets the required standards of accuracy and consistency. This approach provides flexibility to implement a wide range of validation rules based on your business requirements.",
        "selected_answer": "B"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:49",
        "comment": "This option provides the necessary flexibility to define and implement custom validation rules tailored to the company's specific requirements for data accuracy and consistency.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142575-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 100 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 100,
    "question_text": "An insurance company stores transaction data that the company compressed with gzip.\nThe company needs to query the transaction data for occasional audits.\nWhich solution will meet this requirement in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Store the data in Amazon Glacier Flexible Retrieval. Use Amazon S3 Glacier Select to query the data."
      },
      {
        "letter": "B",
        "text": "Store the data in Amazon S3. Use Amazon S3 Select to query the data."
      },
      {
        "letter": "C",
        "text": "Store the data in Amazon S3. Use Amazon Athena to query the data."
      },
      {
        "letter": "D",
        "text": "Store the data in Amazon Glacier Instant Retrieval. Use Amazon Athena to query the data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "tgv",
        "date": "Sun 16 Jun 2024 12:54",
        "comment": "Actually, I think A makes more sense.",
        "selected_answer": "A"
      },
      {
        "author": "AminTriton",
        "date": "Sun 10 Aug 2025 19:57",
        "comment": "- Glacier Flexible Retrieval (formerly Glacier) is **very cheap** for storage but **slow** for retrieval (minutes to hours).  \n- Glacier Select allows **SQL-like queries directly on archive objects**, but it's billed per request plus retrieval.  \n- For *occasional audits*, it can be **cost-effective**, but even simple queries will incur retrieval delay — fine if audits are not time-sensitive.",
        "selected_answer": "A"
      },
      {
        "author": "youonebe",
        "date": "Wed 14 May 2025 01:21",
        "comment": "A for sure.",
        "selected_answer": "A"
      },
      {
        "author": "df94e2b",
        "date": "Mon 12 May 2025 03:50",
        "comment": "\" occasional \" is the key word here. SO will go with \"A\"",
        "selected_answer": "A"
      },
      {
        "author": "Tester_TKK",
        "date": "Mon 21 Apr 2025 11:32",
        "comment": "A would cost more because of higher retrieval costs",
        "selected_answer": "B"
      },
      {
        "author": "AM027",
        "date": "Wed 09 Apr 2025 10:30",
        "comment": "cold data stored in Glacier can \n be easily queried within minutes.",
        "selected_answer": "A"
      },
      {
        "author": "YUICH",
        "date": "Mon 27 Jan 2025 12:45",
        "comment": "For workloads with low access frequency where you only need to query data occasionally (for example, during audits), option (A)—S3 Glacier Flexible Retrieval combined with S3 Glacier Select—provides the most cost-effective solution.",
        "selected_answer": "B"
      },
      {
        "author": "div_div",
        "date": "Wed 22 Jan 2025 07:42",
        "comment": "Transaction Data Refers To Data Which Are Updating Frequently and To Query That Data occasionally Means It Can Be Query At Any Time (In Question Time Is Not Define). So We Can't Take Risk For Customer To Wait For Hours To Get The Result And The Best Way To Query The Data On Top of The S3 Bucket We Can Use Athena.",
        "selected_answer": "C"
      },
      {
        "author": "BigMrT",
        "date": "Sun 05 Jan 2025 19:29",
        "comment": "Glacier Select incurs higher costs compared to S3 Select.",
        "selected_answer": "B"
      },
      {
        "author": "mohamedTR",
        "date": "Mon 07 Oct 2024 13:53",
        "comment": "B is the more cost-effective solution for occasional audits. It allows for easier access to the data without incurring high retrieval costs",
        "selected_answer": "B"
      },
      {
        "author": "LR2023",
        "date": "Wed 25 Sep 2024 20:46",
        "comment": "https://aws.amazon.com/blogs/aws/s3-glacier-select/\n\noption B is not cost effective as it is stored in standard S3",
        "selected_answer": "A"
      },
      {
        "author": "PashoQ",
        "date": "Tue 17 Sep 2024 14:23",
        "comment": "Occasional audits, so go for S3 glacier select",
        "selected_answer": "A"
      },
      {
        "author": "cas_tori",
        "date": "Wed 14 Aug 2024 12:03",
        "comment": "this is B",
        "selected_answer": "B"
      },
      {
        "author": "lenneth39",
        "date": "Sun 04 Aug 2024 07:21",
        "comment": "I am not sure whether to go for B or C. Can anyone comment on this?\nB: No problem, but not available if Parquet is Gzip compressed. But the problem statement doesn't say Parquet is Gzip compressed.\nC: Correct if Parquet is Gzip compressed, but B is more cost-effective if csv or json is Gzip compressed",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142543-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 101 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 101,
    "question_text": "A data engineer finished testing an Amazon Redshift stored procedure that processes and inserts data into a table that is not mission critical. The engineer wants to automatically run the stored procedure on a daily basis.\nWhich solution will meet this requirement in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Lambda function to schedule a cron job to run the stored procedure."
      },
      {
        "letter": "B",
        "text": "Schedule and run the stored procedure by using the Amazon Redshift Data API in an Amazon EC2 Spot Instance."
      },
      {
        "letter": "C",
        "text": "Use query editor v2 to run the stored procedure on a schedule."
      },
      {
        "letter": "D",
        "text": "Schedule an AWS Glue Python shell job to run the stored procedure."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "artworkad",
        "date": "Sat 15 Jun 2024 12:40",
        "comment": "This can be achieved with query editor v2 (https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-schedule-query.html)",
        "selected_answer": "C"
      },
      {
        "author": "andrologin",
        "date": "Thu 18 Jul 2024 05:56",
        "comment": "I go with C because it runs the query within the Redshift instance, B may not be appropriate because it involves other services on top of the Redshift instance and there is movement of data across the services.",
        "selected_answer": "C"
      },
      {
        "author": "4d716d6",
        "date": "Sat 13 Jul 2024 20:25",
        "comment": "given that the table is not mission-critical and requires the \"MOST cost-effective way.\"",
        "selected_answer": "B"
      },
      {
        "author": "salayea28",
        "date": "Thu 27 Jun 2024 20:47",
        "comment": "I am going with option B, given that the table is not mission-critical and requires the \"MOST cost-effective way.\" \nAWS Spot Instances are Amazon EC2 instances that allow you to utilize spare EC2 capacity at a significantly lower cost than On-Demand instances. These instances are ideal for flexible workloads that can tolerate interruptions, such as batch processing, data analysis, and background processing jobs.",
        "selected_answer": "B"
      },
      {
        "author": "GHill1982",
        "date": "Sun 16 Jun 2024 06:38",
        "comment": "I think all options other than using the query editor will incur additional costs.",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 10:54",
        "comment": "AWS Lambda, combined with Amazon CloudWatch Events for scheduling, provides a low-cost, serverless, and reliable way to automatically run the stored procedure daily.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142576-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 102 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 102,
    "question_text": "A marketing company collects clickstream data. The company sends the clickstream data to Amazon Kinesis Data Firehose and stores the clickstream data in Amazon S3. The company wants to build a series of dashboards that hundreds of users from multiple departments will use.\nThe company will use Amazon QuickSight to develop the dashboards. The company wants a solution that can scale and provide daily updates about clickstream activity.\nWhich combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Redshift to store and query the clickstream data."
      },
      {
        "letter": "B",
        "text": "Use Amazon Athena to query the clickstream data"
      },
      {
        "letter": "C",
        "text": "Use Amazon S3 analytics to query the clickstream data."
      },
      {
        "letter": "D",
        "text": "Access the query data through a QuickSight direct SQL query."
      },
      {
        "letter": "E",
        "text": "Access the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset."
      }
    ],
    "correct_answer": "BE",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 16:05",
        "comment": "B. Use Amazon Athena to query the clickstream data.\nE. Access the query data through QuickSight SPICE (Super-fast, Parallel, In-memory Calculation Engine). Configure a daily refresh for the dataset.\n\nHere's why:\n\nB. Use Amazon Athena to query the clickstream data: Amazon Athena allows you to run SQL queries directly on data stored in Amazon S3 without the need for complex ETL processes. It is a cost-effective solution for querying large datasets on S3.\n\nE. Access the query data through QuickSight SPICE: QuickSight SPICE is designed for fast, in-memory data analysis and can scale to support many users and large datasets. By configuring a daily refresh, you ensure that the dashboards are updated with the latest data while keeping query performance high and costs low.",
        "selected_answer": "BE"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Wed 25 Dec 2024 09:05",
        "comment": "both are more or less the only possible",
        "selected_answer": "BE"
      },
      {
        "author": "GHill1982",
        "date": "Sun 16 Jun 2024 06:49",
        "comment": "Agree with B & E. Athena would be cheaper than Redshift. S3 analytics is irrelevant. The functionality in SPICE should be more cost effective than direct SQL by reducing the frequency and volume of queries.",
        "selected_answer": "BE"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:00",
        "comment": "Athena charges based on the amount of data scanned per query, which can be cost-effective for ad-hoc querying and periodic updates.\n\nSPICE can be more cost-effective for frequent access and analysis by multiple users as it reduces the load on the underlying data source.",
        "selected_answer": "BE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142577-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 103 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 103,
    "question_text": "A data engineer is building a data orchestration workflow. The data engineer plans to use a hybrid model that includes some on-premises resources and some resources that are in the cloud. The data engineer wants to prioritize portability and open source resources.\nWhich service should the data engineer use in both the on-premises environment and the cloud-based environment?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Data Exchange"
      },
      {
        "letter": "B",
        "text": "Amazon Simple Workflow Service (Amazon SWF)"
      },
      {
        "letter": "C",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)"
      },
      {
        "letter": "D",
        "text": "AWS Glue"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Thu 18 Jul 2024 06:01",
        "comment": "AMWAA is just Apache Airflow managed by AWS. Using it means you can use the same dags for your on-premises solution that you also use on cloud",
        "selected_answer": "C"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 16:07",
        "comment": "C. Amazon Managed Workflows for Apache Airflow (Amazon MWAA)\n\nAmazon MWAA is a managed service for Apache Airflow, which is an open-source workflow automation tool. Apache Airflow can be used both on-premises and in the cloud, making it ideal for hybrid environments. Using Amazon MWAA allows the data engineer to leverage the managed service in the cloud while maintaining the ability to use the same open-source Airflow setup on-premises, ensuring portability and consistency across environments.",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:02",
        "comment": "Airflow can orchestrate workflows that involve both on-premises and cloud resources, making it ideal for hybrid models.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142542-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 104 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 104,
    "question_text": "A gaming company uses a NoSQL database to store customer information. The company is planning to migrate to AWS.\nThe company needs a fully managed AWS solution that will handle high online transaction processing (OLTP) workload, provide single-digit millisecond performance, and provide high availability around the world.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon Keyspaces (for Apache Cassandra)"
      },
      {
        "letter": "B",
        "text": "Amazon DocumentDB (with MongoDB compatibility)"
      },
      {
        "letter": "C",
        "text": "Amazon DynamoDB"
      },
      {
        "letter": "D",
        "text": "Amazon Timestream"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "JUNGAWS",
        "date": "Fri 14 Jun 2024 21:18",
        "comment": "provide single-digit millisecond performance => DynamoDB",
        "selected_answer": "C"
      },
      {
        "author": "HunkyBunky",
        "date": "Thu 20 Jun 2024 17:00",
        "comment": "No doubt - DynamoDB",
        "selected_answer": "C"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:03",
        "comment": "Amazon DynamoDB is the most appropriate choice given the company's requirements for high performance, global availability, and minimal operational overhead.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142578-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 105 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 105,
    "question_text": "A data engineer creates an AWS Lambda function that an Amazon EventBridge event will invoke. When the data engineer tries to invoke the Lambda function by using an EventBridge event, an AccessDeniedException message appears.\nHow should the data engineer resolve the exception?",
    "choices": [
      {
        "letter": "A",
        "text": "Ensure that the trust policy of the Lambda function execution role allows EventBridge to assume the execution role."
      },
      {
        "letter": "B",
        "text": "Ensure that both the IAM role that EventBridge uses and the Lambda function's resource-based policy have the necessary permissions."
      },
      {
        "letter": "C",
        "text": "Ensure that the subnet where the Lambda function is deployed is configured to be a private subnet."
      },
      {
        "letter": "D",
        "text": "Ensure that EventBridge schemas are valid and that the event mapping configuration is correct."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "artworkad",
        "date": "Mon 17 Jun 2024 20:29",
        "comment": "The lambda resource based policy must allow the events principle to invoke the lambda function. https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html#eb-schedule-create-rule and https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html#eb-schedule-create-rule Amazon SQS, Amazon SNS, Lambda, CloudWatch Logs, and EventBridge bus targets do not use roles, and permissions to EventBridge must be granted via a resource policy.",
        "selected_answer": "B"
      },
      {
        "author": "Shanmahi",
        "date": "Thu 15 Aug 2024 04:09",
        "comment": "Option B",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Mon 24 Jun 2024 13:07",
        "comment": "Only B - makes sense",
        "selected_answer": "B"
      },
      {
        "author": "rpwags",
        "date": "Sat 22 Jun 2024 20:53",
        "comment": "\"B\" is corect because the only way to resolve the AccessDeniedException message is to make sure both the IAM role for EventBridge and the Lambda function's resource-based policy have the necessary permissions.",
        "selected_answer": "B"
      },
      {
        "author": "GHill1982",
        "date": "Sun 16 Jun 2024 07:04",
        "comment": "The trust policy is what grants an AWS service permission to use the role on behalf of the user. Without this trust relationship, EventBridge won’t have the necessary permissions to invoke the Lambda function.",
        "selected_answer": "A"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:06",
        "comment": "IAM Role for EventBridge: EventBridge needs permission to invoke the Lambda function.\nLambda Resource-Based Policy: The Lambda function must have a resource-based policy that allows EventBridge to invoke it.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142579-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 106 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 106,
    "question_text": "A company uses a data lake that is based on an Amazon S3 bucket. To comply with regulations, the company must apply two layers of server-side encryption to files that are uploaded to the S3 bucket. The company wants to use an AWS Lambda function to apply the necessary encryption.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use both server-side encryption with AWS KMS keys (SSE-KMS) and the Amazon S3 Encryption Client."
      },
      {
        "letter": "B",
        "text": "Use dual-layer server-side encryption with AWS KMS keys (DSSE-KMS)."
      },
      {
        "letter": "C",
        "text": "Use server-side encryption with customer-provided keys (SSE-C) before files are uploaded."
      },
      {
        "letter": "D",
        "text": "Use server-side encryption with AWS KMS keys (SSE-KMS)."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "samadal",
        "date": "Wed 21 Aug 2024 08:30",
        "comment": "The most crucial objective in the problem is \"Two layers of server-side encryption must be applied.\"\n\nA: SSE-KMS is a single-layer server-side encryption that uses AWS KMS keys to encrypt data.\nThe Amazon S3 Encryption Client performs client-side encryption, not server-side encryption.\nC: SSE-C is server-side encryption that uses customer-provided encryption keys to encrypt data.\nThis does not provide two layers of encryption.\nD: SSE-KMS is a single-layer server-side encryption. It does not meet the encryption requirement of two layers of encryption.\n\nB: DSSE-KMS (dual-layer server-side encryption) uses two layers of encryption to encrypt data using keys managed by AWS KMS. The first layer is used to encrypt the data key, and the second layer is used to encrypt the actual data. This provides the two layers of server-side encryption required to meet compliance requirements.",
        "selected_answer": "B"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 03:02",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingDSSEncryption.html",
        "selected_answer": "B"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 17:37",
        "comment": "B. Use dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\n\nDual-layer server-side encryption with AWS KMS keys (DSSE-KMS) is specifically designed to apply two layers of encryption to meet regulatory compliance requirements. This ensures that each object stored in Amazon S3 is encrypted twice, providing the additional security layer that the company needs.",
        "selected_answer": "B"
      },
      {
        "author": "bakarys",
        "date": "Wed 03 Jul 2024 01:40",
        "comment": "The solution that will meet these requirements is Option A: Use both server-side encryption with AWS KMS keys (SSE-KMS) and the Amazon S3 Encryption Client.\n\nThis approach provides two layers of encryption. The first layer is the server-side encryption with AWS KMS keys (SSE-KMS), which encrypts the data at rest. The second layer is the client-side encryption using the Amazon S3 Encryption Client before the data is uploaded to S3. This way, the data is already encrypted when it arrives at S3 and then it gets encrypted again by S3, thus providing two layers of encryption.\n\nThe other options are not as suitable:\n\nOption B: There’s no such thing as dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\nOption C: Server-side encryption with customer-provided keys (SSE-C) only provides one layer of encryption.\nOption D: Server-side encryption with AWS KMS keys (SSE-KMS) also only provides one layer of encryption",
        "selected_answer": "A"
      },
      {
        "author": "HunkyBunky",
        "date": "Mon 24 Jun 2024 13:17",
        "comment": "I guess that right answer is - B\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingDSSEncryption.html",
        "selected_answer": "B"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:09",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-dsse-encryption.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/142580-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 107 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 107,
    "question_text": "A data engineer notices that Amazon Athena queries are held in a queue before the queries run.\nHow can the data engineer prevent the queries from queueing?",
    "choices": [
      {
        "letter": "A",
        "text": "Increase the query result limit."
      },
      {
        "letter": "B",
        "text": "Configure provisioned capacity for an existing workgroup."
      },
      {
        "letter": "C",
        "text": "Use federated queries."
      },
      {
        "letter": "D",
        "text": "Allow users who run the Athena queries to an existing workgroup."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 17:39",
        "comment": "B. Configure provisioned capacity for an existing workgroup.\n\nProvisioned capacity in Amazon Athena allows you to allocate dedicated query processing capacity to a workgroup. This helps ensure that your queries are run without being held in a queue, providing more consistent and predictable performance.",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:02",
        "comment": "In my opinion - only B - makes sense",
        "selected_answer": "B"
      },
      {
        "author": "tgv",
        "date": "Sat 15 Jun 2024 11:10",
        "comment": "Provisioning capacity ensures that there are sufficient dedicated resources available to handle the query load, thereby preventing queries from being held in a queue. This approach directly addresses the issue by increasing the available processing capacity for your queries.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143046-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 108 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 108,
    "question_text": "A data engineer needs to debug an AWS Glue job that reads from Amazon S3 and writes to Amazon Redshift. The data engineer enabled the bookmark feature for the AWS Glue job.\nThe data engineer has set the maximum concurrency for the AWS Glue job to 1.\nThe AWS Glue job is successfully writing the output to Amazon Redshift. However, the Amazon S3 files that were loaded during previous runs of the AWS Glue job are being reprocessed by subsequent runs.\nWhat is the likely reason the AWS Glue job is reprocessing the files?",
    "choices": [
      {
        "letter": "A",
        "text": "The AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly."
      },
      {
        "letter": "B",
        "text": "The maximum concurrency for the AWS Glue job is set to 1."
      },
      {
        "letter": "C",
        "text": "The data engineer incorrectly specified an older version of AWS Glue for the Glue job."
      },
      {
        "letter": "D",
        "text": "The AWS Glue job does not have a required commit statement."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "lool",
        "date": "Sat 06 Jul 2024 15:40",
        "comment": "https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data",
        "selected_answer": "D"
      },
      {
        "author": "bonds",
        "date": "Sun 20 Apr 2025 21:59",
        "comment": "Bookmarks need explicit commit after job is complete. If commit did not go through, the state/bookmark did not get saved, and hence the reprocessing of all the previous objects.",
        "selected_answer": "D"
      },
      {
        "author": "AgboolaKun",
        "date": "Thu 07 Nov 2024 18:22",
        "comment": "A \"commit\" statement within your AWS Glue job script is absolutely required to update the job bookmark and properly track processed data, preventing the reprocessing of old data when running the job again; essentially, if you don't include the commit statement, the job will not remember where it left off and may process data multiple times. For more information about job.commit(), please reference this documentation -  https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data",
        "selected_answer": "D"
      },
      {
        "author": "rsmf",
        "date": "Tue 22 Oct 2024 15:31",
        "comment": "It's B the right answer",
        "selected_answer": "D"
      },
      {
        "author": "mohamedTR",
        "date": "Sun 20 Oct 2024 09:07",
        "comment": "Commit statements are relevant to transactional operations in databases like Redshift but are not related to S3 bookmarks or Glue’s tracking mechanism for processed files.",
        "selected_answer": "A"
      },
      {
        "author": "proserv",
        "date": "Fri 04 Oct 2024 12:03",
        "comment": "Ensure that your job run script ends with the following commit:\n\njob.commit()\n\nWhen you include this object, AWS Glue records the timestamp and path of the job run. If you run the job again with the same path, AWS Glue processes only the new files. If you don't include this object and job bookmarks are enabled, the job reprocesses the already processed files along with the new files and creates redundancy in the job's target data store.\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html#error-job-bookmarks-reprocess-data",
        "selected_answer": "D"
      },
      {
        "author": "azure_bimonster",
        "date": "Tue 17 Sep 2024 22:09",
        "comment": "I would go with A option",
        "selected_answer": "A"
      },
      {
        "author": "EJGisME",
        "date": "Fri 06 Sep 2024 04:58",
        "comment": "A. The AWS Glue job does not have the s3:GetObjectAcl permission that is required for bookmarks to work correctly.",
        "selected_answer": "A"
      },
      {
        "author": "mzansikiller",
        "date": "Mon 19 Aug 2024 00:33",
        "comment": "Answer A\n\nthis is a job bookmarks permissions issue",
        "selected_answer": "A"
      },
      {
        "author": "antun3ra",
        "date": "Thu 08 Aug 2024 17:44",
        "comment": "For AWS Glue bookmarks to function correctly, the job needs the necessary permissions to read and write bookmark data, including the s3:GetObjectAcl permission. If these permissions are not correctly set, the job may not be able to track which files have already been processed, leading to reprocessing of previously processed files.",
        "selected_answer": "A"
      },
      {
        "author": "andrologin",
        "date": "Thu 18 Jul 2024 06:16",
        "comment": "AWS Glue Job requires the commit statement to save the last successful run/processing",
        "selected_answer": "D"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:07",
        "comment": "For me - D looks correct",
        "selected_answer": "D"
      },
      {
        "author": "Alagong",
        "date": "Sun 30 Jun 2024 02:21",
        "comment": "The commit statement (Option D) is not required for AWS Glue jobs. AWS Glue commits any open transactions to the database when all the script statements finish running.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143047-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 109 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 109,
    "question_text": "An ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.\nThe company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "AWS Lambda"
      },
      {
        "letter": "B",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MVVAA)"
      },
      {
        "letter": "C",
        "text": "AWS Step Functions"
      },
      {
        "letter": "D",
        "text": "AWS Glue"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HunkyBunky",
        "date": "Fri 05 Jul 2024 05:14",
        "comment": "B - because company want to use same tool on premises and least operational overhead",
        "selected_answer": "B"
      },
      {
        "author": "didorins",
        "date": "Wed 03 Jul 2024 20:42",
        "comment": "\"The company wants a migration solution that does not require the company to manage servers.\". How is it Amazon Managed Workflows for Apache Airflow and not Step Functions when Step Functions is the serverless of the two ?",
        "selected_answer": "C"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 17:44",
        "comment": "An ecommerce company wants to use AWS to migrate data pipelines from an on-premises environment into the AWS Cloud. The company currently uses a third-party tool in the on-premises environment to orchestrate data ingestion processes.\n\nThe company wants a migration solution that does not require the company to manage servers. The solution must be able to orchestrate Python and Bash scripts. The solution must not require the company to refactor any code.\n\nWhich solution will meet these requirements with the LEAST operational overhead?\n\nA. AWS Lambda\nB. Amazon Managed Workflows for Apache Airflow (Amazon MVVAA)\nC. AWS Step Functions\nD. AWS Glue",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:09",
        "comment": "B - best fits in task requirements",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143051-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 110 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 110,
    "question_text": "A retail company stores data from a product lifecycle management (PLM) application in an on-premises MySQL database. The PLM application frequently updates the database when transactions occur.\nThe company wants to gather insights from the PLM application in near real time. The company wants to integrate the insights with other business datasets and to analyze the combined dataset by using an Amazon Redshift data warehouse.\nThe company has already established an AWS Direct Connect connection between the on-premises infrastructure and AWS.\nWhich solution will meet these requirements with the LEAST development effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Run a scheduled AWS Glue extract, transform, and load (ETL) job to get the MySQL database updates by using a Java Database Connectivity (JDBC) connection. Set Amazon Redshift as the destination for the ETL job."
      },
      {
        "letter": "B",
        "text": "Run a full load plus CDC task in AWS Database Migration Service (AWS DMS) to continuously replicate the MySQL database changes. Set Amazon Redshift as the destination for the task."
      },
      {
        "letter": "C",
        "text": "Use the Amazon AppFlow SDK to build a custom connector for the MySQL database to continuously replicate the database changes. Set Amazon Redshift as the destination for the connector."
      },
      {
        "letter": "D",
        "text": "Run scheduled AWS DataSync tasks to synchronize data from the MySQL database. Set Amazon Redshift as the destination for the tasks."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Wed 24 Jul 2024 20:04",
        "comment": "AWS DMS allows for change data capture that will have the destination updated at near real time with changes from the source database",
        "selected_answer": "B"
      },
      {
        "author": "Fredrik1",
        "date": "Wed 24 Jul 2024 18:16",
        "comment": "Should B. Makes most sense.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143053-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 111 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 111,
    "question_text": "A marketing company uses Amazon S3 to store clickstream data. The company queries the data at the end of each day by using a SQL JOIN clause on S3 objects that are stored in separate buckets.\nThe company creates key performance indicators (KPIs) based on the objects. The company needs a serverless solution that will give users the ability to query data by partitioning the data. The solution must maintain the atomicity, consistency, isolation, and durability (ACID) properties of the data.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon S3 Select"
      },
      {
        "letter": "B",
        "text": "Amazon Redshift Spectrum"
      },
      {
        "letter": "C",
        "text": "Amazon Athena"
      },
      {
        "letter": "D",
        "text": "Amazon EMR"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 17:48",
        "comment": "C. Amazon Athena\n\nHere's why Amazon Athena is suitable:\n\nServerless: Amazon Athena is a serverless query service that allows you to run SQL queries directly on data stored in Amazon S3 without the need to manage infrastructure.\nPartitioning: Athena supports querying data by partitioning, which can significantly improve query performance by limiting the amount of data scanned.\nACID Properties: Although Amazon S3 itself does not provide ACID properties, Amazon Athena ensures consistency in query results and durability of the data stored in S3 through its managed query execution.\nCost-effective: With Amazon Athena, you only pay for the queries you run and the amount of data scanned, making it a cost-effective choice compared to managing infrastructure or using dedicated services like Amazon Redshift Spectrum or Amazon EMR.",
        "selected_answer": "C"
      },
      {
        "author": "EJGisME",
        "date": "Thu 12 Sep 2024 06:10",
        "comment": "C. Amazon Athena",
        "selected_answer": "C"
      },
      {
        "author": "EJGisME",
        "date": "Fri 06 Sep 2024 09:44",
        "comment": "Amazon Redshift Spectrum is not serverless.",
        "selected_answer": "C"
      },
      {
        "author": "andrologin",
        "date": "Fri 19 Jul 2024 06:01",
        "comment": "Athena is cost effective as it only charges for queries run",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143054-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 112 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 112,
    "question_text": "A company wants to migrate data from an Amazon RDS for PostgreSQL DB instance in the eu-east-1 Region of an AWS account named Account_A. The company will migrate the data to an Amazon Redshift cluster in the eu-west-1 Region of an AWS account named Account_B.\nWhich solution will give AWS Database Migration Service (AWS DMS) the ability to replicate data between two data stores?",
    "choices": [
      {
        "letter": "A",
        "text": "Set up an AWS DMS replication instance in Account_B in eu-west-1."
      },
      {
        "letter": "B",
        "text": "Set up an AWS DMS replication instance in Account_B in eu-east-1."
      },
      {
        "letter": "C",
        "text": "Set up an AWS DMS replication instance in a new AWS account in eu-west-1."
      },
      {
        "letter": "D",
        "text": "Set up an AWS DMS replication instance in Account_A in eu-east-1."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Thu 18 Jul 2024 06:27",
        "comment": "Redshift needs to be in the same region as the replication instance see docs:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites",
        "selected_answer": "A"
      },
      {
        "author": "XP_2600",
        "date": "Tue 24 Jun 2025 11:50",
        "comment": "The replication instance must be in the same Region as the source if the source is Amazon RDS (PostgreSQL in this case).",
        "selected_answer": "D"
      },
      {
        "author": "samadal",
        "date": "Thu 22 Aug 2024 01:23",
        "comment": "When you use WS DMS to migrate data between different AWS Regions or accounts, you must remember the following:\n\nThe replication instance must be created in the same Region as the source database.\nThe target endpoint must be created in the Region where the target data store is located.\nYou must set up the required IAM roles and permissions to enable DMS to access the source and target resources.",
        "selected_answer": "D"
      },
      {
        "author": "lool",
        "date": "Sat 06 Jul 2024 15:56",
        "comment": "Redshift has to be in the same region as the DMS\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites",
        "selected_answer": "A"
      },
      {
        "author": "bakarys",
        "date": "Wed 03 Jul 2024 00:12",
        "comment": "To enable AWS Database Migration Service (AWS DMS) to replicate data between two data stores in different AWS Regions, you should choose option A. Here’s why:\n\nOption A: Set up an AWS DMS replication instance in Account_B in eu-west-1. This approach allows you to configure replication between the Amazon RDS for PostgreSQL DB instance in eu-east-1 and the Amazon Redshift cluster in eu-west-1. By using AWS DMS, you can efficiently migrate data across Regions while minimizing downtime and ensuring data consistency",
        "selected_answer": "A"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:24",
        "comment": "B - becuase AWS DMS must be in a same region as AWS redshift cluster\n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.Prerequisites",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143056-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 113 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 113,
    "question_text": "A company uses Amazon S3 as a data lake. The company sets up a data warehouse by using a multi-node Amazon Redshift cluster. The company organizes the data files in the data lake based on the data source of each data file.\nThe company loads all the data files into one table in the Redshift cluster by using a separate COPY command for each data file location. This approach takes a long time to load all the data files into the table. The company must increase the speed of the data ingestion. The company does not want to increase the cost of the process.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use a provisioned Amazon EMR cluster to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift."
      },
      {
        "letter": "B",
        "text": "Load all the data files in parallel into Amazon Aurora. Run an AWS Glue job to load the data into Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Use an AWS Give job to copy all the data files into one folder. Use a COPY command to load the data into Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Create a manifest file that contains the data file locations. Use a COPY command to load the data into Amazon Redshift."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Fri 19 Jul 2024 06:05",
        "comment": "D is the right answer based on the docs in this page https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html",
        "selected_answer": "D"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:27",
        "comment": "Only D makes sense\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-single-copy-command.html",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143057-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 114 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 114,
    "question_text": "A company plans to use Amazon Kinesis Data Firehose to store data in Amazon S3. The source data consists of 2 MB .csv files. The company must convert the .csv files to JSON format. The company must store the files in Apache Parquet format.\nWhich solution will meet these requirements with the LEAST development effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Kinesis Data Firehose to convert the .csv files to JSON. Use an AWS Lambda function to store the files in Parquet format."
      },
      {
        "letter": "B",
        "text": "Use Kinesis Data Firehose to convert the .csv files to JSON and to store the files in Parquet format."
      },
      {
        "letter": "C",
        "text": "Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON and stores the files in Parquet format."
      },
      {
        "letter": "D",
        "text": "Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON. Use Kinesis Data Firehose to store the files in Parquet format."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "qwertyuio",
        "date": "Wed 17 Jul 2024 08:24",
        "comment": "https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html",
        "selected_answer": "D"
      },
      {
        "author": "llk",
        "date": "Wed 10 Sep 2025 17:12",
        "comment": "I agree with D",
        "selected_answer": "D"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 03:40",
        "comment": "Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first\nhttps://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html",
        "selected_answer": "D"
      },
      {
        "author": "bonds",
        "date": "Sun 20 Apr 2025 22:54",
        "comment": "Kinesis Data Firehose has native capability to convert files into json and then parquet format. There's no need to create lambda function.",
        "selected_answer": "B"
      },
      {
        "author": "JimOGrady",
        "date": "Wed 26 Mar 2025 18:04",
        "comment": "simplest and most efficient - Firehose to convert to JSON and store in Parquet - no need for Lambda function",
        "selected_answer": "B"
      },
      {
        "author": "saurwt",
        "date": "Sun 16 Mar 2025 12:42",
        "comment": "Amazon Kinesis Data Firehose does not natively support CSV to JSON conversion. However, it does support JSON to Parquet conversion.\n\nGiven that, the best approach with the least development effort is:\n\nD. Use Kinesis Data Firehose to invoke an AWS Lambda function that transforms the .csv files to JSON. Use Kinesis Data Firehose to store the files in Parquet format.",
        "selected_answer": "D"
      },
      {
        "author": "Ramdi1",
        "date": "Tue 11 Mar 2025 13:57",
        "comment": "Kinesis Data Firehose natively supports data format conversion to Parquet, reducing development effort.\nAWS Lambda is needed only for the CSV to JSON conversion, as Firehose does not support direct CSV to JSON transformation.\nFirehose then automatically converts JSON to Parquet and stores it in S3, minimizing custom code.",
        "selected_answer": "D"
      },
      {
        "author": "Salam9",
        "date": "Sat 25 Jan 2025 19:33",
        "comment": "https://aws.amazon.com/ar/about-aws/whats-new/2016/12/amazon-kinesis-firehose-can-now-prepare-and-transform-streaming-data-before-loading-it-to-data-stores/",
        "selected_answer": "B"
      },
      {
        "author": "kailu",
        "date": "Thu 09 Jan 2025 01:22",
        "comment": "Lambda handles both the CSV-to-JSON and JSON-to-Parquet transformations before Firehose stores the data in Amazon S3",
        "selected_answer": "C"
      },
      {
        "author": "zoneout",
        "date": "Wed 25 Dec 2024 00:32",
        "comment": "If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first and then you can use Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC.",
        "selected_answer": "D"
      },
      {
        "author": "kailu",
        "date": "Sat 21 Dec 2024 19:12",
        "comment": "I would go with C. D is close but Kinesis Data Firehose does not really store files in Parquet format.",
        "selected_answer": "C"
      },
      {
        "author": "michele_scar",
        "date": "Thu 14 Nov 2024 14:55",
        "comment": "https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html\n\nYou need firstly a JSON (using Lambda) to be able using Kinesis to store it in Parquet",
        "selected_answer": "D"
      },
      {
        "author": "rsmf",
        "date": "Tue 22 Oct 2024 15:59",
        "comment": "Firehose can't convert csv to json.\n\nSo, that's D",
        "selected_answer": "D"
      },
      {
        "author": "PashoQ",
        "date": "Tue 17 Sep 2024 15:07",
        "comment": "If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information",
        "selected_answer": "D"
      },
      {
        "author": "mzansikiller",
        "date": "Mon 19 Aug 2024 00:21",
        "comment": "Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. If you want to convert an input format other than JSON, such as comma-separated values (CSV) or structured text, you can use AWS Lambda to transform it to JSON first. For more information, see Transform source data in Amazon Data Firehose.\nAnswer D",
        "selected_answer": "D"
      },
      {
        "author": "Shanmahi",
        "date": "Sat 10 Aug 2024 22:03",
        "comment": "Kinesis Data Firehose: It has built-in support for data transformation and format conversion. It can directly convert incoming data from .csv to JSON format and then further convert the data to Apache Parquet format before storing it in Amazon S3.\n\nMinimal Development Effort: This option requires the least development effort because Kinesis Data Firehose handles both the transformation (from .csv to JSON) and the format conversion (to Parquet) natively. No additional AWS Lambda functions or custom code are needed.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143058-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 115 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 115,
    "question_text": "A company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Generate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use."
      },
      {
        "letter": "B",
        "text": "Update the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above."
      },
      {
        "letter": "C",
        "text": "Update the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2"
      },
      {
        "letter": "D",
        "text": "Install an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Palee",
        "date": "Sun 16 Mar 2025 10:23",
        "comment": "C is correct",
        "selected_answer": "C"
      },
      {
        "author": "Ja13",
        "date": "Mon 08 Jul 2024 18:29",
        "comment": "A company is using an AWS Transfer Family server to migrate data from an on-premises environment to AWS. Company policy mandates the use of TLS 1.2 or above to encrypt the data in transit.\n\nWhich solution will meet these requirements?\n\nA. Generate new SSH keys for the Transfer Family server. Make the old keys and the new keys available for use.\nB. Update the security group rules for the on-premises network to allow only connections that use TLS 1.2 or above.\nC. Update the security policy of the Transfer Family server to specify a minimum protocol version of TLS 1.2\nD. Install an SSL certificate on the Transfer Family server to encrypt data transfers by using TLS 1.2.",
        "selected_answer": "C"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:32",
        "comment": "Only C is good",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143060-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 116 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 116,
    "question_text": "A company wants to migrate an application and an on-premises Apache Kafka server to AWS. The application processes incremental updates that an on-premises Oracle database sends to the Kafka server. The company wants to use the replatform migration strategy instead of the refactor strategy.\nWhich solution will meet these requirements with the LEAST management overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon Kinesis Data Streams"
      },
      {
        "letter": "B",
        "text": "Amazon Managed Streaming for Apache Kafka (Amazon MSK) provisioned cluster"
      },
      {
        "letter": "C",
        "text": "Amazon Kinesis Data Firehose"
      },
      {
        "letter": "D",
        "text": "Amazon Managed Streaming for Apache Kafka (Amazon MSK) Serverless"
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HunkyBunky",
        "date": "Wed 03 Jul 2024 06:43",
        "comment": "D - becase this is lift-and-shift migration and serveless - because LEAST management overhead",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143061-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 117 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 117,
    "question_text": "A data engineer is building an automated extract, transform, and load (ETL) ingestion pipeline by using AWS Glue. The pipeline ingests compressed files that are in an Amazon S3 bucket. The ingestion pipeline must support incremental data processing.\nWhich AWS Glue feature should the data engineer use to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Workflows"
      },
      {
        "letter": "B",
        "text": "Triggers"
      },
      {
        "letter": "C",
        "text": "Job bookmarks"
      },
      {
        "letter": "D",
        "text": "Classifiers"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "andrologin",
        "date": "Fri 19 Jul 2024 06:39",
        "comment": "C AWS GLue bookmarks are used to implement incremental processing",
        "selected_answer": "C"
      },
      {
        "author": "Ja13",
        "date": "Wed 03 Jul 2024 18:18",
        "comment": "C. Job bookmarks\n\nHere's why job bookmarks are the appropriate feature:\n\nIncremental Processing: Job bookmarks in AWS Glue help track the last processed state of data in Amazon S3. They enable the ETL job to resume from where it left off in case of interruptions or subsequent runs, ensuring that only new or modified data since the last successful run is processed (incremental processing).\nAutomated ETL: Job bookmarks work seamlessly within AWS Glue ETL jobs, allowing the job to efficiently manage the state of processed data without the need for manual intervention.\nSupport for Compressed Files: AWS Glue natively supports reading compressed files from Amazon S3, so the ingestion pipeline can handle compressed data formats efficiently.",
        "selected_answer": "C"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:36",
        "comment": "C - is right",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143062-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 118 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 118,
    "question_text": "A banking company uses an application to collect large volumes of transactional data. The company uses Amazon Kinesis Data Streams for real-time analytics. The company’s application uses the PutRecord action to send data to Kinesis Data Streams.\nA data engineer has observed network outages during certain times of day. The data engineer wants to configure exactly-once delivery for the entire processing pipeline.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source."
      },
      {
        "letter": "B",
        "text": "Update the checkpoint configuration of the Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) data collection application to avoid duplicate processing of events."
      },
      {
        "letter": "C",
        "text": "Design the data source so events are not ingested into Kinesis Data Streams multiple times."
      },
      {
        "letter": "D",
        "text": "Stop using Kinesis Data Streams. Use Amazon EMR instead. Use Apache Flink and Apache Spark Streaming in Amazon EMR."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Thu 14 Aug 2025 22:22",
        "comment": "•\tB (checkpointing) prevents reprocessing beyond checkpoints but doesn’t stop duplicates created at ingestion.\n\t•\tC isn’t realistic during outages/retries.\n\t•\tD is unnecessary; switching to EMR doesn’t solve producer-side duplicates.",
        "selected_answer": "A"
      },
      {
        "author": "Ramdi1",
        "date": "Tue 11 Mar 2025 14:18",
        "comment": "Amazon Kinesis Data Streams does not provide exactly-once delivery natively. It ensures at-least-once delivery, meaning that under certain conditions (e.g., network failures, retries), duplicate records can occur. To achieve exactly-once processing, deduplication must be handled at the application level.",
        "selected_answer": "A"
      },
      {
        "author": "PashoQ",
        "date": "Tue 17 Sep 2024 15:11",
        "comment": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.",
        "selected_answer": "A"
      },
      {
        "author": "Ja13",
        "date": "Mon 08 Jul 2024 18:41",
        "comment": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nExplanation:\nExactly-Once Delivery: Ensuring exactly-once delivery is a challenge in distributed systems, especially in the presence of network outages and retries. By embedding a unique ID in each record at the source, you can track and identify duplicate records during processing. This approach allows you to implement idempotent processing, where duplicate records can be detected and discarded, ensuring that each record is processed exactly once.\nDe-duplication Logic: Implementing de-duplication logic based on unique IDs ensures that even if the same record is ingested multiple times due to retries or network issues, it will be processed only once by the downstream applications.",
        "selected_answer": "A"
      },
      {
        "author": "bakarys",
        "date": "Tue 02 Jul 2024 14:37",
        "comment": "A. Design the application so it can remove duplicates during processing by embedding a unique ID in each record at the source.\n\nThis approach ensures that even if a record is sent more than once due to network outages or other issues, it will only be processed once because the unique ID can be used to identify and remove any duplicates. This is a common pattern for achieving exactly-once processing semantics in distributed systems. The other options do not guarantee exactly-once delivery across the entire pipeline. Option B is partially correct but it only avoids duplicate processing within the Amazon Managed Service for Apache Flink, not across the entire pipeline. Option C is not always feasible because network issues and other factors can lead to events being ingested into Kinesis Data Streams multiple times. Option D involves changing the entire technology stack, which is not necessary to achieve the desired outcome and could introduce additional complexity and cost.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143120-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 119 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 119,
    "question_text": "A company stores logs in an Amazon S3 bucket. When a data engineer attempts to access several log files, the data engineer discovers that some files have been unintentionally deleted.\nThe data engineer needs a solution that will prevent unintentional file deletion in the future.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Manually back up the S3 bucket on a regular basis."
      },
      {
        "letter": "B",
        "text": "Enable S3 Versioning for the S3 bucket."
      },
      {
        "letter": "C",
        "text": "Configure replication for the S3 bucket."
      },
      {
        "letter": "D",
        "text": "Use an Amazon S3 Glacier storage class to archive the data that is in the S3 bucket."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:39",
        "comment": "B - is right answer",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/143122-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 120 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 120,
    "question_text": "A telecommunications company collects network usage data throughout each day at a rate of several thousand data points each second. The company runs an application to process the usage data in real time. The company aggregates and stores the data in an Amazon Aurora DB instance.\nSudden drops in network usage usually indicate a network outage. The company must be able to identify sudden drops in network usage so the company can take immediate remedial actions.\nWhich solution will meet this requirement with the LEAST latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Lambda function to query Aurora for drops in network usage. Use Amazon EventBridge to automatically invoke the Lambda function every minute."
      },
      {
        "letter": "B",
        "text": "Modify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage."
      },
      {
        "letter": "C",
        "text": "Replace the Aurora database with an Amazon DynamoDB table. Create an AWS Lambda function to query the DynamoDB table for drops in network usage every minute. Use DynamoDB Accelerator (DAX) between the processing application and DynamoDB table."
      },
      {
        "letter": "D",
        "text": "Create an AWS Lambda function within the Database Activity Streams feature of Aurora to detect drops in network usage."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "youonebe",
        "date": "Wed 14 May 2025 01:57",
        "comment": "If you choose D, lambda function still needs to get invoked which introduces latency.\nKinesis Data Analytics is real-time",
        "selected_answer": "B"
      },
      {
        "author": "Adrifersilva",
        "date": "Thu 10 Oct 2024 23:48",
        "comment": "Regarding D, Database Activity Streams in Aurora are primarily for auditing databases actities, not for analyzing app data.",
        "selected_answer": "B"
      },
      {
        "author": "antun3ra",
        "date": "Thu 08 Aug 2024 18:05",
        "comment": "B is the correct answer",
        "selected_answer": "B"
      },
      {
        "author": "portland",
        "date": "Wed 07 Aug 2024 01:38",
        "comment": "reduces latency because it is analyze before the data even gets to the Aurora DB",
        "selected_answer": "B"
      },
      {
        "author": "Ja13",
        "date": "Mon 08 Jul 2024 19:59",
        "comment": "The best solution to identify sudden drops in network usage with the least latency is:\n\nB. Modify the processing application to publish the data to an Amazon Kinesis data stream. Create an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to detect drops in network usage.\n\nThis approach ensures real-time processing with minimal latency and allows immediate detection and response to network usage drops.",
        "selected_answer": "B"
      },
      {
        "author": "HunkyBunky",
        "date": "Tue 02 Jul 2024 16:44",
        "comment": "I guess D. The question is which solution helps to identitfy sudden drops to take immediate actions",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145188-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 121 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 121,
    "question_text": "A data engineer is processing and analyzing multiple terabytes of raw data that is in Amazon S3. The data engineer needs to clean and prepare the data. Then the data engineer needs to load the data into Amazon Redshift for analytics.\nThe data engineer needs a solution that will give data analysts the ability to perform complex queries. The solution must eliminate the need to perform complex extract, transform, and load (ETL) processes or to manage infrastructure.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon EMR to prepare the data. Use AWS Step Functions to load the data into Amazon Redshift. Use Amazon QuickSight to run queries."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue DataBrew to prepare the data. Use AWS Glue to load the data into Amazon Redshift. Use Amazon Redshift to run queries."
      },
      {
        "letter": "C",
        "text": "Use AWS Lambda to prepare the data. Use Amazon Kinesis Data Firehose to load the data into Amazon Redshift. Use Amazon Athena to run queries."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue to prepare the data. Use AWS Database Migration Service (AVVS DMS) to load the data into Amazon Redshift. Use Amazon Redshift Spectrum to run queries."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "teo2157",
        "date": "Sun 25 Aug 2024 08:24",
        "comment": "It can´t be D as DMS doesn´t support S3 as a source, it's B as it achieve all the goals described in the subject.",
        "selected_answer": "B"
      },
      {
        "author": "seouk",
        "date": "Thu 22 Aug 2024 05:43",
        "comment": "the LEAST operational overhead ...",
        "selected_answer": "D"
      },
      {
        "author": "catoteja",
        "date": "Sun 11 Aug 2024 15:58",
        "comment": "B. They can do the \"complex\" queries in redshift.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145187-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 122 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 122,
    "question_text": "A company uses an AWS Lambda function to transfer files from a legacy SFTP environment to Amazon S3 buckets. The Lambda function is VPC enabled to ensure that all communications between the Lambda function and other AVS services that are in the same VPC environment will occur over a secure network.\nThe Lambda function is able to connect to the SFTP environment successfully. However, when the Lambda function attempts to upload files to the S3 buckets, the Lambda function returns timeout errors. A data engineer must resolve the timeout issues in a secure way.\nWhich solution will meet these requirements in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a NAT gateway in the public subnet of the VPC. Route network traffic to the NAT gateway."
      },
      {
        "letter": "B",
        "text": "Create a VPC gateway endpoint for Amazon S3. Route network traffic to the VPC gateway endpoint."
      },
      {
        "letter": "C",
        "text": "Create a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint."
      },
      {
        "letter": "D",
        "text": "Use a VPC internet gateway to connect to the internet. Route network traffic to the VPC internet gateway."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "ArunRav",
        "date": "Wed 07 Aug 2024 15:20",
        "comment": "Option B - VPC Gateway Endpoint for Amazon S3",
        "selected_answer": "B"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 04:21",
        "comment": "You can access Amazon S3 from your VPC using gateway VPC endpoints. After you create the gateway endpoint, you can add it as a target in your route table for traffic destined from your VPC to Amazon S3.\n\nThere is no additional charge for using gateway endpoints.\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html",
        "selected_answer": "B"
      },
      {
        "author": "Ashishk1",
        "date": "Fri 23 Aug 2024 04:27",
        "comment": "The solution that will meet the requirements of resolving the timeout issues when uploading files from the Lambda function to Amazon S3 buckets in a secure and cost-effective way is C. Create a VPC interface endpoint for Amazon S3. Route network traffic to the VPC interface endpoint .",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145289-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 123 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 123,
    "question_text": "A company reads data from customer databases that run on Amazon RDS. The databases contain many inconsistent fields. For example, a customer record field that iPnamed place_id in one database is named location_id in another database. The company needs to link customer records across different databases, even when customer record fields do not match.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use the FindMatches transform to find duplicate records in the data."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue crawler to craw the databases. Use the FindMatches transform to find duplicate records in the data. Evaluate and tune the transform by evaluating the performance and results."
      },
      {
        "letter": "C",
        "text": "Create an AWS Glue crawler to craw the databases. Use Amazon SageMaker to construct Apache Spark ML pipelines to find duplicate records in the data."
      },
      {
        "letter": "D",
        "text": "Create a provisioned Amazon EMR cluster to process and analyze data in the databases. Connect to the Apache Zeppelin notebook. Use an Apache Spark ML model to find duplicate records in the data. Evaluate and tune the model by evaluating the performance and results."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Wed 18 Dec 2024 09:48",
        "comment": "AWS Glue Crawler:\nAutomatically discovers the schema and structure of data in the RDS databases, saving significant manual effort.\nCreates a unified data catalog that can be queried or transformed.",
        "selected_answer": "B"
      },
      {
        "author": "komorebi",
        "date": "Sat 10 Aug 2024 00:48",
        "comment": "Answer is B",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145291-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 124 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 124,
    "question_text": "A finance company receives data from third-party data providers and stores the data as objects in an Amazon S3 bucket.\nThe company ran an AWS Glue crawler on the objects to create a data catalog. The AWS Glue crawler created multiple tables. However, the company expected that the crawler would create only one table.\nThe company needs a solution that will ensure the AVS Glue crawler creates only one table.\nWhich combination of solutions will meet this requirement? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Ensure that the object format, compression type, and schema are the same for each object."
      },
      {
        "letter": "B",
        "text": "Ensure that the object format and schema are the same for each object. Do not enforce consistency for the compression type of each object."
      },
      {
        "letter": "C",
        "text": "Ensure that the schema is the same for each object. Do not enforce consistency for the file format and compression type of each object."
      },
      {
        "letter": "D",
        "text": "Ensure that the structure of the prefix for each S3 object name is consistent."
      },
      {
        "letter": "E",
        "text": "Ensure that all S3 object names follow a similar pattern."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Salam9",
        "date": "Sat 25 Jan 2025 20:11",
        "comment": "I have seen this official answer in the practical exam in the AWS Skills builder website",
        "selected_answer": "AD"
      },
      {
        "author": "kailu",
        "date": "Thu 09 Jan 2025 01:46",
        "comment": "D focuses on the S3 prefix structure, which affects partitioning but not the creation of a single table. Consistency in file format and schema is much more important in determining how AWS Glue handles the data.",
        "selected_answer": "AB"
      },
      {
        "author": "komorebi",
        "date": "Sat 10 Aug 2024 00:53",
        "comment": "Answer is AD",
        "selected_answer": "AD"
      },
      {
        "author": "teo2157",
        "date": "Fri 09 Aug 2024 09:25",
        "comment": "To ensure that the AWS Glue crawler creates only one table and handles the object format, compression type, schema, and prefix structure consistently:\nEnsure Consistent Object Format, Compression Type, Schema, and Prefix Structure\n1. **Consistent Object Format**:\n   - Ensure that all objects in the S3 bucket are in the same format (e.g., CSV, JSON, Parquet).\n\n2. **Consistent Compression Type**:\n   - Ensure that all objects use the same compression type (e.g., GZIP, Snappy).\n\n3. **Consistent Schema**:\n   - Ensure that all objects have the same schema (i.e., the same fields with the same data types).\n\n4. **Consistent Prefix Structure**:\n   - Ensure that all objects follow a consistent naming convention and prefix structure in the S3 bucket (e.g., `s3://your-bucket/path/to/data/`).",
        "selected_answer": "AD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145713-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 125 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 125,
    "question_text": "An application consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue. The application experiences occasional downtime. As a result of the downtime, messages within the queue expire and are deleted after 1 day. The message deletions cause data loss for the application.\nWhich solutions will minimize data loss for the application? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Increase the message retention period"
      },
      {
        "letter": "B",
        "text": "Increase the visibility timeout."
      },
      {
        "letter": "C",
        "text": "Attach a dead-letter queue (DLQ) to the SQS queue."
      },
      {
        "letter": "D",
        "text": "Use a delay queue to delay message delivery"
      },
      {
        "letter": "E",
        "text": "Reduce message processing time."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "axantroff",
        "date": "Wed 25 Dec 2024 01:00",
        "comment": "In my opinion, A is obvious and one of the two correct answers. Additionally, I checked B, C, and D in more detail, and they basically do not make sense as they do not contribute in any way to handling messages that were just delayed. See the documentation for reference:\n\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\nhttps://aws.amazon.com/what-is/dead-letter-queue/\nSo, only E remains as another valid option. It makes sense because the faster we are able to process events, the less likely we are to violate the expiration policy",
        "selected_answer": "AE"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Wed 18 Dec 2024 09:54",
        "comment": "Increasing the message retention period (A) ensures messages are available longer, while attaching a dead-letter queue (C) allows recovery and reprocessing of unprocessed messages, effectively minimizing data loss.",
        "selected_answer": "AC"
      },
      {
        "author": "altonh",
        "date": "Mon 09 Dec 2024 08:33",
        "comment": "It cannot be C. Messages go to DLQ only if processed. But if the message is not processed at all and it expires, then it will be deleted from the queue.",
        "selected_answer": "AE"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:48",
        "comment": "Answer is AC",
        "selected_answer": "AC"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 13:45",
        "comment": "To minimize data loss for the application consuming messages from an Amazon SQS queue, the following two solutions are most effective:\n\nA. Increase the message retention period**: By increasing the message retention period, you ensure that messages remain in the queue for a longer duration before being automatically deleted. This provides more time for the application to recover from downtime and process the messages, thereby reducing the chance of data loss due to message expiration.\n\nC. Attach a dead-letter queue (DLQ) to the SQS queue**: A DLQ can be used to capture messages that cannot be processed successfully. When messages fail to be processed after a certain number of attempts (as defined by the redrive policy), they are moved to the DLQ. This allows you to investigate and handle these messages separately, preventing data loss.",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145714-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 126 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 126,
    "question_text": "A company is creating near real-time dashboards to visualize time series data. The company ingests data into Amazon Managed Streaming for Apache Kafka (Amazon MSK). A customized data pipeline consumes the data. The pipeline then writes data to Amazon Keyspaces (for Apache Cassandra), Amazon OpenSearch Service, and Apache Avro objects in Amazon S3.\nWhich solution will make the data available for the data visualizations with the LEAST latency?",
    "choices": [
      {
        "letter": "A",
        "text": "Create OpenSearch Dashboards by using the data from OpenSearch Service."
      },
      {
        "letter": "B",
        "text": "Use Amazon Athena with an Apache Hive metastore to query the Avro objects in Amazon S3. Use Amazon Managed Grafana to connect to Athena and to create the dashboards."
      },
      {
        "letter": "C",
        "text": "Use Amazon Athena to query the data from the Avro objects in Amazon S3. Configure Amazon Keyspaces as the data catalog. Connect Amazon QuickSight to Athena to create the dashboards."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue to catalog the data. Use S3 Select to query the Avro objects in Amazon S3. Connect Amazon QuickSight to the S3 bucket to create the dashboards."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "jacob_nz",
        "date": "Sun 24 Nov 2024 11:05",
        "comment": "Timeseries/ Graph visualizations we use Graphana",
        "selected_answer": "B"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:48",
        "comment": "Answer is A",
        "selected_answer": "A"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 13:48",
        "comment": "Option A: Create OpenSearch Dashboards by using the data from OpenSearch Service is the best choice for achieving the least latency. OpenSearch is designed for low-latency data retrieval and visualization, making it ideal for near real-time dashboards",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150336-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 127 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 127,
    "question_text": "A data engineer maintains a materialized view that is based on an Amazon Redshift database. The view has a column named load_date that stores the date when each row was loaded.\nThe data engineer needs to reclaim database storage space by deleting all the rows from the materialized view.\nWhich command will reclaim the MOST database storage space?",
    "choices": [
      {
        "letter": "A",
        "text": "DELETE FROM materialized_view_name where 1=1"
      },
      {
        "letter": "B",
        "text": "TRUNCATE materialized_view_name"
      },
      {
        "letter": "C",
        "text": "VACUUM table_name where load_date<=current_datematerializedview"
      },
      {
        "letter": "D",
        "text": "DELETE FROM materialized_view_name where load_date<=current_date"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Nickalodeon99",
        "date": "Sun 21 Sep 2025 21:39",
        "comment": "Option B (Correct):\nThe command also works on a materialized view.\nSyntax:\nTRUNCATE materialized_view_name\n\nTRUNCATE is much more efficient than DELETE and doesn't require a VACUUM and ANALYZE. However, be aware that TRUNCATE commits the transaction in which it is run.\n\nThe following example uses the TRUNCATE command to delete all of the rows from a materialized view.\n\nSyntax:\ntruncate my_materialized_view;\n\nIt deletes all records in the materialized view and leaves the materialized view and its schema intact. In the query, the materialized view name is a sample.\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_TRUNCATE.html",
        "selected_answer": "B"
      },
      {
        "author": "rebasheer",
        "date": "Sat 16 Aug 2025 14:38",
        "comment": "B is the answer",
        "selected_answer": "B"
      },
      {
        "author": "AminTriton",
        "date": "Thu 14 Aug 2025 23:18",
        "comment": "In terms of immediately reclaiming the most space in Redshift, TRUNCATE (or dropping the materialized view) is the fastest and most space-efficient.",
        "selected_answer": "B"
      },
      {
        "author": "XP_2600",
        "date": "Tue 24 Jun 2025 13:35",
        "comment": "Redshift does not support the TRUNCATE command on materialized views.",
        "selected_answer": "A"
      },
      {
        "author": "bad1ccc",
        "date": "Fri 04 Apr 2025 10:52",
        "comment": "When you TRUNCATE a materialized view in Amazon Redshift, it removes all rows from the view and reclaims the most storage space because the operation does not log individual row deletions. This is far more efficient in terms of both time and space than a DELETE operation.",
        "selected_answer": "B"
      },
      {
        "author": "JimOGrady",
        "date": "Mon 03 Mar 2025 20:04",
        "comment": "the key is \"reclaim database storage space\"\nDelete does not reclaim disk space",
        "selected_answer": "B"
      },
      {
        "author": "sravanscr",
        "date": "Fri 31 Jan 2025 16:41",
        "comment": "in AWS Redshift, you can use the \"TRUNCATE\" command to delete all rows from a materialized view, effectively \"truncating\" it, especially when the materialized view is configured for streaming ingestion; this is a faster way to clear the data compared to a \"DELETE\" statement.",
        "selected_answer": "B"
      },
      {
        "author": "YUICH",
        "date": "Mon 27 Jan 2025 04:04",
        "comment": "(B) TRUNCATE is invalid for materialized views, so it is excluded.\nIn actual operations, to most effectively reuse storage, you need to delete all rows with a DELETE statement and then run VACUUM, as shown in (A) or (D).\nIf you want to delete everything, option (A) is the most straightforward approach.",
        "selected_answer": "A"
      },
      {
        "author": "A_E_M",
        "date": "Mon 20 Jan 2025 17:49",
        "comment": "Why this is the best option:\nEfficiency:\nBy using \"WHERE 1=1\", the database doesn't need to iterate through each row individually to check a specific condition, resulting in faster deletion of all data.\nStorage reclamation:\nDeleting all rows using this method will free up the most storage space within the materialized view. \nImportant Considerations:\nTRUNCATE vs DELETE:\nWhile \"TRUNCATE\" can also be used to remove all data from a table, it is not recommended for materialized views in Redshift as it might not always reclaim all the storage space effectively.\nVACUUM command:\n\"VACUUM\" is used to reclaim space within a table after deletions, but it's not necessary when deleting all rows using \"DELETE FROM ... WHERE 1=1;\" as the entire table will be emptied.",
        "selected_answer": "A"
      },
      {
        "author": "AgboolaKun",
        "date": "Fri 08 Nov 2024 16:37",
        "comment": "B is the correct answer.\n\nHere is why:\nTRUNCATE is the most efficient way to remove all rows from a table or materialized view in Amazon Redshift. It's faster than DELETE and immediately reclaims disk space.\n\nTRUNCATE removes all rows in a table without scanning them individually. This makes it much faster than DELETE operations, especially for large tables.\n\nTRUNCATE automatically performs a VACUUM operation, which sorts the table and reclaims space.\n\nTRUNCATE resets any auto-increment columns.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145715-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 128 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 128,
    "question_text": "A media company wants to use Amazon OpenSearch Service to analyze rea-time data about popular musical artists and songs. The company expects to ingest millions of new data events every day. The new data events will arrive through an Amazon Kinesis data stream. The company must transform the data and then ingest the data into the OpenSearch Service domain.\nWhich method should the company use to ingest the data with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service."
      },
      {
        "letter": "B",
        "text": "Use a Logstash pipeline that has prebuilt filters to transform the data and deliver the transformed data to OpenSearch Service."
      },
      {
        "letter": "C",
        "text": "Use an AWS Lambda function to call the Amazon Kinesis Agent to transform the data and deliver the transformed data OpenSearch Service."
      },
      {
        "letter": "D",
        "text": "Use the Kinesis Client Library (KCL) to transform the data and deliver the transformed data to OpenSearch Service."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Thu 14 Aug 2025 23:21",
        "comment": "Why not B? Requires provisioning and managing EC2 instances or containers for Logstash.  \nYou’d have to handle scaling, patching, and monitoring.  \n-> Higher operational overhead",
        "selected_answer": "A"
      },
      {
        "author": "Evan_Lin",
        "date": "Thu 20 Feb 2025 03:27",
        "comment": "why not B?\nLogstash is an open-source data ingestion tool that allows you to collect data from various sources, transform it, and send it to your desired destination. With prebuilt filters and support for over 200 plugins, Logstash allows users to easily ingest data regardless of the data source or type.",
        "selected_answer": "B"
      },
      {
        "author": "maddyr",
        "date": "Sat 11 Jan 2025 00:08",
        "comment": "Logstash is a lightweight, open-source, server-side data processing pipeline that allows you to collect data from various sources, transform it on the fly, and send it to your desired destination. It is most often used as a data pipeline for Elasticsearch, an open-source analytics and search engine\nhttps://aws.amazon.com/what-is/elk-stack/#seo-faq-pairs#what-is-the-elk-stack",
        "selected_answer": "B"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:49",
        "comment": "Answer is A",
        "selected_answer": "A"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 13:50",
        "comment": "Option A: Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service is the best choice for achieving the least operational overhead. Kinesis Data Firehose is a managed service that automates the data ingestion process, scales seamlessly, and integrates directly with OpenSearch Service, minimizing the need for manual intervention and infrastructure management.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145293-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 129 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 129,
    "question_text": "A company stores customer data tables that include customer addresses in an AWS Lake Formation data lake. To comply with new regulations, the company must ensure that users cannot access data for customers who are in Canada.\nThe company needs a solution that will prevent user access to rows for customers who are in Canada.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Set a row-level filter to prevent user access to a row where the country is Canada."
      },
      {
        "letter": "B",
        "text": "Create an IAM role that restricts user access to an address where the country is Canada."
      },
      {
        "letter": "C",
        "text": "Set a column-level filter to prevent user access to a row where the country is Canada."
      },
      {
        "letter": "D",
        "text": "Apply a tag to all rows where Canada is the country. Prevent user access where the tag is equal to “Canada”."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AgboolaKun",
        "date": "Fri 08 Nov 2024 16:47",
        "comment": "The solution that will meet the requirement with the least operational effort is A.\n\nHere's why:\n\nRow-level security: AWS Lake Formation provides built-in row-level security, which allows you to control access to specific rows in a table based on conditions. This is precisely what's needed in this scenario.\n\nLeast operational effort: Once set up, this filter will automatically apply to all queries without needing to modify the data or create complex IAM policies.\n\nScalability: As new data is added to the table, the filter will automatically apply, requiring no additional effort.\n\nPrecision: It directly addresses the requirement by preventing access to rows where the country is Canada, without affecting other data.",
        "selected_answer": "A"
      },
      {
        "author": "komorebi",
        "date": "Sat 10 Aug 2024 00:49",
        "comment": "Answer is A",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145294-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 130 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 130,
    "question_text": "A company has implemented a lake house architecture in Amazon Redshift. The company needs to give users the ability to authenticate into Redshift query editor by using a third-party identity provider (IdP).\nA data engineer must set up the authentication mechanism.\nWhat is the first step the data engineer should take to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Register the third-party IdP as an identity provider in the configuration settings of the Redshift cluster."
      },
      {
        "letter": "B",
        "text": "Register the third-party IdP as an identity provider from within Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Register the third-party IdP as an identity provider for AVS Secrets Manager. Configure Amazon Redshift to use Secrets Manager to manage user credentials."
      },
      {
        "letter": "D",
        "text": "Register the third-party IdP as an identity provider for AWS Certificate Manager (ACM). Configure Amazon Redshift to use ACM to manage user credentials."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "PashoQ",
        "date": "Wed 18 Sep 2024 10:33",
        "comment": "https://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html\nregister the identity provider with Amazon Redshift, using SQL statements, which set authentication parameters that are unique to the identity provider.",
        "selected_answer": "B"
      },
      {
        "author": "komorebi",
        "date": "Sat 10 Aug 2024 00:49",
        "comment": "Answer is A",
        "selected_answer": "A"
      },
      {
        "author": "Nickalodeon99",
        "date": "Sun 21 Sep 2025 22:13",
        "comment": "Option B Correct:\nThis Question was in AWS Official Practice Exam and they provide explanation:\n\nAmazon Redshift provides native IdP federation. Therefore, you can use your third-party IdP for authentication and permission management. To use this feature, you must first register the IdP with Amazon Redshift. Then, Amazon Redshift will trust the IdP for authentication. After you register the IdP, you can configure your Redshift clusters to use the registered IdP for authentication.\n\nLearn more about how to set up an IdP on :\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html#redshift-iam-access-control-native-idp-setup",
        "selected_answer": "B"
      },
      {
        "author": "solopez_111",
        "date": "Fri 31 Jan 2025 19:49",
        "comment": "Since the question is asking for \"The first step\", the correct answer is A.\n\"First, you register Amazon Redshift as a third-party application with your identity provider, requesting the necessary API permissions\"\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html",
        "selected_answer": "A"
      },
      {
        "author": "YUICH",
        "date": "Thu 30 Jan 2025 11:06",
        "comment": "Why Option (A) is Correct\nRedshift Uses SAML at the Cluster Level\nTo enable single sign-on with a SAML 2.0–compatible IdP (for example, Okta or Azure AD) for Redshift Query Editor, you register the IdP by uploading its SAML metadata in the Amazon Redshift console. This is done at the cluster configuration or security level—not “within” the database engine itself.\n\nOption (B): “Within Amazon Redshift”\nThere is no direct command such as CREATE IDENTITY PROVIDER inside Redshift SQL. Federating a third-party IdP requires configuring the cluster to trust that IdP’s SAML metadata. That is done via the AWS console or CLI at the cluster level, not by running commands inside the database.",
        "selected_answer": "B"
      },
      {
        "author": "Salam9",
        "date": "Sat 25 Jan 2025 20:35",
        "comment": "I have seen this official answer in the practical exam on the AWS Skills Builder website.\nLearn more https://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html#redshift-iam-access-control-native-idp-setup",
        "selected_answer": "B"
      },
      {
        "author": "BigMrT",
        "date": "Sun 05 Jan 2025 20:56",
        "comment": "Redshift does not support directly registering the IdP \"within\" the service. The registration must be done through the cluster configuration settings.",
        "selected_answer": "A"
      },
      {
        "author": "paali",
        "date": "Mon 16 Dec 2024 11:30",
        "comment": "o complete the preliminary setup between the identity provider and Amazon Redshift, you perform a couple of steps: First, you register Amazon Redshift as a third-party application with your identity provider, requesting the necessary API permissions. Then you create users and groups in the identity provider. Last, you register the identity provider with Amazon Redshift, using SQL statements, which set authentication parameters that are unique to the identity provider. As part of registering the identity provider with Redshift, you assign a namespace to make sure users and roles are grouped correctly.",
        "selected_answer": "B"
      },
      {
        "author": "RockyLeon",
        "date": "Tue 26 Nov 2024 17:28",
        "comment": "https://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-access-control-native-idp.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145106-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 131 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 131,
    "question_text": "A company currently uses a provisioned Amazon EMR cluster that includes general purpose Amazon EC2 instances. The EMR cluster uses EMR managed scaling between one to five task nodes for the company’s long-running Apache Spark extract, transform, and load (ETL) job. The company runs the ETL job every day.\nWhen the company runs the ETL job, the EMR cluster quickly scales up to five nodes. The EMR cluster often reaches maximum CPU usage, but the memory usage remains under 30%.\nThe company wants to modify the EMR cluster configuration to reduce the EMR costs to run the daily ETL job.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Increase the maximum number of task nodes for EMR managed scaling to 10."
      },
      {
        "letter": "B",
        "text": "Change the task node type from general purpose EC2 instances to memory optimized EC2 instances."
      },
      {
        "letter": "C",
        "text": "Switch the task node type from general purpose Re instances to compute optimized EC2 instances."
      },
      {
        "letter": "D",
        "text": "Reduce the scaling cooldown period for the provisioned EMR cluster."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Tester_TKK",
        "date": "Tue 22 Apr 2025 01:03",
        "comment": "Your Spark ETL is clearly CPU‑bound (100% CPU, <30% memory), so you’ll get better price‑performance by trading in excess RAM for extra vCPUs rather than adding nodes, upsizing memory, or tweaking cooldowns.",
        "selected_answer": "C"
      },
      {
        "author": "AgboolaKun",
        "date": "Fri 08 Nov 2024 17:06",
        "comment": "C is the correct answer. \n\nHere is why:\nCompute optimized Amazon EC2 instances are less expensive per CPU core than general purpose instances, making them the better choice for workloads that require high processing power, as they prioritize CPU cores over memory, resulting in a lower cost per vCPU compared to general purpose instances.",
        "selected_answer": "C"
      },
      {
        "author": "antun3ra",
        "date": "Wed 07 Aug 2024 19:41",
        "comment": "current situation shows that the EMR cluster is reaching maximum CPU usage, but memory usage remains low (under 30%). This indicates that the workload is CPU-bound rather than memory-bound.",
        "selected_answer": "C"
      },
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 07:33",
        "comment": "Since the ETL job reaches maximum CPU usage but not memory usage, switching from general-purpose instances to compute-optimized instances (such as C5 or C6g instances) can provide better performance per dollar for CPU-bound workloads.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145107-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 132 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 132,
    "question_text": "A company uploads .csv files to an Amazon S3 bucket. The company’s data platform team has set up an AWS Glue crawler to perform data discovery and to create the tables and schemas.\nAn AWS Glue job writes processed data from the tables to an Amazon Redshift database. The AWS Glue job handles column mapping and creates the Amazon Redshift tables in the Redshift database appropriately.\nIf the company reruns the AWS Glue job for any reason, duplicate records are introduced into the Amazon Redshift tables. The company needs a solution that will update the Redshift tables without duplicates.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Modify the AWS Glue job to copy the rows into a staging Redshift table. Add SQL commands to update the existing rows with new values from the staging Redshift table."
      },
      {
        "letter": "B",
        "text": "Modify the AWS Glue job to load the previously inserted data into a MySQL database. Perform an upsert operation in the MySQL database. Copy the results to the Amazon Redshift tables."
      },
      {
        "letter": "C",
        "text": "Use Apache Spark’s DataFrame dropDuplicates() API to eliminate duplicates. Write the data to the Redshift tables."
      },
      {
        "letter": "D",
        "text": "Use the AWS Glue ResolveChoice built-in transform to select the value of the column from the most recent record."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 07:40",
        "comment": "Two step approach involving creating a staging table, followed by using Redshift's merge statement to update the target table from staging table and finally truncate/housekeep the staging table.",
        "selected_answer": "A"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 06:42",
        "comment": "Write all processed data (including potential duplicates) to a temporary staging table in Redshift.\nExecute SQL commands in Redshift to identify and either update existing records in the target table with the new values from the staging table (based on a unique key) or insert new records if they don't already exist.\nOptionally truncate the staging table after the upsert operation. This method leverages Redshift's SQL capabilities for efficient data manipulation and ensures no duplicates in the final tables.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145007-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 133 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 133,
    "question_text": "A company is using Amazon Redshift to build a data warehouse solution. The company is loading hundreds of files into a fact table that is in a Redshift cluster.\nThe company wants the data warehouse solution to achieve the greatest possible throughput. The solution must use cluster resources optimally when the company loads data into the fact table.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use multiple COPY commands to load the data into the Redshift cluster."
      },
      {
        "letter": "B",
        "text": "Use S3DistCp to load multiple files into Hadoop Distributed File System (HDFS). Use an HDFS connector to ingest the data into the Redshift cluster."
      },
      {
        "letter": "C",
        "text": "Use a number of INSERT statements equal to the number of Redshift cluster nodes. Load the data in parallel into each node."
      },
      {
        "letter": "D",
        "text": "Use a single COPY command to load the data into the Redshift cluster."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "cas_tori",
        "date": "Wed 14 Aug 2024 10:21",
        "comment": "this is D",
        "selected_answer": "D"
      },
      {
        "author": "antun3ra",
        "date": "Wed 07 Aug 2024 19:45",
        "comment": "A single COPY command automatically parallelizes the load operation across all nodes in the Redshift cluster. This ensures optimal use of cluster resources.",
        "selected_answer": "D"
      },
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 07:44",
        "comment": "Agree with canace; Redshift's copy command uses MPP architecture to read and load in parallel from files into DWH.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145116-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 134 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 134,
    "question_text": "A company ingests data from multiple data sources and stores the data in an Amazon S3 bucket. An AWS Glue extract, transform, and load (ETL) job transforms the data and writes the transformed data to an Amazon S3 based data lake. The company uses Amazon Athena to query the data that is in the data lake.\nThe company needs to identify matching records even when the records do not have a common unique identifier.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Macie pattern matching as part of the ETL job."
      },
      {
        "letter": "B",
        "text": "Train and use the AWS Glue PySpark Filter class in the ETL job."
      },
      {
        "letter": "C",
        "text": "Partition tables and use the ETL job to partition the data on a unique identifier."
      },
      {
        "letter": "D",
        "text": "Train and use the AWS Lake Formation FindMatches transform in the ETL job."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 07:52",
        "comment": "AWS Lake Formation provides machine learning capabilities to create custom transforms to cleanse your data. There is currently one available transform named FindMatches. The FindMatches transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works.",
        "selected_answer": "D"
      },
      {
        "author": "RockyLeon",
        "date": "Tue 26 Nov 2024 17:50",
        "comment": "Correct answer is D",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145607-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 135 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 135,
    "question_text": "A data engineer is using an AWS Glue crawler to catalog data that is in an Amazon S3 bucket. The S3 bucket contains both .csv and json files. The data engineer configured the crawler to exclude the .json files from the catalog.\nWhen the data engineer runs queries in Amazon Athena, the queries also process the excluded .json files. The data engineer wants to resolve this issue. The data engineer needs a solution that will not affect access requirements for the .csv files in the source S3 bucket.\nWhich solution will meet this requirement with the SHORTEST query times?",
    "choices": [
      {
        "letter": "A",
        "text": "Adjust the AWS Glue crawler settings to ensure that the AWS Glue crawler also excludes .json files."
      },
      {
        "letter": "B",
        "text": "Use the Athena console to ensure the Athena queries also exclude the .json files."
      },
      {
        "letter": "C",
        "text": "Relocate the .json files to a different path within the S3 bucket."
      },
      {
        "letter": "D",
        "text": "Use S3 bucket policies to block access to the .json files."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "teo2157",
        "date": "Mon 12 Aug 2024 12:48",
        "comment": "Athena does not recognize exclude patterns that you specify an AWS Glue crawler. For example, if you have an Amazon S3 bucket that contains both .csv and .json files and you exclude the .json files from the crawler, Athena queries both groups of files. To avoid this, place the files that you want to exclude in a different location. \nhttps://docs.aws.amazon.com/athena/latest/ug/troubleshooting-athena.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145725-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 136 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 136,
    "question_text": "A data engineer set up an AWS Lambda function to read an object that is stored in an Amazon S3 bucket. The object is encrypted by an AWS KMS key.\nThe data engineer configured the Lambda function’s execution role to access the S3 bucket. However, the Lambda function encountered an error and failed to retrieve the content of the object.\nWhat is the likely cause of the error?",
    "choices": [
      {
        "letter": "A",
        "text": "The data engineer misconfigured the permissions of the S3 bucket. The Lambda function could not access the object."
      },
      {
        "letter": "B",
        "text": "The Lambda function is using an outdated SDK version, which caused the read failure."
      },
      {
        "letter": "C",
        "text": "The S3 bucket is located in a different AWS Region than the Region where the data engineer works. Latency issues caused the Lambda function to encounter an error."
      },
      {
        "letter": "D",
        "text": "The Lambda function’s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AgboolaKun",
        "date": "Fri 08 Nov 2024 18:43",
        "comment": "The correct answer is D. \n\nHere is why:\n\nThe Lambda function is configured to access the S3 bucket: The data engineer has already set up the Lambda function's execution role to access the S3 bucket. This means that basic S3 access permissions are likely in place.\n\nThe object is encrypted with a KMS key: This is a crucial detail. When an object in S3 is encrypted with a KMS key, any entity trying to read that object needs two sets of permissions: a. Permission to access the S3 bucket and object b. Permission to use the specific KMS key for decryption\n\nThe error occurs when trying to retrieve the content: This suggests that the Lambda function can likely see the object (as it has S3 access) but fails when trying to read its contents.\n\nTo resolve this issue, the data engineer should grant the Lambda function's execution role the required KMS permissions. Specifically, add the 'kms:Decrypt' permission for the KMS key used to encrypt the S3 object.",
        "selected_answer": "D"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:49",
        "comment": "Answer is D",
        "selected_answer": "D"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 14:06",
        "comment": "Option D: The Lambda function’s execution role does not have the necessary permissions to access the KMS key that can decrypt the S3 object.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145726-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 137 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 137,
    "question_text": "A data engineer has implemented data quality rules in 1,000 AWS Glue Data Catalog tables. Because of a recent change in business requirements, the data engineer must edit the data quality rules.\nHow should the data engineer meet this requirement with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a pipeline in AWS Glue ETL to edit the rules for each of the 1,000 Data Catalog tables. Use an AWS Lambda function to call the corresponding AWS Glue job for each Data Catalog table."
      },
      {
        "letter": "B",
        "text": "Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits."
      },
      {
        "letter": "C",
        "text": "Create an Amazon EMR cluster. Run a pipeline on Amazon EMR that edits the rules for each Data Catalog table. Use an AWS Lambda function to run the EMR pipeline."
      },
      {
        "letter": "D",
        "text": "Use the AWS Management Console to edit the rules within the Data Catalog."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:50",
        "comment": "Answer is B",
        "selected_answer": "B"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 14:07",
        "comment": "Option B: Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145728-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 138 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 138,
    "question_text": "Two developers are working on separate application releases. The developers have created feature branches named Branch A and Branch B by using a GitHub repository’s master branch as the source.\nThe developer for Branch A deployed code to the production system. The code for Branch B will merge into a master branch in the following week’s scheduled application release.\nWhich command should the developer for Branch B run before the developer raises a pull request to the master branch?",
    "choices": [
      {
        "letter": "A",
        "text": "git diff branchB mastergit commit -m"
      },
      {
        "letter": "B",
        "text": "git pull master"
      },
      {
        "letter": "C",
        "text": "git rebase master"
      },
      {
        "letter": "D",
        "text": "git fetch -b master"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "simon2133",
        "date": "Sat 01 Mar 2025 09:19",
        "comment": "B\nIt's considered the general default option for a few reasons\n(1) it includes git fetch and\n(2) you won't get a merge conflict if there happens to be a branch C spawned off after A was merged, you won't need to use --force\n(3) related to (2) you're less likely to be clobbering commit history",
        "selected_answer": "B"
      },
      {
        "author": "AgboolaKun",
        "date": "Fri 08 Nov 2024 18:42",
        "comment": "The correct answer is C.\n\nHere is why:\n\nRebasing Branch B onto the updated master branch ensures that Branch B incorporates all the recent changes from the master branch (including the changes from Branch A that were deployed to production).\n\nIt helps maintain a linear, clean history by placing Branch B's commits on top of the latest master branch commits.\n\nThis approach reduces the likelihood of merge conflicts when the pull request is eventually merged into master.\n\nIt makes the code review process easier as all the changes in the pull request will be relevant and up-to-date.\n\nBy using git rebase master, the developer ensures that Branch B is up-to-date with all changes in the master branch, including those from Branch A, before creating the pull request. This approach helps maintain a clean, linear history and reduces the likelihood of conflicts during the merge process.",
        "selected_answer": "C"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:50",
        "comment": "Answer is C",
        "selected_answer": "C"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 14:08",
        "comment": "Option C: git rebase maste",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145119-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 139 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 139,
    "question_text": "A company stores employee data in Amazon Resdshift. A table names Employee uses columns named Region ID, Department ID, and Role ID as a compound sort key.\nWhich queries will MOST increase the speed of query by using a compound sort key of the table? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Select *from Employee where Region ID=’North America’;"
      },
      {
        "letter": "B",
        "text": "Select *from Employee where Region ID=’North America’ and Department ID=20;"
      },
      {
        "letter": "C",
        "text": "Select *from Employee where Department ID=20 and Region ID=’North America’;"
      },
      {
        "letter": "D",
        "text": "Select *from Employee where Role ID=50;"
      },
      {
        "letter": "E",
        "text": "Select *from Employee where Region ID=’North America’ and Role ID=50;"
      }
    ],
    "correct_answer": "AB",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "teo2157",
        "date": "Tue 13 Aug 2024 07:08",
        "comment": "To maximize the speed of queries using a compound sort key in Amazon Redshift, you should structure your queries to take advantage of the order of the columns in the sort key. The most efficient queries will filter or join on the columns in the same order as the sort key. Saying that, the most efficient queries would be:\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n  AND Department_ID = 'dept1' \n  AND Role_ID = 'role1';\nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1' \n  AND Department_ID = 'dept1'; \nSELECT * \nFROM Employee \nWHERE Region_ID = 'region1';",
        "selected_answer": "AB"
      },
      {
        "author": "antun3ra",
        "date": "Thu 08 Aug 2024 18:29",
        "comment": "To maximize the speed of queries by using the compound sort key (Region ID, Department ID, and Role ID) in the Employee table in Amazon Redshift, the queries should align with the order of the columns in the sort key.",
        "selected_answer": "BE"
      },
      {
        "author": "minhhnh",
        "date": "Sun 05 Jan 2025 06:29",
        "comment": "The filter order in the query is irrelevant to the performance because the sort key itself determines the storage order. So the execution plan is the same",
        "selected_answer": "BC"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Wed 18 Dec 2024 09:13",
        "comment": "E is not optimal bc of skipping of the second column.",
        "selected_answer": "AB"
      },
      {
        "author": "altonh",
        "date": "Tue 10 Dec 2024 01:59",
        "comment": "The execution plan of these 2 queries should be the same.",
        "selected_answer": "BC"
      },
      {
        "author": "RockyLeon",
        "date": "Tue 26 Nov 2024 18:28",
        "comment": "sort key works best with the first column in the sort key and continuing in sequential order",
        "selected_answer": "BC"
      },
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 13:43",
        "comment": "The order is the key to speed up queries",
        "selected_answer": "AB"
      },
      {
        "author": "tucobbad",
        "date": "Sat 26 Oct 2024 01:04",
        "comment": "I would vote for B and C. I've tested with a compound sort key (3 columns) and even inverting predicate order the explain plan was the same.",
        "selected_answer": "BC"
      },
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 08:07",
        "comment": "Based on the order of the compound sort key columns.",
        "selected_answer": "BE"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145729-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 140 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 140,
    "question_text": "A company receives test results from testing facilities that are located around the world. The company stores the test results in millions of 1 KB JSON files in an Amazon S3 bucket. A data engineer needs to process the files, convert them into Apache Parquet format, and load them into Amazon Redshift tables. The data engineer uses AWS Glue to process the files, AWS Step Functions to orchestrate the processes, and Amazon EventBridge to schedule jobs.\nThe company recently added more testing facilities. The time required to process files is increasing. The data engineer must reduce the data processing time.\nWhich solution will MOST reduce the data processing time?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Lambda to group the raw input files into larger files. Write the larger files back to Amazon S3. Use AWS Glue to process the files. Load the files into the Amazon Redshift tables."
      },
      {
        "letter": "B",
        "text": "Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables."
      },
      {
        "letter": "C",
        "text": "Use the Amazon Redshift COPY command to move the raw input files from Amazon S3 directly into the Amazon Redshift tables. Process the files in Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Use Amazon EMR instead of AWS Glue to group the raw input files. Process the files in Amazon EMR. Load the files into the Amazon Redshift tables."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Mitchdu",
        "date": "Sat 14 Jun 2025 21:48",
        "comment": "Option A: Lambda to group files → AWS Glue\n- Pre-processing step: Lambda combines small files into larger ones\n- Benefits: Reduces number of files Glue needs to process\n- Efficiency: Fewer S3 API calls, better parallelization in Glue\n- Result: Significant reduction in processing overhead\n\nOption B: AWS Glue dynamic frame file-grouping\n- Built-in feature: Glue can group small files during processing\n- Benefits: Reduces overhead within Glue job execution\n- Limitation: Still needs to read all individual files initially\n- Result: Some improvement but less than pre-grouping",
        "selected_answer": "A"
      },
      {
        "author": "bac9792",
        "date": "Wed 21 May 2025 10:07",
        "comment": "While AWS Glue's groupFiles parameter can help, it doesn't eliminate the overhead of reading numerous small files. Preprocessing files into larger ones before they reach AWS Glue is more effective.",
        "selected_answer": "A"
      },
      {
        "author": "minhhnh",
        "date": "Sun 05 Jan 2025 06:25",
        "comment": "The key requirement is to reduce processing time for millions of small JSON files stored in Amazon S3. The solution needs to address the inefficiencies caused by the large number of small files while leveraging the existing AWS Glue and Amazon Redshift setup.",
        "selected_answer": "B"
      },
      {
        "author": "aragon_saa",
        "date": "Wed 14 Aug 2024 14:50",
        "comment": "Answer is B",
        "selected_answer": "B"
      },
      {
        "author": "matt200",
        "date": "Wed 14 Aug 2024 14:09",
        "comment": "Option B: Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files. Process the files. Load the files into the Amazon Redshift tables.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/145096-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 141 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 141,
    "question_text": "A data engineer uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to run data pipelines in an AWS account.\nA workflow recently failed to run. The data engineer needs to use Apache Airflow logs to diagnose the failure of the workflow.\nWhich log type should the data engineer use to diagnose the cause of the failure?",
    "choices": [
      {
        "letter": "A",
        "text": "YourEnvironmentName-WebServer"
      },
      {
        "letter": "B",
        "text": "YourEnvironmentName-Scheduler"
      },
      {
        "letter": "C",
        "text": "YourEnvironmentName-DAGProcessing"
      },
      {
        "letter": "D",
        "text": "YourEnvironmentName-Task"
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "150b64e",
        "date": "Mon 12 Aug 2024 09:11",
        "comment": "https://pupuweb.com/amazon-dea-c01-which-apache-airflow-log-type-should-you-use-to-diagnose-workflow-failures-in-amazon-mwaa/\n\nWhen a workflow fails to run in Amazon MWAA, the task logs (YourEnvironmentName-Task) are the most relevant for diagnosing the issue. Task logs contain detailed information about the execution of individual tasks within the workflow, including any error messages or stack traces that can help pinpoint the cause of the failure.",
        "selected_answer": "D"
      },
      {
        "author": "teo2157",
        "date": "Tue 13 Aug 2024 07:18",
        "comment": "Agree with D based on 150b64e comments",
        "selected_answer": "D"
      },
      {
        "author": "Shanmahi",
        "date": "Tue 06 Aug 2024 07:12",
        "comment": "Reference --> https://aws.amazon.com/managed-workflows-for-apache-airflow/",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/147826-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 142 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 142,
    "question_text": "A finance company uses Amazon Redshift as a data warehouse. The company stores the data in a shared Amazon S3 bucket. The company uses Amazon Redshift Spectrum to access the data that is stored in the S3 bucket. The data comes from certified third-party data providers. Each third-party data provider has unique connection details.\nTo comply with regulations, the company must ensure that none of the data is accessible from outside the company's AWS environment.\nWhich combination of steps should the company take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Replace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket."
      },
      {
        "letter": "B",
        "text": "Create an AWS CloudHSM hardware security module (HSM) for each data provider. Encrypt each data provider's data by using the corresponding HSM for each data provider."
      },
      {
        "letter": "C",
        "text": "Turn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company’s VP"
      },
      {
        "letter": "D",
        "text": "Define table constraints for the primary keys and the foreign keys."
      },
      {
        "letter": "E",
        "text": "Use federated queries to access the data from each data provider. Do not upload the data to the S3 bucket. Perform the federated queries through a gateway VPC endpoint."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Sun 17 Aug 2025 04:04",
        "comment": "using a NAT gateway still routes traffic through the public internet, which breaks the “no outside access” compliance rule.",
        "selected_answer": "CE"
      },
      {
        "author": "BigMrT",
        "date": "Sun 05 Jan 2025 21:45",
        "comment": "A doesn't make sense considering the NAT gateway since that's usually used to facilitate traffic to the internet? Maybe if it was a S3 Gateway Endpoint it would make more sense but E makes sense if the configurations are correct?",
        "selected_answer": "CE"
      },
      {
        "author": "kailu",
        "date": "Sat 21 Dec 2024 20:39",
        "comment": "Shouldn't it be E and not C? Federated Queries: This method allows Redshift to query data directly from external sources without needing to store the data in Amazon S3. By using federated queries, the company can query third-party data sources without moving data into S3, reducing the attack surface.\nGateway VPC Endpoint: A gateway VPC endpoint allows secure access to S3 from within the VPC without routing traffic over the public internet. This is crucial for maintaining compliance with regulations by ensuring that no data leaves the AWS environment.",
        "selected_answer": "AE"
      },
      {
        "author": "paali",
        "date": "Mon 16 Dec 2024 12:51",
        "comment": "Why do we need NAT GW when we can have VPC GW or Interface Endpoints for S3 as well.",
        "selected_answer": "AC"
      },
      {
        "author": "hk0308",
        "date": "Fri 13 Dec 2024 08:08",
        "comment": "None of the answers satisfy the constraints. A C both dont address how s3 bucket will be accessed through a VPC.",
        "selected_answer": "AC"
      },
      {
        "author": "EJGisME",
        "date": "Thu 19 Sep 2024 03:11",
        "comment": "A. Replace the existing Redshift cluster with a new Redshift cluster that is in a private subnet. Use an interface VPC endpoint to connect to the Redshift cluster. Use a NAT gateway to give Redshift access to the S3 bucket.\nC. Turn on enhanced VPC routing for the Amazon Redshift cluster. Set up an AWS Direct Connect connection and configure a connection between each data provider and the finance company’s VPC.",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/146986-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 143 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 143,
    "question_text": "Files from multiple data sources arrive in an Amazon S3 bucket on a regular basis. A data engineer wants to ingest new files into Amazon Redshift in near real time when the new files arrive in the S3 bucket.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the query editor v2 to schedule a COPY command to load new files into Amazon Redshift."
      },
      {
        "letter": "B",
        "text": "Use the zero-ETL integration between Amazon Aurora and Amazon Redshift to load new files into Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue job bookmarks to extract, transform, and load (ETL) load new files into Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Use S3 Event Notifications to invoke an AWS Lambda function that loads new files into Amazon Redshift."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:14",
        "comment": "A: No, Query Editor v2 isn't designed for event-driven near-real-time loads.\nB: No, the zero-ETL integration applies to Aurora, not to files arriving in S3.\nC: No, Glue job bookmarks are for batch ETL jobs, not near real-time processing.\nD: Sí, because using S3 Event Notifications to trigger a Lambda function enables near-real-time ingestion via COPY commands into Redshift.",
        "selected_answer": "D"
      },
      {
        "author": "dashapetr",
        "date": "Thu 05 Sep 2024 07:47",
        "comment": "Seems like the trigger on upload would be the fastest option",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/146967-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 144 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 144,
    "question_text": "A technology company currently uses Amazon Kinesis Data Streams to collect log data in real time. The company wants to use Amazon Redshift for downstream real-time queries and to enrich the log data.\nWhich solution will ingest data into Amazon Redshift with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Set up an Amazon Kinesis Data Firehose delivery stream to send data to a Redshift provisioned cluster table."
      },
      {
        "letter": "B",
        "text": "Set up an Amazon Kinesis Data Firehose delivery stream to send data to Amazon S3. Configure a Redshift provisioned cluster to load data every minute."
      },
      {
        "letter": "C",
        "text": "Configure Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to send data directly to a Redshift provisioned cluster table."
      },
      {
        "letter": "D",
        "text": "Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Tester_TKK",
        "date": "Wed 23 Apr 2025 23:32",
        "comment": "D. Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view.",
        "selected_answer": "D"
      },
      {
        "author": "dashapetr",
        "date": "Thu 05 Sep 2024 07:56",
        "comment": "Amazon Redshift supports streaming ingestion from Amazon Kinesis Data Streams. The Amazon Redshift streaming ingestion feature provides low-latency, high-speed ingestion of streaming data from Amazon Kinesis Data Streams into an Amazon Redshift materialized view. Amazon Redshift streaming ingestion removes the need to stage data in Amazon S3before ingesting into Amazon Redshift.\n\nlink: https://docs.aws.amazon.com/streams/latest/dev/using-other-services-redshift.html",
        "selected_answer": "D"
      },
      {
        "author": "EJGisME",
        "date": "Thu 05 Sep 2024 03:02",
        "comment": "D. Use Amazon Redshift streaming ingestion from Kinesis Data Streams and to present data as a materialized view.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/147821-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 145 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 145,
    "question_text": "A company maintains a data warehouse in an on-premises Oracle database. The company wants to build a data lake on AWS. The company wants to load data warehouse tables into Amazon S3 and synchronize the tables with incremental data that arrives from the data warehouse every day.\nEach table has a column that contains monotonically increasing values. The size of each table is less than 50 GB. The data warehouse tables are refreshed every night between 1 AM and 2 AM. A business intelligence team queries the tables between 10 AM and 8 PM every day.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Database Migration Service (AWS DMS) full load plus CDC job to load tables that contain monotonically increasing data columns from the on-premises data warehouse to Amazon S3. Use custom logic in AWS Glue to append the daily incremental data to a full-load copy that is in Amazon S3."
      },
      {
        "letter": "B",
        "text": "Use an AWS Glue Java Database Connectivity (JDBC) connection. Configure a job bookmark for a column that contains monotonically increasing values. Write custom logic to append the daily incremental data to a full-load copy that is in Amazon S3."
      },
      {
        "letter": "C",
        "text": "Use an AWS Database Migration Service (AWS DMS) full load migration to load the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue to load a full copy of the data warehouse tables into Amazon S3 every day. Overwrite the previous day's full-load copy every day."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Sun 17 Aug 2025 04:29",
        "comment": "Use AWS DMS full load + CDC to S3. It’s built for migrating from Oracle and continuously capturing daily changes with minimal ops overhead. DMS handles the initial full load and streams incremental updates (based on the monotonically increasing column), so you don’t need to rebuild or craft complex extract logic.",
        "selected_answer": "A"
      },
      {
        "author": "Mitchdu",
        "date": "Sun 22 Jun 2025 14:18",
        "comment": "Here's why this is the optimal choice:\n\n**AWS Glue with Job Bookmarks Benefits:**\n- **Built-in incremental processing** - Job bookmarks automatically track the last processed value in the monotonically increasing column\n- **Efficient data transfer** - Only processes new/changed records, not the entire dataset\n- **Simple setup** - Native AWS Glue functionality without additional services\n- **Cost-effective** - Processes only incremental data daily (much smaller than 50GB full loads)\n- **Reliable tracking** - Job bookmarks persist state between runs automatically",
        "selected_answer": "B"
      },
      {
        "author": "Tester_TKK",
        "date": "Wed 23 Apr 2025 23:43",
        "comment": "Simplifies the architecture by using a single service (Glue) for both full and incremental loads.",
        "selected_answer": "B"
      },
      {
        "author": "LR2023",
        "date": "Sat 28 Sep 2024 20:42",
        "comment": "A seems to be an overkill using custom logic",
        "selected_answer": "C"
      },
      {
        "author": "Fawk",
        "date": "Thu 19 Sep 2024 02:00",
        "comment": "DMS is definitely the service, and C is obviously wrong",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/146993-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 146 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 146,
    "question_text": "A company is building a data lake for a new analytics team. The company is using Amazon S3 for storage and Amazon Athena for query analysis. All data that is in Amazon S3 is in Apache Parquet format.\nThe company is running a new Oracle database as a source system in the company’s data center. The company has 70 tables in the Oracle database. All the tables have primary keys. Data can occasionally change in the source system. The company wants to ingest the tables every day into the data lake.\nWhich solution will meet this requirement with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Apache Sqoop job in Amazon EMR to read the data from the Oracle database. Configure the Sqoop job to write the data to Amazon S3 in Parquet format."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue connection to the Oracle database. Create an AWS Glue bookmark job to ingest the data incrementally and to write the data to Amazon S3 in Parquet format."
      },
      {
        "letter": "C",
        "text": "Create an AWS Database Migration Service (AWS DMS) task for ongoing replication. Set the Oracle database as the source. Set Amazon S3 as the target. Configure the task to write the data in Parquet format."
      },
      {
        "letter": "D",
        "text": "Create an Oracle database in Amazon RDS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises Oracle database to Amazon RDS. Configure triggers on the tables to invoke AWS Lambda functions to write changed records to Amazon S3 in Parquet format."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "dashapetr",
        "date": "Thu 05 Sep 2024 08:14",
        "comment": "C: You can use S3 as a target and configure files to be in Parquet format https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html",
        "selected_answer": "C"
      },
      {
        "author": "Nickalodeon99",
        "date": "Mon 22 Sep 2025 23:35",
        "comment": "I think option B is correct, Why:\nThere are many limitations to use S3 as target for DMS\nAlso Converting to Parget required the following :\n\nTo set .parquet as the storage format for your migrated S3 target objects, you can use the following mechanisms:\n\n-Endpoint settings that you provide as parameters of a JSON object when you create the endpoint using the AWS CLI or the API for AWS DMS. \n\n-Extra connection attributes that you provide as a semicolon-separated list when you create the endpoint. \n\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Limitations",
        "selected_answer": "B"
      },
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 13:59",
        "comment": "A and D wrong. B also wrong because Bookmark is used to mantain files that you don't want to re-analyze in case of a re-run about a glue job.",
        "selected_answer": "C"
      },
      {
        "author": "siheom",
        "date": "Fri 06 Sep 2024 15:19",
        "comment": "VOTE B",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/147168-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 147 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 147,
    "question_text": "A transportation company wants to track vehicle movements by capturing geolocation records. The records are 10 bytes in size. The company receives up to 10.000 records every second. Data transmission delays of a few minutes are acceptable because of unreliable network conditions.\nThe transportation company wants to use Amazon Kinesis Data Streams to ingest the geolocation data. The company needs a reliable mechanism to send data to Kinesis Data Streams. The company needs to maximize the throughput efficiency of the Kinesis shards.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "Kinesis Agent"
      },
      {
        "letter": "B",
        "text": "Kinesis Producer Library (KPL)"
      },
      {
        "letter": "C",
        "text": "Amazon Kinesis Data Firehose"
      },
      {
        "letter": "D",
        "text": "Kinesis SDK"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 14:12",
        "comment": "KPL automatically batches and aggregates multiple records into a single payload before sending them to Kinesis Data Streams. This reduces the number of records sent and optimizes shard throughput usage.",
        "selected_answer": "B"
      },
      {
        "author": "EJGisME",
        "date": "Sun 08 Sep 2024 09:38",
        "comment": "B. Kinesis Producer Library (KPL)",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/147823-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 148 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 148,
    "question_text": "An investment company needs to manage and extract insights from a volume of semi-structured data that grows continuously.\nA data engineer needs to deduplicate the semi-structured data, remove records that are duplicates, and remove common misspellings of duplicates.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the FindMatches feature of AWS Glue to remove duplicate records."
      },
      {
        "letter": "B",
        "text": "Use non-Windows functions in Amazon Athena to remove duplicate records."
      },
      {
        "letter": "C",
        "text": "Use Amazon Neptune ML and an Apache Gremlin script to remove duplicate records."
      },
      {
        "letter": "D",
        "text": "Use the global tables feature of Amazon DynamoDB to prevent duplicate data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:11",
        "comment": "A: Sí, porque AWS Glue FindMatches utiliza machine learning para deduplicar datos y corregir errores ortográficos con mínima sobrecarga operativa.\nB: No, usar Athena requiere escribir consultas manuales y no maneja bien las variaciones de escritura.\nC: No, Neptune ML está orientado a análisis en grafos, no a la deduplicación de datos semi-estructurados.\nD: No, global tables en DynamoDB se usan para replicación, no para eliminar duplicados.",
        "selected_answer": "A"
      },
      {
        "author": "Fawk",
        "date": "Thu 19 Sep 2024 02:04",
        "comment": "A - The other options are dumb and hardly make sense",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150357-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 149 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 149,
    "question_text": "A company is building an inventory management system and an inventory reordering system to automatically reorder products. Both systems use Amazon Kinesis Data Streams. The inventory management system uses the Amazon Kinesis Producer Library (KPL) to publish data to a stream. The inventory reordering system uses the Amazon Kinesis Client Library (KCL) to consume data from the stream. The company configures the stream to scale up and down as needed.\nBefore the company deploys the systems to production, the company discovers that the inventory reordering system received duplicated data.\nWhich factors could have caused the reordering system to receive duplicated data? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "The producer experienced network-related timeouts."
      },
      {
        "letter": "B",
        "text": "The stream’s value for the IteratorAgeMilliseconds metric was too high."
      },
      {
        "letter": "C",
        "text": "There was a change in the number of shards, record processors, or both."
      },
      {
        "letter": "D",
        "text": "The AggregationEnabled configuration property was set to true."
      },
      {
        "letter": "E",
        "text": "The max_records configuration property was set to a number that was too high."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 14:16",
        "comment": "https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151928-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 150 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 150,
    "question_text": "An ecommerce company operates a complex order fulfilment process that spans several operational systems hosted in AWS. Each of the operational systems has a Java Database\nConnectivity (JDBC)-compliant relational database where the latest processing state is captured.\nThe company needs to give an operations team the ability to track orders on an hourly basis across the entire fulfillment process.\nWhich solution will meet these requirements with the LEAST development overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue to build ingestion pipelines from the operational systems into Amazon Redshift Build dashboards in Amazon QuickSight that track the orders."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue to build ingestion pipelines from the operational systems into Amazon DynamoDBuild dashboards in Amazon QuickSight that track the orders."
      },
      {
        "letter": "C",
        "text": "Use AWS Database Migration Service (AWS DMS) to capture changed records in the operational systems. Publish the changes to an Amazon DynamoDB table in a different AWS region from the source database. Build Grafana dashboards that track the orders."
      },
      {
        "letter": "D",
        "text": "Use AWS Database Migration Service (AWS DMS) to capture changed records in the operational systems. Publish the changes to an Amazon DynamoDB table in a different AWS region from the source database. Build Amazon QuickSight dashboards that track the orders."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "youonebe",
        "date": "Wed 14 May 2025 22:28",
        "comment": "D - While DynamoDB is a NoSQL database, AWS DMS can denormalize relational data into a single DynamoDB table using object mapping, simplifying the data model for order tracking.\n\nCross-region replication in DynamoDB ensures data durability and low-latency access for dashboards.\n\nA - Requires building and maintaining hourly ETL jobs in Glue, increasing development effort.",
        "selected_answer": "D"
      },
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:34",
        "comment": "DynamoDB is not designed to support relational databases. Redshift, however is.",
        "selected_answer": "A"
      },
      {
        "author": "pepedaruiz999",
        "date": "Sun 12 Jan 2025 20:43",
        "comment": "DynamoDB is not relational data base",
        "selected_answer": "A"
      },
      {
        "author": "axantroff",
        "date": "Wed 25 Dec 2024 12:03",
        "comment": "IDK, it feels like from DEV overhead D > A",
        "selected_answer": "D"
      },
      {
        "author": "kailu",
        "date": "Sat 21 Dec 2024 20:59",
        "comment": "Using AWS DMS for real-time change data capture (CDC) and publishing the changes to DynamoDB, followed by building QuickSight dashboards, is the most efficient solution with the least development overhead for this use case",
        "selected_answer": "D"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 02:20",
        "comment": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151917-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 151 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 151,
    "question_text": "A data engineer needs to use Amazon Neptune to develop graph applications.\nWhich programming languages should the engineer use to develop the graph applications? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Gremlin"
      },
      {
        "letter": "B",
        "text": "SQL"
      },
      {
        "letter": "C",
        "text": "ANSI SQL"
      },
      {
        "letter": "D",
        "text": "SPARQL"
      },
      {
        "letter": "E",
        "text": "Spark SQL"
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 00:07",
        "comment": "https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-queries.html",
        "selected_answer": "AD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/147824-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 152 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 152,
    "question_text": "A mobile gaming company wants to capture data from its gaming app. The company wants to make the data available to three internal consumers of the data. The data records are approximately 20 KB in size.\nThe company wants to achieve optimal throughput from each device that runs the gaming app. Additionally, the company wants to develop an application to process data streams. The stream-processing application must have dedicated throughput for each internal consumer.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature with a stream for each internal consumer."
      },
      {
        "letter": "B",
        "text": "Configure the mobile app to call the PutRecordBatch API operation to send data to Amazon Kinesis Data Firehose. Submit an AWS Support case to turn on dedicated throughput for the company’s AWS account. Allow each internal consumer to access the stream."
      },
      {
        "letter": "C",
        "text": "Configure the mobile app to use the Amazon Kinesis Producer Library (KPL) to send data to Amazon Kinesis Data Firehose. Use the enhanced fan-out feature with a stream for each internal consumer."
      },
      {
        "letter": "D",
        "text": "Configure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Host the stream-processing application for each internal consumer on Amazon EC2 instances. Configure auto scaling for the EC2 instances."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:36",
        "comment": "The fan out feature allows consumers to receive data from a stream with dedicated throughput",
        "selected_answer": "A"
      },
      {
        "author": "AgboolaKun",
        "date": "Sat 09 Nov 2024 18:48",
        "comment": "The correct answer is A.\n\nHere is why:\n\nAmazon Kinesis Data Streams is designed for real-time streaming data.\n    \nThe PutRecords API is efficient for sending multiple records to Kinesis in a single call, which is good for optimizing throughput from mobile devices.\n\nThe enhanced fan-out feature allows multiple consumers to read from the same stream with dedicated throughput for each consumer, which meets the requirement of dedicated throughput for each internal consumer.\n\nThis option uses uses Kinesis Data Streams for real-time processing, optimizes throughput from mobile devices with the PutRecords API, and provides dedicated throughput for each consumer using the enhanced fan-out feature.",
        "selected_answer": "A"
      },
      {
        "author": "pikuantne",
        "date": "Thu 31 Oct 2024 12:21",
        "comment": "A is best, but I think it was supposed to be a SHARD for each consumer.\nB - doesn't make any sense\nC - Firehose does not have enhanced fan-out afaik\nD - does not have the dedicated throughput as it doesn't use enhanced fan-out with KDS",
        "selected_answer": "A"
      },
      {
        "author": "LR2023",
        "date": "Thu 26 Sep 2024 03:46",
        "comment": "https://docs.aws.amazon.com/streams/latest/dev/kpl-with-firehose.html\n\nKPL does work with firehose",
        "selected_answer": "B"
      },
      {
        "author": "Fawk",
        "date": "Thu 19 Sep 2024 02:14",
        "comment": "Seems to be A - Since KPL does not work into firehouse and only streams, and additionally the dedicated throughput is solved through fan-out",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151918-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 153 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 153,
    "question_text": "A retail company uses an Amazon Redshift data warehouse and an Amazon S3 bucket. The company ingests retail order data into the S3 bucket every day.\nThe company stores all order data at a single path within the S3 bucket. The data has more than 100 columns. The company ingests the order data from a third-party application that generates more than 30 files in CSV format every day. Each CSV file is between 50 and 70 MB in size.\nThe company uses Amazon Redshift Spectrum to run queries that select sets of columns. Users aggregate metrics based on daily orders. Recently, users have reported that the performance of the queries has degraded. A data engineer must resolve the performance issues for the queries.\nWhich combination of steps will meet this requirement with LEAST developmental effort? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the third-party application to create the files in a columnar format."
      },
      {
        "letter": "B",
        "text": "Develop an AWS Glue ETL job to convert the multiple daily CSV files to one file for each day."
      },
      {
        "letter": "C",
        "text": "Partition the order data in the S3 bucket based on order date."
      },
      {
        "letter": "D",
        "text": "Configure the third-party application to create the files in JSON format."
      },
      {
        "letter": "E",
        "text": "Load the JSON data into the Amazon Redshift table in a SUPER type column."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ell89",
        "date": "Thu 27 Feb 2025 01:00",
        "comment": "using parqueet or ORC is efficient and so will be partitioning by order date so the range of data is lower",
        "selected_answer": "AC"
      },
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:22",
        "comment": "No, porque la opción A implica modificar la aplicación de terceros para que genere archivos en formato columnar, lo cual puede ser más complejo o inviable, mientras que la opción B utiliza un job de Glue para consolidar los CSV sin tocar la fuente. La opción C sigue siendo esencial para particionar por fecha y optimizar las consultas.",
        "selected_answer": "BC"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 00:30",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151919-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 154 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 154,
    "question_text": "A company stores customer records in Amazon S3. The company must not delete or modify the customer record data for 7 years after each record is created. The root user also must not have the ability to delete or modify the data.\nA data engineer wants to use S3 Object Lock to secure the data.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Enable governance mode on the S3 bucket. Use a default retention period of 7 years."
      },
      {
        "letter": "B",
        "text": "Enable compliance mode on the S3 bucket. Use a default retention period of 7 years."
      },
      {
        "letter": "C",
        "text": "Place a legal hold on individual objects in the S3 bucket. Set the retention period to 7 years."
      },
      {
        "letter": "D",
        "text": "Set the retention period for individual objects in the S3 bucket to 7 years."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:39",
        "comment": "\"In compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.\"",
        "selected_answer": "B"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 00:35",
        "comment": "https://aws.amazon.com/s3/features/object-lock/",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150339-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 155 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 155,
    "question_text": "A data engineer needs to create a new empty table in Amazon Athena that has the same schema as an existing table named old_table.\nWhich SQL statement should the data engineer use to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "CREATE TABLE new_table AS SELECT * FROM old_tables;"
      },
      {
        "letter": "B",
        "text": "INSERT INTO new_table SELECT * FROM old_table;"
      },
      {
        "letter": "C",
        "text": "CREATE TABLE new_table (LIKE old_table);"
      },
      {
        "letter": "D",
        "text": "CREATE TABLE new_table AS (SELECT * FROM old_table) WITH NO DATA;"
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AgboolaKun",
        "date": "Sat 09 Nov 2024 19:39",
        "comment": "D is the correct answer.\n\nHere is why:\n\nThe AS clause allows you to define the new table's schema based on a SELECT statement.\n \nThe WITH NO DATA clause at the end explicitly tells Athena to create the table structure without copying any data.\n\nFor more information, see the \"Creating an empty copy of an existing table\" section in this documentation - https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html",
        "selected_answer": "D"
      },
      {
        "author": "Eleftheriia",
        "date": "Fri 01 Nov 2024 12:49",
        "comment": "with no data, creates a new table with the same schema as the old one.\nhttps://docs.aws.amazon.com/athena/latest/ug/create-table-as.html",
        "selected_answer": "D"
      },
      {
        "author": "pikuantne",
        "date": "Thu 31 Oct 2024 13:44",
        "comment": "D is correct",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150340-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 156 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 156,
    "question_text": "A data engineer needs to create an Amazon Athena table based on a subset of data from an existing Athena table named cities_world. The cities_world table contains cities that are located around the world. The data engineer must create a new table named cities_us to contain only the cities from cities_world that are located in the US.\nWhich SQL statement should the data engineer use to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "INSERT INTO cities_usa (city,state) SELECT city, state FROM cities_world WHERE country=’usa’;"
      },
      {
        "letter": "B",
        "text": "MOVE city, state FROM cities_world TO cities_usa WHERE country=’usa’;"
      },
      {
        "letter": "C",
        "text": "INSERT INTO cities_usa SELECT city, state FROM cities_world WHERE country=’usa’;"
      },
      {
        "letter": "D",
        "text": "UPDATE cities_usa SET (city, state) = (SELECT city, state FROM cities_world WHERE country=’usa’);"
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Eleftheriia",
        "date": "Fri 01 Nov 2024 16:31",
        "comment": "INSERT INTO cities_usa (city,state)\nSELECT city,state\nFROM cities_world\n    WHERE country='usa'",
        "selected_answer": "A"
      },
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:41",
        "comment": "The INSERT INTO SELECT statement copies data from one table and inserts it into another table.",
        "selected_answer": "A"
      },
      {
        "author": "truongnguyen86",
        "date": "Thu 31 Oct 2024 09:31",
        "comment": "should be A or C but C will failed if cities_usa contains more than 2 columns so specify the list of column want to insert is the good one.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150341-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 157 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 157,
    "question_text": "A company implements a data mesh that has a central governance account. The company needs to catalog all data in the governance account. The governance account uses AWS Lake Formation to centrally share data and grant access permissions.\nThe company has created a new data product that includes a group of Amazon Redshift Serverless tables. A data engineer needs to share the data product with a marketing team. The marketing team must have access to only a subset of columns. The data engineer needs to share the same data product with a compliance team. The compliance team must have access to a different subset of columns than the marketing team needs access to.\nWhich combination of steps should the data engineer take to meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Create views of the tables that need to be shared. Include only the required columns."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Redshift data share that includes the tables that need to be shared."
      },
      {
        "letter": "C",
        "text": "Create an Amazon Redshift managed VPC endpoint in the marketing team’s account. Grant the marketing team access to the views."
      },
      {
        "letter": "D",
        "text": "Share the Amazon Redshift data share to the Lake Formation catalog in the governance account."
      },
      {
        "letter": "E",
        "text": "Share the Amazon Redshift data share to the Amazon Redshift Serverless workgroup in the marketing team's account."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Tue 19 Aug 2025 12:11",
        "comment": "•\tStep 1 (Column Subsets):\n👉 A. Create views of the tables that need to be shared. Include only the required columns.\n\t•\tThis satisfies the requirement for different subsets of columns for marketing and compliance teams.\n\t•\tStep 2 (Central Governance Sharing):\n👉 D. Share the Amazon Redshift data share to the Lake Formation catalog in the governance account.\n\t•\tThis ensures the data product is discoverable and governable under the company’s Lake Formation–based central catalog",
        "selected_answer": "AD"
      },
      {
        "author": "Tester_TKK",
        "date": "Thu 24 Apr 2025 14:12",
        "comment": "B and D",
        "selected_answer": "AB"
      },
      {
        "author": "Eleftheriia",
        "date": "Mon 04 Nov 2024 17:52",
        "comment": "I think that D is one of the correct answers as described\n\nhttps://aws.amazon.com/blogs/big-data/centrally-manage-access-and-permissions-for-amazon-redshift-data-sharing-with-aws-lake-formation/",
        "selected_answer": "D"
      },
      {
        "author": "ae35a02",
        "date": "Mon 28 Oct 2024 14:18",
        "comment": "workgroups don't manage permission to tables and views, they manage resource allocation for queries execution.",
        "selected_answer": "BD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151925-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 158 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 158,
    "question_text": "A company has a data lake in Amazon S3. The company uses AWS Glue to catalog data and AWS Glue Studio to implement data extract, transform, and load (ETL) pipelines.\nThe company needs to ensure that data quality issues are checked every time the pipelines run. A data engineer must enhance the existing pipelines to evaluate data quality rules based on predefined thresholds.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Add a new transform that is defined by a SQL query to each Glue ETL job. Use the SQL query to implement a ruleset that includes the data quality rules that need to be evaluated."
      },
      {
        "letter": "B",
        "text": "Add a new Evaluate Data Quality transform to each Glue ETL job. Use Data Quality Definition Language (DQDL) to implement a ruleset that includes the data quality rules that need to be evaluated."
      },
      {
        "letter": "C",
        "text": "Add a new custom transform to each Glue ETL job. Use the PyDeequ library to implement a ruleset that includes the data quality rules that need to be evaluated."
      },
      {
        "letter": "D",
        "text": "Add a new custom transform to each Glue ETL job. Use the Great Expectations library to implement a ruleset that includes the data quality rules that need to be evaluated."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:45",
        "comment": "AWS Glue Data Quality works with Data Quality Definition Language (DQDL) to define data quality rules.",
        "selected_answer": "B"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 01:56",
        "comment": "https://docs.aws.amazon.com/glue/latest/dg/tutorial-data-quality.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151926-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 159 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 159,
    "question_text": "A company has an application that uses a microservice architecture. The company hosts the application on an Amazon Elastic Kubernetes Services (Amazon EKS) cluster.\nThe company wants to set up a robust monitoring system for the application. The company needs to analyze the logs from the EKS cluster and the application. The company needs to correlate the cluster's logs with the application's traces to identify points of failure in the whole application request flow.\nWhich combination of steps will meet these requirements with the LEAST development effort? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use FluentBit to collect logs. Use OpenTelemetry to collect traces."
      },
      {
        "letter": "B",
        "text": "Use Amazon CloudWatch to collect logs. Use Amazon Kinesis to collect traces."
      },
      {
        "letter": "C",
        "text": "Use Amazon CloudWatch to collect logs. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to collect traces."
      },
      {
        "letter": "D",
        "text": "Use Amazon OpenSearch to correlate the logs and traces."
      },
      {
        "letter": "E",
        "text": "Use AWS Glue to correlate the logs and traces."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 02:11",
        "comment": "https://aws.amazon.com/blogs/big-data/part-1-microservice-observability-with-amazon-opensearch-service-trace-and-log-correlation/",
        "selected_answer": "AD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151927-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 160 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 160,
    "question_text": "A company has a gaming application that stores data in Amazon DynamoDB tables. A data engineer needs to ingest the game data into an Amazon OpenSearch Service cluster. Data updates must occur in near real time.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Step Functions to periodically export data from the Amazon DynamoDB tables to an Amazon S3 bucket. Use an AWS Lambda function to load the data into Amazon OpenSearch Service."
      },
      {
        "letter": "B",
        "text": "Configure an AWS Glue job to have a source of Amazon DynamoDB and a destination of Amazon OpenSearch Service to transfer data in near real time."
      },
      {
        "letter": "C",
        "text": "Use Amazon DynamoDB Streams to capture table changes. Use an AWS Lambda function to process and update the data in Amazon OpenSearch Service."
      },
      {
        "letter": "D",
        "text": "Use a custom OpenSearch plugin to sync data from the Amazon DynamoDB tables."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:49",
        "comment": "DynamoDB supports streaming of item-level change data capture records in *near-real time*",
        "selected_answer": "C"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 02:14",
        "comment": "https://docs.aws.amazon.com/opensearch-service/latest/developerguide/configure-client-ddb.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151853-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 161 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 161,
    "question_text": "A company uses Amazon Redshift as its data warehouse service. A data engineer needs to design a physical data model.\nThe data engineer encounters a de-normalized table that is growing in size. The table does not have a suitable column to use as the distribution key.\nWhich distribution style should the data engineer use to meet these requirements with the LEAST maintenance overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "ALL distribution"
      },
      {
        "letter": "B",
        "text": "EVEN distribution"
      },
      {
        "letter": "C",
        "text": "AUTO distribution"
      },
      {
        "letter": "D",
        "text": "KEY distribution"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alya99",
        "date": "Sun 21 Sep 2025 15:37",
        "comment": "\"If the table grows larger and none of the columns are suitable to be the distribution key, Amazon Redshift changes the distribution style to EVEN.\"\n\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html",
        "selected_answer": "B"
      },
      {
        "author": "idenrai",
        "date": "Mon 09 Dec 2024 19:41",
        "comment": "the LEAST maintenance overhead = C\n\nWith AUTO distribution, Amazon Redshift assigns an optimal distribution style based on the size of the table data. For example, if AUTO distribution style is specified, Amazon Redshift initially assigns the ALL distribution style to a small table. When the table grows larger, Amazon Redshift might change the distribution style to KEY, choosing the primary key (or a column of the composite primary key) as the distribution key. If the table grows larger and none of the columns are suitable to be the distribution key, Amazon Redshift changes the distribution style to EVEN. The change in distribution style occurs in the background with minimal impact to user queries.",
        "selected_answer": "C"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 02:45",
        "comment": "https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html",
        "selected_answer": "C"
      },
      {
        "author": "jacob_nz",
        "date": "Fri 22 Nov 2024 23:06",
        "comment": "If the table grows larger and none of the columns are suitable to be the distribution key, Amazon Redshift changes the distribution style to EVEN.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151931-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 162 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 162,
    "question_text": "A retail company is expanding its operations globally. The company needs to use Amazon QuickSight to accurately calculate currency exchange rates for financial reports. The company has an existing dashboard that includes a visual that is based on an analysis of a dataset that contains global currency values and exchange rates.\nA data engineer needs to ensure that exchange rates are calculated with a precision of four decimal places. The calculations must be precomputed. The data engineer must materialize results in QuickSight super-fast, parallel, in-memory calculation engine (SPICE).\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Define and create the calculated field in the dataset."
      },
      {
        "letter": "B",
        "text": "Define and create the calculated field in the analysis."
      },
      {
        "letter": "C",
        "text": "Define and create the calculated field in the visual."
      },
      {
        "letter": "D",
        "text": "Define and create the calculated field in the dashboard."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:24",
        "comment": "A: Sí, porque al crear el campo calculado en el dataset se precomputan los valores y se materializan en SPICE, asegurando precisión y rapidez.\nB: No, porque los campos calculados en el análisis se calculan en tiempo de consulta, no se precomputan.\nC: No, porque los campos calculados en la visual se generan al renderizar, no se almacenan en SPICE.\nD: No, porque los campos calculados en el dashboard tampoco se precomputan en SPICE.",
        "selected_answer": "A"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 03:09",
        "comment": "https://docs.aws.amazon.com/quicksight/latest/user/adding-a-calculated-field-analysis.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150342-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 163 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 163,
    "question_text": "A company has three subsidiaries. Each subsidiary uses a different data warehousing solution. The first subsidiary hosts its data warehouse in Amazon Redshift. The second subsidiary uses Teradata Vantage on AWS. The third subsidiary uses Google BigQuery.\nThe company wants to aggregate all the data into a central Amazon S3 data lake. The company wants to use Apache Iceberg as the table format.\nA data engineer needs to build a new pipeline to connect to all the data sources, run transformations by using each source engine, join the data, and write the data to Iceberg.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use native Amazon Redshift, Teradata, and BigQuery connectors to build the pipeline in AWS Glue. Use native AWS Glue transforms to join the data. Run a Merge operation on the data lake Iceberg table."
      },
      {
        "letter": "B",
        "text": "Use the Amazon Athena federated query connectors for Amazon Redshift, Teradata, and BigQuery to build the pipeline in Athena. Write a SQL query to read from all the data sources, join the data, and run a Merge operation on the data lake Iceberg table."
      },
      {
        "letter": "C",
        "text": "Use the native Amazon Redshift connector, the Java Database Connectivity (JDBC) connector for Teradata, and the open source Apache Spark BigQuery connector to build the pipeline in Amazon EMR. Write code in PySpark to join the data. Run a Merge operation on the data lake Iceberg table."
      },
      {
        "letter": "D",
        "text": "Use the native Amazon Redshift, Teradata, and BigQuery connectors in Amazon Appflow to write data to Amazon S3 and AWS Glue Data Catalog. Use Amazon Athena to join the data. Run a Merge operation on the data lake Iceberg table."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AlejandroU",
        "date": "Mon 13 Oct 2025 04:41",
        "comment": "B) Answer B:\n1) Athena with federated connectors ( Redshift, Teradata via JDBC connector, BigQuery connector). No cluster to run. \n2) You write one SQL to transform/join across all three.\n3) Athena supports MERGE into Inceberg tables so that you can materialise results directly in the S3 Data Lake.",
        "selected_answer": "B"
      },
      {
        "author": "AminTriton",
        "date": "Tue 19 Aug 2025 12:49",
        "comment": "Athena supports federated queries, including connectors for Redshift, Teradata, and BigQuery.\nQueries can push down operations to each source engine (meeting the “run transformations using each source engine” requirement).\nAthena natively supports Iceberg tables on Amazon S3, including MERGE INTO operations.\nVery low operational overhead: only SQL queries, no infrastructure to manage.\n✅ Best fit.",
        "selected_answer": "B"
      },
      {
        "author": "Mitchdu",
        "date": "Sun 15 Jun 2025 19:40",
        "comment": "Glue, for sure. Athena is an ad-hoc querying tool, not and ETL tool and besides doesn't have connectors for Bigquery and Terradata!",
        "selected_answer": "A"
      },
      {
        "author": "AWSMM",
        "date": "Mon 28 Apr 2025 08:40",
        "comment": "Native Connectors: AWS Glue provides built-in connectors for Amazon Redshift, Teradata, and Google BigQuery. This eliminates the need for custom-built connectors, reducing development and maintenance overhead.",
        "selected_answer": "A"
      },
      {
        "author": "bad1ccc",
        "date": "Thu 03 Apr 2025 10:13",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/federated-queries.html",
        "selected_answer": "B"
      },
      {
        "author": "Palee",
        "date": "Mon 17 Mar 2025 05:41",
        "comment": "The requirement is to aggregate the data in S3. Only option has exclusively called this out. So Ans D is correct",
        "selected_answer": "D"
      },
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 18:55",
        "comment": "Athena can be used to build certain types of data pipelines, particularly when the primary focus is on ad-hoc analysis and querying large datasets stored in S3 without the need for complex data transformations, but for more intricate data processing and heavy ETL operations, other AWS services like Glue are often more suitable due to their dedicated data processing capabilities.",
        "selected_answer": "A"
      },
      {
        "author": "Eeshav15",
        "date": "Mon 13 Jan 2025 01:07",
        "comment": "Glue is the right tool \nto build pipeline",
        "selected_answer": "A"
      },
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 16:25",
        "comment": "https://docs.aws.amazon.com/athena/latest/ug/connectors-available.html",
        "selected_answer": "B"
      },
      {
        "author": "Eleftheriia",
        "date": "Thu 14 Nov 2024 11:21",
        "comment": "Would it be B\n\"If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.\"\n\nhttps://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html",
        "selected_answer": "B"
      },
      {
        "author": "ae35a02",
        "date": "Mon 28 Oct 2024 14:36",
        "comment": "AWS GLUE has native connectors to Redshift, BigQuery and Terradata, and integrates with Iceberg format.\nAthena is not for building Pipelines, AppFlow is for transfering data from Saas applications",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151854-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 164 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 164,
    "question_text": "A company is building a data stream processing application. The application runs in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The application stores processed data in an Amazon DynamoDB table.\nThe company needs the application containers in the EKS cluster to have secure access to the DynamoDB table. The company does not want to embed AWS credentials in the containers.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Store the AWS credentials in an Amazon S3 bucket. Grant the EKS containers access to the S3 bucket to retrieve the credentials."
      },
      {
        "letter": "B",
        "text": "Attach an IAM role to the EKS worker nodes, Grant the IAM role access to DynamoDUse the IAM role to set up IAM roles service accounts (IRSA) functionality."
      },
      {
        "letter": "C",
        "text": "Create an IAM user that has an access key to access the DynamoDB table. Use environment variables in the EKS containers to store the IAM user access key data."
      },
      {
        "letter": "D",
        "text": "Create an IAM user that has an access key to access the DynamoDB table. Use Kubernetes secrets that are mounted in a volume of the EKS duster nodes to store the user access key data."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 03:41",
        "comment": "https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html",
        "selected_answer": "B"
      },
      {
        "author": "jacob_nz",
        "date": "Fri 22 Nov 2024 23:28",
        "comment": "https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151933-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 165 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 165,
    "question_text": "A data engineer needs to onboard a new data producer into AWS. The data producer needs to migrate data products to AWS.\nThe data producer maintains many data pipelines that support a business application. Each pipeline must have service accounts and their corresponding credentials. The data engineer must establish a secure connection from the data producer's on-premises data center to AWS. The data engineer must not use the public internet to transfer data from an on-premises data center to AWS.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Instruct the new data producer to create Amazon Machine Images (AMIs) on Amazon Elastic Container Service (Amazon ECS) to store the code base of the application. Create security groups in a public subnet that allow connections only to the on-premises data center."
      },
      {
        "letter": "B",
        "text": "Create an AWS Direct Connect connection to the on-premises data center. Store the service account credentials in AWS Secrets manager."
      },
      {
        "letter": "C",
        "text": "Create a security group in a public subnet. Configure the security group to allow only connections from the CIDR blocks that correspond to the data producer. Create Amazon S3 buckets than contain presigned URLS that have one-day expiration dates."
      },
      {
        "letter": "D",
        "text": "Create an AWS Direct Connect connection to the on-premises data center. Store the application keys in AWS Secrets Manager. Create Amazon S3 buckets that contain presigned URLS that have one-day expiration dates."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ell89",
        "date": "Thu 27 Feb 2025 22:40",
        "comment": "B. all others contain partial nonsense",
        "selected_answer": "B"
      },
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:04",
        "comment": "For secure connections without cost constraints, always think Direct Connect.",
        "selected_answer": "B"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 03:48",
        "comment": "Direct Connect + Secret Manager",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150401-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 166 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 166,
    "question_text": "A data engineer configured an AWS Glue Data Catalog for data that is stored in Amazon S3 buckets. The data engineer needs to configure the Data Catalog to receive incremental updates.\nThe data engineer sets up event notifications for the S3 bucket and creates an Amazon Simple Queue Service (Amazon SQS) queue to receive the S3 events.\nWhich combination of steps should the data engineer take to meet these requirements with LEAST operational overhead? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Create an S3 event-based AWS Glue crawler to consume events from the SQS queue."
      },
      {
        "letter": "B",
        "text": "Define a time-based schedule to run the AWS Glue crawler, and perform incremental updates to the Data Catalog."
      },
      {
        "letter": "C",
        "text": "Use an AWS Lambda function to directly update the Data Catalog based on S3 events that the SQS queue receives."
      },
      {
        "letter": "D",
        "text": "Manually initiate the AWS Glue crawler to perform updates to the Data Catalog when there is a change in the S3 bucket."
      },
      {
        "letter": "E",
        "text": "Use AWS Step Functions to orchestrate the process of updating the Data Catalog based on S3 events that the SQS queue receives."
      }
    ],
    "correct_answer": "AB",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AlejandroU",
        "date": "Mon 13 Oct 2025 05:30",
        "comment": "A,B. Answer A and B.\nA) S3 event-based Glue crawler: configure S3 to send object events to SQS, and set the crawler to “Crawl based on events” using that queue. The crawler ingests only changes (incremental) and avoids full listings.\nB) Time-based schedule. Event-based crawlers still run on a schedule to poll SQS. If there are events, they update the Catalog.",
        "selected_answer": "AB"
      },
      {
        "author": "AminTriton",
        "date": "Tue 19 Aug 2025 13:31",
        "comment": "C has higher operational effort: you’d need to write/maintain Lambda code for schema inference, catalog updates, and error handling. Glue already provides managed crawlers.",
        "selected_answer": "AB"
      },
      {
        "author": "Ell89",
        "date": "Thu 27 Feb 2025 22:42",
        "comment": "•\tA leverages the event-driven capability of Glue Crawlers.\n\t•\tC uses AWS Lambda for direct and real-time updates to the Data Catalog.\n\t•\tThis combination ensures incremental updates are made only when changes occur, reducing costs and operational complexity.",
        "selected_answer": "AC"
      },
      {
        "author": "YUICH",
        "date": "Thu 30 Jan 2025 01:37",
        "comment": "(A) S3 Event-Based Crawler: Automatically triggers incremental catalog updates whenever new data arrives in the S3 bucket, reducing the need for custom code and manual intervention.\n\n(B) Time-Based Schedule: Periodically runs the crawler to catch any missed events and keep the data catalog accurate and up to date.\n\nUsing both methods minimizes operational overhead while ensuring comprehensive and reliable incremental updates.",
        "selected_answer": "AB"
      },
      {
        "author": "axantroff",
        "date": "Thu 26 Dec 2024 15:52",
        "comment": "Check out the design pattern documentation for this case. There's no need for Lambda here, so option C should be excluded. Option B seems viable, along with option A (A is the obvious choice for me).\n\nhttps://aws.amazon.com/blogs/big-data/run-aws-glue-crawlers-using-amazon-s3-event-notifications/",
        "selected_answer": "AB"
      },
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 16:32",
        "comment": "B and D are wrong due too \"Manually\" and \"Scheduling\".\nE is too much for this use case",
        "selected_answer": "AC"
      },
      {
        "author": "tucobbad",
        "date": "Wed 06 Nov 2024 17:54",
        "comment": "- Option A suggests creating an S3 event-based AWS Glue crawler to consume events from the SQS queue. This option is appropriate as it allows the crawler to automatically respond to events, thereby reducing manual intervention and ensuring timely updates to the Data Catalog\n\n- Option C involves using an AWS Lambda function to directly update the Data Catalog based on S3 events received from the SQS queue. This is a strong candidate as it automates the update process without the need for manual scheduling or intervention, thus minimizing operational overhead. AWS Glue Crawlers can consume events from an SQS queue: https://docs.aws.amazon.com/glue/latest/dg/crawler-s3-event-notifications.html",
        "selected_answer": "AC"
      },
      {
        "author": "pikuantne",
        "date": "Thu 31 Oct 2024 13:06",
        "comment": "Based on this article (Option 1 for the architecture) it should be AB:\n\n1. Run the crawler on a schedule.\n2. Crawler polls for object create events in the SQS queue\n3a. If there are events, crawler updates the Data Catalog\n3b. If not, crawler stops",
        "selected_answer": "AB"
      },
      {
        "author": "ae35a02",
        "date": "Mon 28 Oct 2024 14:54",
        "comment": "AWS Glue Crawlers can not consupe events from an SQS queue\nD introduce a manual operation\nE introduce more complexity\nso BC",
        "selected_answer": "BC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151935-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 167 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 167,
    "question_text": "A company uses AWS Glue Data Catalog to index data that is uploaded to an Amazon S3 bucket every day. The company uses a daily batch processes in an extract, transform, and load (ETL) pipeline to upload data from external sources into the S3 bucket.\nThe company runs a daily report on the S3 data. Some days, the company runs the report before all the daily data has been uploaded to the S3 bucket. A data engineer must be able to send a message that identifies any incomplete data to an existing Amazon Simple Notification Service (Amazon SNS) topic.\nWhich solution will meet this requirement with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create data quality checks for the source datasets that the daily reports use. Create a new AWS managed Apache Airflow cluster. Run the data quality checks by using Airflow tasks that run data quality queries on the columns data type and the presence of null values. Configure Airflow Directed Acyclic Graphs (DAGs) to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic."
      },
      {
        "letter": "B",
        "text": "Create data quality checks on the source datasets that the daily reports use. Create a new Amazon EMR cluster. Use Apache Spark SQL to create Apache Spark jobs in the EMR cluster that run data quality queries on the columns data type and the presence of null values. Orchestrate the ETL pipeline by using an AWS Step Functions workflow. Configure the workflow to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic."
      },
      {
        "letter": "C",
        "text": "Create data quality checks on the source datasets that the daily reports use. Create data quality actions by using AWS Glue workflows to confirm the completeness and consistency of the datasets. Configure the data quality actions to create an event in Amazon EventBridge if a dataset is incomplete. Configure EventBridge to send the event that informs the data engineer about the incomplete datasets to the Amazon SNS topic."
      },
      {
        "letter": "D",
        "text": "Create AWS Lambda functions that run data quality queries on the columns data type and the presence of null values. Orchestrate the ETL pipeline by using an AWS Step Functions workflow that runs the Lambda functions. Configure the Step Functions workflow to send an email notification that informs the data engineer about the incomplete datasets to the SNS topic."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "7a1d491",
        "date": "Wed 18 Dec 2024 11:46",
        "comment": "C LEAST operational overhead",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150885-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 168 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 168,
    "question_text": "A company stores customer data that contains personally identifiable information (PII) in an Amazon Redshift cluster. The company's marketing, claims, and analytics teams need to be able to access the customer data.\nThe marketing team should have access to obfuscated claim information but should have full access to customer contact information. The claims team should have access to customer information for each claim that the team processes. The analytics team should have access only to obfuscated PII data.\nWhich solution will enforce these data access requirements with the LEAST administrative overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a separate Redshift cluster for each team. Load only the required data for each team. Restrict access to clusters based on the teams."
      },
      {
        "letter": "B",
        "text": "Create views that include required fields for each of the data requirements. Grant the teams access only to the view that each team requires."
      },
      {
        "letter": "C",
        "text": "Create a separate Amazon Redshift database role for each team. Define masking policies that apply for each team separately. Attach appropriate masking policies to each team role."
      },
      {
        "letter": "D",
        "text": "Move the customer data to an Amazon S3 bucket. Use AWS Lake Formation to create a data lake. Use fine-grained security capabilities to grant each team appropriate permissions to access the data."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Tue 19 Aug 2025 13:42",
        "comment": "It cannot be C. Why? Dynamic data masking handles column privacy but does not implement the claims team’s row-level restriction by itself. So it does not meet the LEAST operational requirement. Creating a view does.",
        "selected_answer": "B"
      },
      {
        "author": "rssrss",
        "date": "Mon 18 Aug 2025 19:44",
        "comment": "Amazon Redshift does not natively support column-level masking policies. Implementing this would require custom logic or using stored procedures, which increases complexity.\nViews are a simpler and more standard solution for this use case.",
        "selected_answer": "B"
      },
      {
        "author": "michele_scar",
        "date": "Fri 15 Nov 2024 16:34",
        "comment": "It's the only answer that match least operation and masking information",
        "selected_answer": "C"
      },
      {
        "author": "tucobbad",
        "date": "Wed 06 Nov 2024 18:01",
        "comment": "To me, it seems C is the best approach as Redshift has Dynamic Data Masking feature:\nhttps://docs.aws.amazon.com/redshift/latest/dg/t_ddm.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151937-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 169 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 169,
    "question_text": "A financial company recently added more features to its mobile app. The new features required the company to create a new topic in an existing Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster.\nA few days after the company added the new topic, Amazon CloudWatch raised an alarm on the RootDiskUsed metric for the MSK cluster.\nHow should the company address the CloudWatch alarm?",
    "choices": [
      {
        "letter": "A",
        "text": "Expand the storage of the MSK broker. Configure the MSK cluster storage to expand automatically."
      },
      {
        "letter": "B",
        "text": "Expand the storage of the Apache ZooKeeper nodes."
      },
      {
        "letter": "C",
        "text": "Update the MSK broker instance to a larger instance type. Restart the MSK cluster."
      },
      {
        "letter": "D",
        "text": "Specify the Target Volume-in-GiB parameter for the existing topic."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:12",
        "comment": "\"RootDiskUsed\" is the percentage of the percentage of root disk used by the broker. Expanding storage and enabling automatic scaling seems like the best bet.",
        "selected_answer": "A"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 04:39",
        "comment": "https://docs.aws.amazon.com/msk/latest/developerguide/metrics-details.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151938-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 170 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 170,
    "question_text": "A data engineer needs to build an enterprise data catalog based on the company's Amazon S3 buckets and Amazon RDS databases. The data catalog must include storage format metadata for the data in the catalog.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Glue crawler to scan the S3 buckets and RDS databases and build a data catalog. Use data stewards to inspect the data and update the data catalog with the data format."
      },
      {
        "letter": "B",
        "text": "Use an AWS Glue crawler to build a data catalog. Use AWS Glue crawler classifiers to recognize the format of data and store the format in the catalog."
      },
      {
        "letter": "C",
        "text": "Use Amazon Macie to build a data catalog and to identify sensitive data elements. Collect the data format information from Macie."
      },
      {
        "letter": "D",
        "text": "Use scripts to scan data elements and to assign data classifications based on the format of the data."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 04:43",
        "comment": "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151939-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 171 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 171,
    "question_text": "A company analyzes data in a data lake every quarter to perform inventory assessments. A data engineer uses AWS Glue DataBrew to detect any personally identifiable formation (PII) about customers within the data. The company's privacy policy considers some custom categories of information to be PII. However, the categories are not included in standard DataBrew data quality rules.\nThe data engineer needs to modify the current process to scan for the custom PII categories across multiple datasets within the data lake.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Manually review the data for custom PII categories."
      },
      {
        "letter": "B",
        "text": "Implement custom data quality rules in DataBrew. Apply the custom rules across datasets."
      },
      {
        "letter": "C",
        "text": "Develop custom Python scripts to detect the custom PII categories. Call the scripts from DataBrew."
      },
      {
        "letter": "D",
        "text": "Implement regex patterns to extract PII information from fields during extract transform, and load (ETL) operations into the data lake."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 04:54",
        "comment": "https://aws.amazon.com/blogs/big-data/enforce-customized-data-quality-rules-in-aws-glue-databrew/",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151940-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 172 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 172,
    "question_text": "A company receives a data file from a partner each day in an Amazon S3 bucket. The company uses a daily AWS Glue extract, transform, and load (ETL) pipeline to clean and transform each data file. The output of the ETL pipeline is written to a CSV file named Daily.csv in a second S3 bucket.\nOccasionally, the daily data file is empty or is missing values for required fields. When the file is missing data, the company can use the previous day’s CSV file.\nA data engineer needs to ensure that the previous day's data file is overwritten only if the new daily file is complete and valid.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Invoke an AWS Lambda function to check the file for missing data and to fill in missing values in required fields."
      },
      {
        "letter": "B",
        "text": "Configure the AWS Glue ETL pipeline to use AWS Glue Data Quality rules. Develop rules in Data Quality Definition Language (DQDL) to check for missing values in required fields and empty files."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue Studio to change the code in the ETL pipeline to fill in any missing values in the required fields with the most common values for each field."
      },
      {
        "letter": "D",
        "text": "Run a SQL query in Amazon Athena to read the CSV file and drop missing rows. Copy the corrected CSV file to the second S3 bucket."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 04:58",
        "comment": "https://docs.aws.amazon.com/glue/latest/dg/glue-data-quality.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150343-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 173 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 173,
    "question_text": "A marketing company uses Amazon S3 to store marketing data. The company uses versioning in some buckets. The company runs several jobs to read and load data into the buckets.\nTo help cost-optimize its storage, the company wants to gather information about incomplete multipart uploads and outdated versions that are present in the S3 buckets.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS CLI to gather the information."
      },
      {
        "letter": "B",
        "text": "Use Amazon S3 Inventory configurations reports to gather the information."
      },
      {
        "letter": "C",
        "text": "Use the Amazon S3 Storage Lens dashboard to gather the information."
      },
      {
        "letter": "D",
        "text": "Use AWS usage reports for Amazon S3 to gather the information."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Wed 20 Aug 2025 20:15",
        "comment": "Amazon S3 Storage Lens provides organization-wide visibility into storage usage and activity trends, including:\n\t•\tIncomplete multipart uploads\n\t•\tObject versions (current and noncurrent/outdated)\n\t•\tOther insights like replication status, encryption, access activity, etc.",
        "selected_answer": "C"
      },
      {
        "author": "rssrss",
        "date": "Mon 18 Aug 2025 19:54",
        "comment": "B is the best solution because Amazon S3 Inventory provides detailed, automated reports about object versions and incomplete multipart uploads with the least operational effort, helping the company optimise storage costs effectively.",
        "selected_answer": "B"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 12:14",
        "comment": "Amazon S3 Storage Lens provides a comprehensive view of your S3 storage usage and activity. It includes metrics and insights related to incomplete multipart uploads, outdated versions of objects, and other storage characteristics.",
        "selected_answer": "C"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 05:12",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html",
        "selected_answer": "C"
      },
      {
        "author": "truongnguyen86",
        "date": "Thu 31 Oct 2024 10:05",
        "comment": "Should be C AWS Storage Lens: I asked ChatGPT it answered AWS S3 Storage Lens too",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150779-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 174 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 174,
    "question_text": "A gaming company uses Amazon Kinesis Data Streams to collect clickstream data. The company uses Amazon Data Firehose delivery streams to store the data in JSON format in Amazon S3. Data scientists at the company use Amazon Athena to query the most recent data to obtain business insights.\nThe company wants to reduce Athena costs but does not want to recreate the data pipeline.\nWhich solution will meet these requirements with the LEAST management effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Change the Firehose output format to Apache Parquet. Provide a custom S3 object YYYYMMDD prefix expression and specify a large buffer size. For the existing data, create an AWS Glue extract, transform, and load (ETL) job. Configure the ETL job to combine small JSON files, convert the JSON files to large Parquet files, and add the YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table."
      },
      {
        "letter": "B",
        "text": "Create an Apache Spark job that combines JSON files and converts the JSON files to Apache Parquet files. Launch an Amazon EMR ephemeral cluster every day to run the Spark job to create new Parquet files in a different S3 location. Use the ALTER TABLE SET LOCATION statement to reflect the new S3 location on the existing Athena table."
      },
      {
        "letter": "C",
        "text": "Create a Kinesis data stream as a delivery destination for Firehose. Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to run Apache Flink on the Kinesis data stream. Use Flink to aggregate the data and save the data to Amazon S3 in Apache Parquet format with a custom S3 object YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table."
      },
      {
        "letter": "D",
        "text": "Integrate an AWS Lambda function with Firehose to convert source records to Apache Parquet and write them to Amazon S3. In parallel, run an AWS Glue extract, transform, and load (ETL) job to combine the JSON files and convert the JSON files to large Parquet files. Create a custom S3 object YYYYMMDD prefix. Use the ALTER TABLE ADD PARTITION statement to reflect the partition on the existing Athena table."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "michele_scar",
        "date": "Mon 18 Nov 2024 12:02",
        "comment": "If you have JSON, Firehose should convert it without the needs of a Lambda",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151941-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 175 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 175,
    "question_text": "A company needs a solution to manage costs for an existing Amazon DynamoDB table. The company also needs to control the size of the table. The solution must not disrupt any ongoing read or write operations. The company wants to use a solution that automatically deletes data from the table after 1 month.\nWhich solution will meet these requirements with the LEAST ongoing maintenance?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the DynamoDB TTL feature to automatically expire data based on timestamps."
      },
      {
        "letter": "B",
        "text": "Configure a scheduled Amazon EventBridge rule to invoke an AWS Lambda function to check for data that is older than 1 month. Configure the Lambda function to delete old data."
      },
      {
        "letter": "C",
        "text": "Configure a stream on the DynamoDB table to invoke an AWS Lambda function. Configure the Lambda function to delete data in the table that is older than 1 month."
      },
      {
        "letter": "D",
        "text": "Use an AWS Lambda function to periodically scan the DynamoDB table for data that is older than 1 month. Configure the Lambda function to delete old data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:20",
        "comment": "DynamoDB TTL will automatically delete items based on how you configure.",
        "selected_answer": "A"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 05:21",
        "comment": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150344-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 176 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 176,
    "question_text": "A company uses Amazon S3 to store data and Amazon QuickSight to create visualizations,\nThe company has an S3 bucket in an AWS account named Hub-Account. The S3 bucket is encrypted by an AWS Key Management Service (AWS KMS) key. The company's QuickSight instance is in a separate account named BI-Account.\nThe company updates the S3 bucket policy to grant access to the QuickSight service role. The company wants to enable cross-account access to allow QuickSight to interact with the S3 bucket.\nWhich combination of steps will meet this requirement? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use the existing AWS KMS key to encrypt connections from QuickSight to the S3 bucket."
      },
      {
        "letter": "B",
        "text": "Add the S3 bucket as a resource that the QuickSight service role can access."
      },
      {
        "letter": "C",
        "text": "Use AWS Resource Access Manager (AWS RAM) to share the S3 bucket with the BI-Account account."
      },
      {
        "letter": "D",
        "text": "Add an IAM policy to the QuickSight service role to give QuickSight access to the KMS key that encrypts the S3 bucket."
      },
      {
        "letter": "E",
        "text": "Add the KMS key as a resource that the QuickSight service role can access."
      }
    ],
    "correct_answer": "E",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "devan007",
        "date": "Sun 24 Nov 2024 05:42",
        "comment": "D & E\nS3 bucket policy is already updated from the question. Hence KMS key policy and IAM policy need to be altered to allow QuickSight service account to access KMS key.",
        "selected_answer": "E"
      },
      {
        "author": "AlejandroU",
        "date": "Mon 13 Oct 2025 07:17",
        "comment": "Answer: B and D.\nB explicitly states adding S3 as a resource to the QuickSight role's IAM policy (s3:GetObject, s3:ListBucket permissions needed in BI-Account)\nD explicitly states adding KMS permissions to the role's IAM policy (kms:Decrypt needed)\n\nE is vague—\"add KMS key as a resource\" could mean UI config or policy, but doesn't specify the IAM policy on the role.",
        "selected_answer": "D"
      },
      {
        "author": "AminTriton",
        "date": "Wed 20 Aug 2025 21:22",
        "comment": "B and E.\n\t•\tB. Add the cross-account S3 bucket as a resource the QuickSight service role can access (in QuickSight: Manage QuickSight → Security & permissions → QuickSight access to AWS services → S3 buckets).\n\t•\tE. Add the KMS key as a resource the QuickSight service role can access (same page, add the key ARN), so the role can decrypt the S3 objects.",
        "selected_answer": "B"
      },
      {
        "author": "praveenu",
        "date": "Tue 20 May 2025 06:28",
        "comment": "Correct answer B & D\nhttps://repost.aws/knowledge-center/quicksight-cross-account-s3\n\nI want to use data from an Amazon Simple Storage Service (Amazon S3) bucket in another account to create a dataset in Amazon QuickSight.\n\nShort description\nComplete the following steps to create cross-account access from Amazon QuickSight (Account A) to an encrypted Amazon S3 bucket in another account (Account B):\n\nUpdate your S3 bucket policy in Account B where your S3 bucket resides.\nAdd the S3 bucket as a resource that the QuickSight service role (Account A) can access.\nAllow the QuickSight service role access to the AWS Key Management Service (AWS KMS) key for the S3 bucket.",
        "selected_answer": "B"
      },
      {
        "author": "Ell89",
        "date": "Thu 27 Feb 2025 23:49",
        "comment": "B & E. the issue isnt with sharing the bucket as the bucket policy does that already to the service role. its an encryption issue.",
        "selected_answer": "E"
      },
      {
        "author": "fnuuu",
        "date": "Tue 11 Feb 2025 18:23",
        "comment": "BD :\nB - To ensure QS has permissions to access the S3\nD - To ensure QS has permission for KMS to decrypt date in S3",
        "selected_answer": "B"
      },
      {
        "author": "YUICH",
        "date": "Wed 22 Jan 2025 09:50",
        "comment": "BD\nConclusion: To enable cross-account access for both (1) the Amazon S3 bucket and (2) the KMS key used to encrypt that bucket, the QuickSight service role must be granted the appropriate permissions. Among the provided options, the following two steps are essential:\n\nB. Add the S3 bucket as a resource the QuickSight service role can access\n(→ Allows cross-account access to the S3 bucket)\n\nD. Add an IAM policy to the QuickSight service role that grants access to the KMS key\n(→ Allows decryption of data encrypted by the KMS key)",
        "selected_answer": "B"
      },
      {
        "author": "stevejake",
        "date": "Thu 16 Jan 2025 09:39",
        "comment": "S3 bucket policy is already updated from the question. Hence KMS key policy and IAM policy need to be altered to allow QuickSight service account to access KMS key.",
        "selected_answer": "D"
      },
      {
        "author": "YUICH",
        "date": "Tue 07 Jan 2025 06:52",
        "comment": "Given that the question states “Update the S3 bucket policy to allow access for the QuickSight service role” and, from the perspective of “enabling cross-account access so that QuickSight can interact with the S3 bucket,” is asking what additional steps are needed, we can conclude that:\n\n(B) “Add the S3 bucket as a resource accessible by the QuickSight service role”\n(E) “Add the KMS key as a resource accessible by the QuickSight service role”\n\ntogether most succinctly represent the final actions required.",
        "selected_answer": "B"
      },
      {
        "author": "michele_scar",
        "date": "Mon 18 Nov 2024 13:12",
        "comment": "B for bucket access\nE for KMS key policy",
        "selected_answer": "E"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151942-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 177 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 177,
    "question_text": "A car sales company maintains data about cars that are listed for sale in an area. The company receives data about new car listings from vendors who upload the data daily as compressed files into Amazon S3. The compressed files are up to 5 KB in size. The company wants to see the most up-to-date listings as soon as the data is uploaded to Amazon S3.\nA data engineer must automate and orchestrate the data processing workflow of the listings to feed a dashboard. The data engineer must also provide the ability to perform one-time queries and analytical reporting. The query solution must be scalable.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an Amazon EMR cluster to process incoming data. Use AWS Step Functions to orchestrate workflows. Use Apache Hive for one-time queries and analytical reporting. Use Amazon OpenSearch Service to bulk ingest the data into compute optimized instances. Use OpenSearch Dashboards in OpenSearch Service for the dashboard."
      },
      {
        "letter": "B",
        "text": "Use a provisioned Amazon EMR cluster to process incoming data. Use AWS Step Functions to orchestrate workflows. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue to process incoming data. Use AWS Step Functions to orchestrate workflows. Use Amazon Redshift Spectrum for one-time queries and analytical reporting. Use OpenSearch Dashboards in Amazon OpenSearch Service for the dashboard."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue to process incoming data. Use AWS Lambda and S3 Event Notifications to orchestrate workflows. Use Amazon Athena for one-time queries and analytical reporting. Use Amazon QuickSight for the dashboard."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "axantroff",
        "date": "Thu 26 Dec 2024 18:25",
        "comment": "I don't particularly like the formulation where AWS Lambda and S3 Event Notifications are described as being responsible for orchestrating any workflow. However, I believe Athena is a much more suitable solution in this case compared to AWS Redshift, so going with option D seems to be a reasonable choice at some point",
        "selected_answer": "D"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 11:57",
        "comment": "seems like C could be the answer but setting up redshift cluster takes much longer to get the same thing like Athena. so D",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/152459-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 178 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 178,
    "question_text": "A company has AWS resources in multiple AWS Regions. The company has an Amazon EFS file system in each Region where the company operates. The company’s data science team operates within only a single Region. The data that the data science team works with must remain within the team's Region.\nA data engineer needs to create a single dataset by processing files that are in each of the company's Regional EFS file systems. The data engineer wants to use an AWS Step Functions state machine to orchestrate AWS Lambda functions to process the data.\nWhich solution will meet these requirements with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Peer the VPCs that host the EFS file systems in each Region with the VPC that is in the data science team’s Region. Enable EFS file locking. Configure the Lambda functions in the data science team's Region to mount each of the Region specific file systems. Use the Lambda functions to process the data."
      },
      {
        "letter": "B",
        "text": "Configure each of the Regional EFS file systems to replicate data to the data science team's Region. In the data science team’s Region, configure the Lambda functions to mount the replica file systems. Use the Lambda functions to process the data."
      },
      {
        "letter": "C",
        "text": "Deploy the Lambda functions to each Region. Mount the Regional EFS file systems to the Lambda functions. Use the Lambda functions to process the data. Store the output in an Amazon S3 bucket in the data science team’s Region."
      },
      {
        "letter": "D",
        "text": "Use AWS DataSync to transfer files from each of the Regional EFS files systems to the file system that is in the data science team's Region. Configure the Lambda functions in the data science team's Region to mount the file system that is in the same Region. Use the Lambda functions to process the data."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 11:59",
        "comment": "Data Sync is for large scale migration, Lambdas would do just fine here... C",
        "selected_answer": "C"
      },
      {
        "author": "Vidhi212",
        "date": "Mon 16 Dec 2024 13:46",
        "comment": "Using AWS DataSync in Option D achieves the desired data consolidation efficiently while keeping the workflow simple and cost-effective. It aligns with the data locality requirement and reduces engineering effort.",
        "selected_answer": "D"
      },
      {
        "author": "7a1d491",
        "date": "Mon 16 Dec 2024 13:40",
        "comment": "Peer the VPC introduce complexity, D is a much better solution",
        "selected_answer": "D"
      },
      {
        "author": "emupsx1",
        "date": "Mon 02 Dec 2024 11:44",
        "comment": "maybe A?",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151944-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 179 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 179,
    "question_text": "A company hosts its applications on Amazon EC2 instances. The company must use SSL/TLS connections that encrypt data in transit to communicate securely with AWS infrastructure that is managed by a customer.\nA data engineer needs to implement a solution to simplify the generation, distribution, and rotation of digital certificates. The solution must automatically renew and deploy SSL/TLS certificates.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Store self-managed certificates on the EC2 instances."
      },
      {
        "letter": "B",
        "text": "Use AWS Certificate Manager (ACM)."
      },
      {
        "letter": "C",
        "text": "Implement custom automation scripts in AWS Secrets Manager."
      },
      {
        "letter": "D",
        "text": "Use Amazon Elastic Container Service (Amazon ECS) Service Connect."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:27",
        "comment": "ACM takes care of creating, storing, and renewing SSL/TLS certificates and keys",
        "selected_answer": "B"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 07:27",
        "comment": "https://aws.amazon.com/tw/certificate-manager/",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151945-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 180 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 180,
    "question_text": "A company saves customer data to an Amazon S3 bucket. The company uses server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the bucket. The dataset includes personally identifiable information (PII) such as social security numbers and account details.\nData that is tagged as PII must be masked before the company uses customer data for analysis. Some users must have secure access to the PII data during the pre-processing phase. The company needs a low-maintenance solution to mask and secure the PII data throughout the entire engineering pipeline.\nWhich combination of solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue DataBrew to perform extract, transform, and load (ETL) tasks that mask the PII data before analysis."
      },
      {
        "letter": "B",
        "text": "Use Amazon GuardDuty to monitor access patterns for the PII data that is used in the engineering pipeline."
      },
      {
        "letter": "C",
        "text": "Configure an Amazon Macie discovery job for the S3 bucket."
      },
      {
        "letter": "D",
        "text": "Use AWS Identity and Access Management (IAM) to manage permissions and to control access to the PII data."
      },
      {
        "letter": "E",
        "text": "Write custom scripts in an application to mask the PII data and to control access."
      }
    ],
    "correct_answer": "AD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:28",
        "comment": "A will find and mask the PII\nD for access",
        "selected_answer": "AD"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 12:08",
        "comment": "A for data maskin and D for access",
        "selected_answer": "AD"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 07:31",
        "comment": "https://aws.amazon.com/tw/blogs/big-data/build-a-data-pipeline-to-automatically-discover-and-mask-pii-data-with-aws-glue-databrew/",
        "selected_answer": "AD"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151948-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 181 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 181,
    "question_text": "A data engineer is launching an Amazon EMR cluster. The data that the data engineer needs to load into the new cluster is currently in an Amazon S3 bucket. The data engineer needs to ensure that data is encrypted both at rest and in transit.\nThe data that is in the S3 bucket is encrypted by an AWS Key Management Service (AWS KMS) key. The data engineer has an Amazon S3 path that has a Privacy Enhanced Mail (PEM) file.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Create a second security configuration. Specify the Amazon S3 path of the PEM file for in-transit encryption. Create the EMR cluster, and attach both security configurations to the cluster."
      },
      {
        "letter": "B",
        "text": "Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for local disk encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Use the security configuration during EMR cluster creation."
      },
      {
        "letter": "C",
        "text": "Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Use the security configuration during EMR cluster creation."
      },
      {
        "letter": "D",
        "text": "Create an Amazon EMR security configuration. Specify the appropriate AWS KMS key for at-rest encryption for the S3 bucket. Specify the Amazon S3 path of the PEM file for in-transit encryption. Create the EMR cluster, and attach the security configuration to the cluster."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "praveenu",
        "date": "Tue 20 May 2025 06:39",
        "comment": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-specify-security-configuration.html\n\nUnder Security configuration and permissions, find the Security configuration field. Select the dropdown menu or choose Browse to select the name of a security configuration that you created previously. Alternatively, choose Create security configuration to create a configuration that you can use for your cluster.\n\nChoose any other options that apply to your cluster.\n\nTo launch your cluster, choose Create cluster.",
        "selected_answer": "C"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 10:06",
        "comment": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-specify-security-configuration.html\nUnder Security configuration and permissions, find the Security configuration field. Select the dropdown menu or choose Browse to select the name of a security configuration that you created previously. Alternatively, choose Create security configuration to create a configuration that you can use for your cluster.\n\nChoose any other options that apply to your cluster.\n\nTo launch your cluster, choose Create cluster.",
        "selected_answer": "D"
      },
      {
        "author": "AWSMM",
        "date": "Mon 28 Apr 2025 20:46",
        "comment": "D correctly identifies the need for a single EMR security configuration that encompasses both at-rest (using the KMS key related to your S3 data) and in-transit (using the PEM file path) encryption settings. Attaching this single configuration to the cluster during creation is the standard and most efficient way to apply these security measures.",
        "selected_answer": "D"
      },
      {
        "author": "italiancloud2025",
        "date": "Sun 23 Feb 2025 15:20",
        "comment": "D: Sí, porque crea una única configuración de seguridad que especifica encriptación en reposo (con KMS) y en tránsito (usando el archivo PEM), y se adjunta al clúster durante su creación.",
        "selected_answer": "D"
      },
      {
        "author": "emupsx1",
        "date": "Mon 25 Nov 2024 07:43",
        "comment": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-specify-security-configuration.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/151958-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 182 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 182,
    "question_text": "A retail company is using an Amazon Redshift cluster to support real-time inventory management. The company has deployed an ML model on a real-time endpoint in Amazon SageMaker.\nThe company wants to make real-time inventory recommendations. The company also wants to make predictions about future inventory needs.\nWhich solutions will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Redshift ML to generate inventory recommendations."
      },
      {
        "letter": "B",
        "text": "Use SQL to invoke a remote SageMaker endpoint for prediction."
      },
      {
        "letter": "C",
        "text": "Use Amazon Redshift ML to schedule regular data exports for offline model training."
      },
      {
        "letter": "D",
        "text": "Use SageMaker Autopilot to create inventory management dashboards in Amazon Redshift."
      },
      {
        "letter": "E",
        "text": "Use Amazon Redshift as a file storage system to archive old inventory management reports."
      }
    ],
    "correct_answer": "AB",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:33",
        "comment": "A and B \nRedshift ML for data exports? Nah. \nSageMaker autopilot is for building/training/deploying models\nRedshift for file storage?",
        "selected_answer": "AB"
      },
      {
        "author": "emupsx1",
        "date": "Sat 30 Nov 2024 01:42",
        "comment": "The company wants to make real-time inventory recommendations. Select (A) recommendations.\nThe company also wants to make predictions about future inventory needs. Select (B) prediction.",
        "selected_answer": "AB"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/152017-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 183 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 183,
    "question_text": "A company stores CSV files in an Amazon S3 bucket. A data engineer needs to process the data in the CSV files and store the processed data in a new S3 bucket.\nThe process needs to rename a column, remove specific columns, ignore the second row of each file, create a new column based on the values of the first row of the data, and filter the results by a numeric value of a column.\nWhich solution will meet these requirements with the LEAST development effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue Python jobs to read and transform the CSV files."
      },
      {
        "letter": "B",
        "text": "Use an AWS Glue custom crawler to read and transform the CSV files."
      },
      {
        "letter": "C",
        "text": "Use an AWS Glue workflow to build a set of jobs to crawl and transform the CSV files."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue DataBrew recipes to read and transform the CSV files."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 11:46",
        "comment": "all more or less common operations all avilalble in data brew.",
        "selected_answer": "D"
      },
      {
        "author": "emupsx1",
        "date": "Tue 26 Nov 2024 00:05",
        "comment": "https://docs.aws.amazon.com/databrew/latest/dg/recipes.html",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150749-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 184 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 184,
    "question_text": "A company uses Amazon Redshift as its data warehouse. Data encoding is applied to the existing tables of the data warehouse. A data engineer discovers that the compression encoding applied to some of the tables is not the best fit for the data.\nThe data engineer needs to improve the data encoding for the tables that have sub-optimal encoding.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Run the ANALYZE command against the identified tables. Manually update the compression encoding of columns based on the output of the command."
      },
      {
        "letter": "B",
        "text": "Run the ANALYZE COMPRESSION command against the identified tables. Manually update the compression encoding of columns based on the output of the command."
      },
      {
        "letter": "C",
        "text": "Run the VACUUM REINDEX command against the identified tables."
      },
      {
        "letter": "D",
        "text": "Run the VACUUM RECLUSTER command against the identified tables."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Ramdi1",
        "date": "Sat 15 Mar 2025 15:31",
        "comment": "Amazon Redshift uses columnar storage with compression encoding to optimize query performance and reduce storage costs. Over time, sub-optimal encoding may lead to poor performance.\n\nTo determine the best compression encoding for a table, use the ANALYZE COMPRESSION command, which:\n🔹 Scans the table's data and suggests optimal encoding types for each column.\n🔹 Helps reduce storage size and improve query efficiency.\n🔹 Requires a manual column update because Amazon Redshift does not automatically apply new encodings.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/152018-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 185 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 185,
    "question_text": "The company stores a large volume of customer records in Amazon S3. To comply with regulations, the company must be able to access new customer records immediately for the first 30 days after the records are created. The company accesses records that are older than 30 days infrequently.\nThe company needs to cost-optimize its Amazon S3 storage.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Apply a lifecycle policy to transition records to S3 Standard Infrequent-Access (S3 Standard-IA) storage after 30 days."
      },
      {
        "letter": "B",
        "text": "Use S3 Intelligent-Tiering storage."
      },
      {
        "letter": "C",
        "text": "Transition records to S3 Glacier Deep Archive storage after 30 days."
      },
      {
        "letter": "D",
        "text": "Use S3 Standard-Infrequent Access (S3 Standard-IA) storage for all customer records."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Kayceetalks",
        "date": "Thu 22 May 2025 22:27",
        "comment": "Using S3 Intelligent-Tiering storage would save cost on the long.",
        "selected_answer": "B"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 11:52",
        "comment": "this is badly defined question, it is not saying what is going on with data in firs 30 days, but cost efficiency indicates it is not B thus I would chose A",
        "selected_answer": "A"
      },
      {
        "author": "emupsx1",
        "date": "Tue 26 Nov 2024 00:11",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/152019-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 186 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 186,
    "question_text": "A data engineer is using Amazon QuickSight to build a dashboard to report a company’s revenue in multiple AWS Regions. The data engineer wants the dashboard to display the total revenue for a Region, regardless of the drill-down levels shown in the visual.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a table calculation."
      },
      {
        "letter": "B",
        "text": "Create a simple calculated field."
      },
      {
        "letter": "C",
        "text": "Create a level-aware calculation - aggregate (LAC-A) function."
      },
      {
        "letter": "D",
        "text": "Create a level-aware calculation - window (LAC-W) function."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "emupsx1",
        "date": "Tue 26 Nov 2024 00:15",
        "comment": "https://docs.aws.amazon.com/quicksight/latest/user/level-aware-calculations.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150748-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 187 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 187,
    "question_text": "A retail company stores customer data in an Amazon S3 bucket. Some of the customer data contains personally identifiable information (PII) about customers. The company must not share PII data with business partners.\nA data engineer must determine whether a dataset contains PII before making objects in the dataset available to business partners.\nWhich solution will meet this requirement with the LEAST manual intervention?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the S3 bucket and S3 objects to allow access to Amazon Macie. Use automated sensitive data discovery in Macie."
      },
      {
        "letter": "B",
        "text": "Configure AWS CloudTrail to monitor S3 PUT operations. Inspect the CloudTrail trails to identify operations that save PII."
      },
      {
        "letter": "C",
        "text": "Create an AWS Lambda function to identify PII in S3 objects. Schedule the function to run periodically."
      },
      {
        "letter": "D",
        "text": "Create a table in AWS Glue Data Catalog. Write custom SQL queries to identify PII in the table. Use Amazon Athena to run the queries."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:38",
        "comment": "Macie is a great option for PII discovery.",
        "selected_answer": "A"
      },
      {
        "author": "michele_scar",
        "date": "Mon 18 Nov 2024 13:23",
        "comment": "PII -> Macie",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150345-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 188 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 188,
    "question_text": "A data engineer needs to create an empty copy of an existing table in Amazon Athena to perform data processing tasks. The existing table in Athena contains 1,000 rows.\nWhich query will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "CREATE TABLE new_table -LIKE old_table;"
      },
      {
        "letter": "B",
        "text": "CREATE TABLE new_table -AS SELECT *FROM old_table -WITH NO DATA;"
      },
      {
        "letter": "C",
        "text": "CREATE TABLE new_table -AS SELECT *FROM old_table;"
      },
      {
        "letter": "D",
        "text": "CREATE TABLE new_table -as SELECT *FROM old_cable -WHERE 1=1;"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "pikuantne",
        "date": "Thu 31 Oct 2024 13:13",
        "comment": "Definitely B",
        "selected_answer": "B"
      },
      {
        "author": "truongnguyen86",
        "date": "Thu 31 Oct 2024 10:26",
        "comment": "should be B with no data option to create empty table from CTAS",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150586-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 189 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 189,
    "question_text": "A company has a data lake in Amazon S3. The company collects AWS CloudTrail logs for multiple applications. The company stores the logs in the data lake, catalogs the logs in AWS Glue, and partitions the logs based on the year. The company uses Amazon Athena to analyze the logs.\nRecently, customers reported that a query on one of the Athena tables did not return any data. A data engineer must resolve the issue.\nWhich combination of troubleshooting steps should the data engineer take? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Confirm that Athena is pointing to the correct Amazon S3 location."
      },
      {
        "letter": "B",
        "text": "Increase the query timeout duration."
      },
      {
        "letter": "C",
        "text": "Use the MSCK REPAIR TABLE command."
      },
      {
        "letter": "D",
        "text": "Restart Athena."
      },
      {
        "letter": "E",
        "text": "Delete and recreate the problematic Athena table."
      }
    ],
    "correct_answer": "AC",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "michele_scar",
        "date": "Mon 18 Nov 2024 13:30",
        "comment": "B and D definitely wrong\nE should be dangerous",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150746-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 190 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 190,
    "question_text": "A data engineer wants to orchestrate a set of extract, transform, and load (ETL) jobs that run on AWS. The ETL jobs contain tasks that must run Apache Spark jobs on Amazon EMR, make API calls to Salesforce, and load data into Amazon Redshift.\nThe ETL jobs need to handle failures and retries automatically. The data engineer needs to use Python to orchestrate the jobs.\nWhich service will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)"
      },
      {
        "letter": "B",
        "text": "AWS Step Functions"
      },
      {
        "letter": "C",
        "text": "AWS Glue"
      },
      {
        "letter": "D",
        "text": "Amazon EventBridge"
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Eleftheriia",
        "date": "Thu 07 Nov 2024 09:21",
        "comment": "Even though both MWAA and Step functions can be used for managing task failures, MWAA is more suitable since the engineer would like to use python to orchestrate jobs. Usually, Step functions is used for minimal infrastructure management",
        "selected_answer": "A"
      },
      {
        "author": "AWSMM",
        "date": "Thu 01 May 2025 12:09",
        "comment": "B: Step Functions offers a serverless, visually-driven, and well-integrated solution for orchestrating diverse tasks with built-in error handling, making it a a better option.   \n\nMWAA is a powerful option if you are already familiar with or prefer the flexibility of Apache Airflow. However, it comes with the overhead of managing an Airflow environment.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150745-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 191 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 191,
    "question_text": "A data engineer maintains custom Python scripts that perform a data formatting process that many AWS Lambda functions use. When the data engineer needs to modify the Python scripts, the data engineer must manually update all the Lambda functions.\nThe data engineer requires a less manual way to update the Lambda functions.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Store the custom Python scripts in a shared Amazon S3 bucket. Store a pointer to the custom scripts in the execution context object."
      },
      {
        "letter": "B",
        "text": "Package the custom Python scripts into Lambda layers. Apply the Lambda layers to the Lambda functions."
      },
      {
        "letter": "C",
        "text": "Store the custom Python scripts in a shared Amazon S3 bucket. Store a pointer to the customer scripts in environment variables."
      },
      {
        "letter": "D",
        "text": "Assign the same alias to each Lambda function. Call each Lambda function by specifying the function's alias."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:32",
        "comment": "text book example of Lambda Layers ...",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150743-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 192 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 192,
    "question_text": "A company stores customer data in an Amazon S3 bucket. Multiple teams in the company want to use the customer data for downstream analysis. The company needs to ensure that the teams do not have access to personally identifiable information (PII) about the customers.\nWhich solution will meet this requirement with LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Macie to create and run a sensitive data discovery job to detect and remove PII."
      },
      {
        "letter": "B",
        "text": "Use S3 Object Lambda to access the data, and use Amazon Comprehend to detect and remove PII."
      },
      {
        "letter": "C",
        "text": "Use Amazon Data Firehose and Amazon Comprehend to detect and remove PII."
      },
      {
        "letter": "D",
        "text": "Use an AWS Glue DataBrew job to store the PII data in a second S3 bucket. Perform analysis on the data that remains in the original S3 bucket."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AlejandroU",
        "date": "Tue 14 Oct 2025 00:39",
        "comment": "The correct answer is D (Glue DataBrew) because it's:\n- Fully managed with GUI configuration\n- Purpose-built for PII detection/masking\n- Minimal ongoing maintenance\n- True \"least operational overhead\"",
        "selected_answer": "D"
      },
      {
        "author": "annadri",
        "date": "Thu 18 Sep 2025 10:21",
        "comment": "why not D?",
        "selected_answer": "D"
      },
      {
        "author": "praveenu",
        "date": "Sun 18 May 2025 21:30",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-redact-pii.html\n\nWith S3 Object Lambda and a prebuilt AWS Lambda function powered by Amazon Comprehend, you can protect PII data retrieved from S3 before returning it to an application.",
        "selected_answer": "B"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:36",
        "comment": "it is not A, Macie can only detect the PII",
        "selected_answer": "B"
      },
      {
        "author": "WarPig666",
        "date": "Wed 18 Dec 2024 03:22",
        "comment": "A can’t be correct. Macie can discover PII, but not automatically redact it.",
        "selected_answer": "B"
      },
      {
        "author": "paali",
        "date": "Tue 17 Dec 2024 10:24",
        "comment": "Macie will only detect sensitive data, it can't redact it. So, we can use option B\n\nWith S3 Object Lambda and a prebuilt AWS Lambda function powered by Amazon Comprehend, you can protect PII data retrieved from S3 before returning it to an application.",
        "selected_answer": "B"
      },
      {
        "author": "7a1d491",
        "date": "Sun 15 Dec 2024 10:34",
        "comment": "Amazon Macie is a fully managed data security and privacy service that uses machine learning to automatically discover, classify, and protect sensitive data, including personally identifiable information (PII). By running a sensitive data discovery job in Macie, the company can automatically identify PII in the S3 bucket and provide actionable insights to help secure it. The operational overhead is minimized because Macie handles the discovery and classification of PII automatically.",
        "selected_answer": "A"
      },
      {
        "author": "michele_scar",
        "date": "Mon 18 Nov 2024 13:45",
        "comment": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-redact-pii.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150741-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 193 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 193,
    "question_text": "A company stores its processed data in an S3 bucket. The company has a strict data access policy. The company uses IAM roles to grant teams within the company different levels of access to the S3 bucket.\nThe company wants to receive notifications when a user violates the data access policy. Each notification must include the username of the user who violated the policy.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Config rules to detect violations of the data access policy. Set up compliance alarms."
      },
      {
        "letter": "B",
        "text": "Use Amazon CloudWatch metrics to gather object-level metrics. Set up CloudWatch alarms."
      },
      {
        "letter": "C",
        "text": "Use AWS CloudTrail to track object-level events for the S3 bucket. Forward events to Amazon CloudWatch to set up CloudWatch alarms."
      },
      {
        "letter": "D",
        "text": "Use Amazon S3 server access logs to monitor access to the bucket. Forward the access logs to an Amazon CloudWatch log group. Use metric filters on the log group to set up CloudWatch alarms."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AlejandroU",
        "date": "Tue 14 Oct 2025 00:38",
        "comment": "The correct answer is D (Glue DataBrew) because it's:\n- Fully managed with GUI configuration\n- Purpose-built for PII detection/masking\n- Minimal ongoing maintenance\n- True \"least operational overhead\"",
        "selected_answer": "D"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:37",
        "comment": "for monitoring API calls use CloutTrial, it is that simple",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/150740-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 194 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 194,
    "question_text": "A company needs to load customer data that comes from a third party into an Amazon Redshift data warehouse. The company stores order data and product data in the same data warehouse. The company wants to use the combined dataset to identify potential new customers.\nA data engineer notices that one of the fields in the source data includes values that are in JSON format.\nHow should the data engineer load the JSON data into the data warehouse with the LEAST effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the SUPER data type to store the data in the Amazon Redshift table."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue to flatten the JSON data and ingest it into the Amazon Redshift table."
      },
      {
        "letter": "C",
        "text": "Use Amazon S3 to store the JSON data. Use Amazon Athena to query the data."
      },
      {
        "letter": "D",
        "text": "Use an AWS Lambda function to flatten the JSON data. Store the data in Amazon S3."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:39",
        "comment": "The SUPER data type in Amazon Redshift allows you to store semi-structured data such as JSON directly in a Redshift table without the need to flatten or transform the data first.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153160-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 195 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 195,
    "question_text": "A company wants to analyze sales records that the company stores in a MySQL database. The company wants to correlate the records with sales opportunities identified by Salesforce.\nThe company receives 2 GB of sales records every day. The company has 100 GB of identified sales opportunities. A data engineer needs to develop a process that will analyze and correlate sales records and sales opportunities. The process must run once each night.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to fetch both datasets. Use AWS Lambda functions to correlate the datasets. Use AWS Step Functions to orchestrate the process."
      },
      {
        "letter": "B",
        "text": "Use Amazon AppFlow to fetch sales opportunities from Salesforce. Use AWS Glue to fetch sales records from the MySQL database. Correlate the sales records with the sales opportunities. Use Amazon Managed Workflows for Apache Airflow (Amazon MWAA) to orchestrate the process."
      },
      {
        "letter": "C",
        "text": "Use Amazon AppFlow to fetch sales opportunities from Salesforce. Use AWS Glue to fetch sales records from the MySQL database. Correlate the sales records with sales opportunities. Use AWS Step Functions to orchestrate the process."
      },
      {
        "letter": "D",
        "text": "Use Amazon AppFlow to fetch sales opportunities from Salesforce. Use Amazon Kinesis Data Streams to fetch sales records from the MySQL database. Use Amazon Managed Service for Apache Flink to correlate the datasets. Use AWS Step Functions to orchestrate the process."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:42",
        "comment": "App Flow to get the data from Salse Force, Glue for ETL and Step Functions for orchestration, all managed all serverless, LEAST OVERHEAD!",
        "selected_answer": "C"
      },
      {
        "author": "Eleftheriia",
        "date": "Thu 26 Dec 2024 06:46",
        "comment": "I assume Step Functions are more on the data processing side and also with less cost and overhead, but since the question is about workflow orchestration and this is the default definition of MWAA, why should someone select C over B?",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153366-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 196 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 196,
    "question_text": "A company stores server logs in an Amazon S3 bucket. The company needs to keep the logs for 1 year. The logs are not required after 1 year.\nA data engineer needs a solution to automatically delete logs that are older than 1 year.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Define an S3 Lifecycle configuration to delete the logs after 1 year."
      },
      {
        "letter": "B",
        "text": "Create an AWS Lambda function to delete the logs after 1 year."
      },
      {
        "letter": "C",
        "text": "Schedule a cron job on an Amazon EC2 instance to delete the logs after 1 year."
      },
      {
        "letter": "D",
        "text": "Configure an AWS Step Functions state machine to delete the logs after 1 year."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:31",
        "comment": "A: Sí, porque una configuración de ciclo de vida de S3 elimina automáticamente los logs después de 1 año sin intervención manual.\nB: No, requiere código personalizado y mantenimiento extra.\nC: No, implica gestionar una instancia EC2 y cron jobs, lo que añade complejidad.\nD: No, usar Step Functions añade sobrecarga operativa innecesaria para una tarea simple.",
        "selected_answer": "A"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:09",
        "comment": "Amazon S3 provides Lifecycle policies, which allow you to automate the management of objects stored in a bucket. You can configure a rule to automatically delete objects older than a specified age",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153367-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 197 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 197,
    "question_text": "A company is designing a serverless data processing workflow in AWS Step Functions that involves multiple steps. The processing workflow ingests data from an external API, transforms the data by using multiple AWS Lambda functions, and loads the transformed data into Amazon DynamoDB.\nThe company needs the workflow to perform specific steps based on the content of the incoming data.\nWhich Step Functions state type should the company use to meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Parallel"
      },
      {
        "letter": "B",
        "text": "Choice"
      },
      {
        "letter": "C",
        "text": "Task"
      },
      {
        "letter": "D",
        "text": "Map"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:50",
        "comment": "Choice adds conditional logic.  IE, the status of incoming data.",
        "selected_answer": "B"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:17",
        "comment": "if something depends on something before that than it is Choice State.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153159-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 198 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 198,
    "question_text": "A data engineer created a table named cloudtrail_logs in Amazon Athena to query AWS CloudTrail logs and prepare data for audits. The data engineer needs to write a query to display errors with error codes that have occurred since the beginning of 2024. The query must return the 10 most recent errors.\nWhich query will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "select count (*) as TotalEvents, eventname, errorcode, errormessage from cloudtrail_logswhere errorcode is not nulland eventtime >= '2024-01-01T00:00:00Z' group by eventname, errorcode, errormessageorder by TotalEvents desclimit 10;"
      },
      {
        "letter": "B",
        "text": "select count (*) as TotalEvents, eventname, errorcode, errormessage from cloudtrail_logs where eventtime >= '2024-01-01T00:00:00Z' group by eventname, errorcode, errormessage order by TotalEvents desc limit 10;"
      },
      {
        "letter": "C",
        "text": "select count (*) as TotalEvents, eventname, errorcode, errormessage from cloudtrail_logswhere eventtime >= '2024-01-01T00:00:00Z' group by eventname, errorcode, errormessageorder by eventname asc limit 10;"
      },
      {
        "letter": "D",
        "text": "select count (*) as TotalEvents, eventname, errorcode, errormessage from cloudtrail_logs where errorcode is not nulland eventtime >= '2024-01-01T00:00:00Z' group by eventname, errorcode, errormessagelimit 10;"
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AWSMM",
        "date": "Mon 28 Apr 2025 20:29",
        "comment": "select count (*) as TotalEvents, eventname, errorcode, errormessage \nfrom cloudtrail_logs\nwhere errorcode is not null and eventtime >= '2024-01-01T00:00:00Z' \ngroup by eventname, errorcode, errormessage \norder by TotalEvents desc\nlimit 10;",
        "selected_answer": "A"
      },
      {
        "author": "Ramdi1",
        "date": "Sun 16 Mar 2025 14:50",
        "comment": "Why Option A is Correct?\n✅ WHERE errorcode IS NOT NULL → Filters out successful events, keeping only errors.\n✅ AND eventtime >= '2024-01-01T00:00:00Z' → Ensures only logs from 2024 are considered.\n✅ GROUP BY eventname, errorcode, errormessage → Aggregates error occurrences per event.\n✅ ORDER BY TotalEvents DESC → Sorts by the number of occurrences, ensuring the most frequent errors appear first.\n✅ LIMIT 10 → Returns only the 10 most recent errors.",
        "selected_answer": "A"
      },
      {
        "author": "simon2133",
        "date": "Sun 02 Mar 2025 00:57",
        "comment": "A. Same as B but including filter for error code being set",
        "selected_answer": "A"
      },
      {
        "author": "fnuuu",
        "date": "Tue 11 Feb 2025 18:59",
        "comment": "Query in the Option B is correct with 'desc' order",
        "selected_answer": "B"
      },
      {
        "author": "A_E_M",
        "date": "Wed 22 Jan 2025 16:03",
        "comment": "This is not the same, but it shows the important point.  Descending order is the correct answer.  \nSELECT * \nFROM cloudtrail_logs\nWHERE  \n    eventTime >= '2024-01-01' \n    AND errorCode IS NOT NULL \nORDER BY eventTime DESC \nLIMIT 10;",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153157-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 199 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 199,
    "question_text": "An online retailer uses multiple delivery partners to deliver products to customers. The delivery partners send order summaries to the retailer. The retailer stores the order summaries in Amazon S3.\nSome of the order summaries contain personally identifiable information (PII) about customers. A data engineer needs to detect PII in the order summaries so the company can redact the PII.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Amazon Textract"
      },
      {
        "letter": "B",
        "text": "Amazon S3 Storage Lens"
      },
      {
        "letter": "C",
        "text": "Amazon Macie"
      },
      {
        "letter": "D",
        "text": "Amazon SageMaker Data Wrangler"
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:52",
        "comment": "Detection only (no redaction) = Macie",
        "selected_answer": "C"
      },
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:22",
        "comment": "PII in AWS --> Macie",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153158-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 200 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 200,
    "question_text": "A company has an Amazon Redshift data warehouse that users access by using a variety of IAM roles. More than 100 users access the data warehouse every day.\nThe company wants to control user access to the objects based on each user's job role, permissions, and how sensitive the data is.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the role-based access control (RBAC) feature of Amazon Redshift."
      },
      {
        "letter": "B",
        "text": "Use the row-level security (RLS) feature of Amazon Redshift."
      },
      {
        "letter": "C",
        "text": "Use the column-level security (CLS) feature of Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Use dynamic data masking policies in Amazon Redshift."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 10:25",
        "comment": "the only possible answers are A and B but B wouldn't be enough.",
        "selected_answer": "A"
      },
      {
        "author": "7a1d491",
        "date": "Wed 18 Dec 2024 11:57",
        "comment": "Row level or column level is not enough in this case",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153156-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 201 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 201,
    "question_text": "A company uses Amazon DataZone as a data governance and business catalog solution. The company stores data in an Amazon S3 data lake. The company uses AWS Glue with an AWS Glue Data Catalog.\nA data engineer needs to publish AWS Glue Data Quality scores to the Amazon DataZone portal.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a data quality ruleset with Data Quality Definition language (DQDL) rules that apply to a specific AWS Glue table. Schedule the ruleset to run daily. Configure the Amazon DataZone project to have an Amazon Redshift data source. Enable the data quality configuration for the data source."
      },
      {
        "letter": "B",
        "text": "Configure AWS Glue ETL jobs to use an Evaluate Data Quality transform. Define a data quality ruleset inside the jobs. Configure the Amazon DataZone project to have an AWS Glue data source. Enable the data quality configuration for the data source."
      },
      {
        "letter": "C",
        "text": "Create a data quality ruleset with Data Quality Definition language (DQDL) rules that apply to a specific AWS Glue table. Schedule the ruleset to run daily. Configure the Amazon DataZone project to have an AWS Glue data source. Enable the data quality configuration for the data source."
      },
      {
        "letter": "D",
        "text": "Configure AWS Glue ETL jobs to use an Evaluate Data Quality transform. Define a data quality ruleset inside the jobs. Configure the Amazon DataZone project to have an Amazon Redshift data source. Enable the data quality configuration for the data source."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 09:34",
        "comment": "data zone should be configured to work with glue as data source",
        "selected_answer": "C"
      },
      {
        "author": "7a1d491",
        "date": "Wed 18 Dec 2024 11:50",
        "comment": "Glue has to be the data source",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153155-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 202 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 202,
    "question_text": "A company has a data warehouse in Amazon Redshift. To comply with security regulations, the company needs to log and store all user activities and connection activities for the data warehouse.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Amazon S3 bucket. Enable logging for the Amazon Redshift cluster. Specify the S3 bucket in the logging configuration to store the logs."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Elastic File System (Amazon EFS) file system. Enable logging for the Amazon Redshift cluster. Write logs to the EFS file system."
      },
      {
        "letter": "C",
        "text": "Create an Amazon Aurora MySQL database. Enable logging for the Amazon Redshift cluster. Write the logs to a table in the Aurora MySQL database."
      },
      {
        "letter": "D",
        "text": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable logging for the Amazon Redshift cluster. Write the logs to the EBS volume."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:32",
        "comment": "A: Sí, porque Amazon Redshift permite habilitar el registro de actividades y conexiones, y almacenar esos logs automáticamente en un bucket de S3, lo que cumple con las regulaciones y minimiza la sobrecarga operativa.",
        "selected_answer": "A"
      },
      {
        "author": "7a1d491",
        "date": "Wed 18 Dec 2024 11:49",
        "comment": "S3 Bucket to store logs",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/153154-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 203 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 203,
    "question_text": "A company wants to migrate a data warehouse from Teradata to Amazon Redshift.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Database Migration Service (AWS DMS) Schema Conversion to migrate the schema. Use AWS DMS to migrate the data."
      },
      {
        "letter": "B",
        "text": "Use the AWS Schema Conversion Tool (AWS SCT) to migrate the schema. Use AWS Database Migration Service (AWS DMS) to migrate the data."
      },
      {
        "letter": "C",
        "text": "Use AWS Database Migration Service (AWS DMS) to migrate the data. Use automatic schema conversion."
      },
      {
        "letter": "D",
        "text": "Manually export the schema definition from Teradata. Apply the schema to the Amazon Redshift database. Use AWS Database Migration Service (AWS DMS) to migrate the data."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "HagarTheHorrible",
        "date": "Mon 23 Dec 2024 09:37",
        "comment": "B, \nA, seems a lot like it but AWS DMS has limited schema conversion capabilities. It is better paired with AWS SCT for schema migration.",
        "selected_answer": "B"
      },
      {
        "author": "7a1d491",
        "date": "Wed 18 Dec 2024 11:48",
        "comment": "B for least operational overhead",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/152992-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 204 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 204,
    "question_text": "A company uses a variety of AWS and third-party data stores. The company wants to consolidate all the data into a central data warehouse to perform analytics. Users need fast response times for analytics queries.\nThe company uses Amazon QuickSight in direct query mode to visualize the data. Users normally run queries during a few hours each day with unpredictable spikes.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon Redshift Serverless to load all the data into Amazon Redshift managed storage (RMS)."
      },
      {
        "letter": "B",
        "text": "Use Amazon Athena to load all the data into Amazon S3 in Apache Parquet format."
      },
      {
        "letter": "C",
        "text": "Use Amazon Redshift provisioned clusters to load all the data into Amazon Redshift managed storage (RMS)."
      },
      {
        "letter": "D",
        "text": "Use Amazon Aurora PostgreSQL to load all the data into Aurora."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "MerryLew",
        "date": "Wed 15 Jan 2025 19:56",
        "comment": "A so you don't have to bother with manual provisioning.",
        "selected_answer": "A"
      },
      {
        "author": "ceaser221",
        "date": "Sat 11 Jan 2025 20:42",
        "comment": "Redshift Serverless automatically scales resources up or down based on query workload. This eliminates the need for manual capacity provisioning and scaling, significantly reducing operational overhead.",
        "selected_answer": "A"
      },
      {
        "author": "7a1d491",
        "date": "Sun 15 Dec 2024 10:20",
        "comment": "Serverless is for unpredictable loads",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/156778-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 205 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 205,
    "question_text": "A data engineer uses Amazon Kinesis Data Streams to ingest and process records that contain user behavior data from an application every day.\nThe data engineer notices that the data stream is experiencing throttling because hot shards receive much more data than other shards in the data stream.\nHow should the data engineer resolve the throttling issue?",
    "choices": [
      {
        "letter": "A",
        "text": "Use a random partition key to distribute the ingested records."
      },
      {
        "letter": "B",
        "text": "Increase the number of shards in the data stream. Distribute the records across the shards."
      },
      {
        "letter": "C",
        "text": "Limit the number of records that are sent each second by the producer to match the capacity of the stream."
      },
      {
        "letter": "D",
        "text": "Decrease the size of the records that the producer sends to match the capacity of the stream."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "Tani0908",
        "date": "Fri 27 Jun 2025 16:34",
        "comment": "In case of user behavior data the order of the data/events needs to be maintained so we cannot use random partition key as it would scatter data. So B is correct for this scenario",
        "selected_answer": "B"
      },
      {
        "author": "Mitchdu",
        "date": "Mon 16 Jun 2025 16:27",
        "comment": "Option A: Use random partition key to distribute records**\n- ✅ **Addresses root cause**: Random keys ensure even distribution across all shards\n- ✅ **Immediate solution**: No infrastructure changes needed\n- ✅ **Cost-effective**: Uses existing shard capacity efficiently\n- ✅ **Eliminates hot shards**: Random distribution prevents any single shard from being overloaded\n\n**Option B: Increase number of shards + distribute records**\n- ⚠️ **Partial solution**: More shards provide more capacity\n- ❌ **Doesn't fix root cause**: Poor partition key selection will still create hot shards\n- ❌ **Higher cost**: More shards = higher operational costs\n- ❌ **Temporary fix**: Hot shard problem will likely persist",
        "selected_answer": "A"
      },
      {
        "author": "zits88",
        "date": "Wed 11 Jun 2025 20:12",
        "comment": "After reading about this EXTENSIVELY (and not just asking ChatGPT like some of these folks), the actual correct answer is B here.  Option A WOULD indeed resolve the hot shards, but all order and logic to what data goes to which shard would be lost, which is not good in this scenario with user behavior in an application.\n\nAs someone below said, it's true that just \"increasing number of shards\" would NOT rectify the problem. However, the key second sentence about \"distributing\" the data across the shards completes the tasks asked of us in the question. It doesn't specify how we'd do that, but it doesn't matter.\n\nNow, this doesn't guarantee that AWS themselves considers A the actual correct answer -- in all the real-life exams I have taken, I have seen at least one or two questions with NO CORRECT ANSWERS, and many more that seem almost AI-generated.  But in real life data engineering, you don't want to randomize the partition key here.",
        "selected_answer": "B"
      },
      {
        "author": "siheom",
        "date": "Mon 09 Jun 2025 01:04",
        "comment": "VOTE B",
        "selected_answer": "B"
      },
      {
        "author": "Faye15599",
        "date": "Sat 15 Mar 2025 17:33",
        "comment": "A is the best solution because the issue of hot shards is typically caused by an uneven distribution of records across shards due to poorly chosen partition keys. Using a random partition key ensures that records are distributed more evenly across all shards, reducing the likelihood of any single shard becoming \"hot\" and experiencing throttling.\n\nB is incorrect because while increasing the number of shards can help handle more data, it does not resolve the root cause of hot shards, which is uneven distribution due to poor partition key selection. Without addressing the partition key issue, adding shards may still result in some shards being overloaded.",
        "selected_answer": "A"
      },
      {
        "author": "JekChong",
        "date": "Sat 22 Feb 2025 07:59",
        "comment": "Amazon Kinesis Data Streams uses shards to distribute data, and each shard has a fixed throughput limit. If certain shards receive significantly more data than others (hot shards), they will experience throttling. To resolve this issue:\n\nIncrease the number of shards – This increases the overall capacity of the stream.\nDistribute records more evenly across shards – This can be done by modifying the partition key strategy so that data is spread more evenly.",
        "selected_answer": "B"
      },
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:33",
        "comment": "A: Sí, usar una clave de partición aleatoria distribuirá uniformemente los registros entre los shards, reduciendo cuellos de botella en shards \"calientes\".\nB: No, aumentar shards no soluciona la desproporción si la clave sigue causando concentración.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/156779-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 206 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 206,
    "question_text": "A company has a data processing pipeline that includes several dozen steps. The data processing pipeline needs to send alerts in real time when a step fails or succeeds. The data processing pipeline uses a combination of Amazon S3 buckets, AWS Lambda functions, and AWS Step Functions state machines.\nA data engineer needs to create a solution to monitor the entire pipeline.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the Step Functions state machines to store notifications in an Amazon S3 bucket when the state machines finish running. Enable S3 event notifications on the S3 bucket."
      },
      {
        "letter": "B",
        "text": "Configure the AWS Lambda functions to store notifications in an Amazon S3 bucket when the state machines finish running. Enable S3 event notifications on the S3 bucket."
      },
      {
        "letter": "C",
        "text": "Use AWS CloudTrail to send a message to an Amazon Simple Notification Service (Amazon SNS) topic that sends notifications when a state machine fails to run or succeeds to run."
      },
      {
        "letter": "D",
        "text": "Configure an Amazon EventBridge rule to react when the execution status of a state machine changes. Configure the rule to send a message to an Amazon Simple Notification Service (Amazon SNS) topic that sends notifications."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:35",
        "comment": "D: Sí, porque EventBridge puede reaccionar en tiempo real a los cambios de estado de las state machines y enviar notificaciones a través de SNS.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/156780-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 207 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 207,
    "question_text": "A company has an application that uses an Amazon API Gateway REST API and an AWS Lambda function to retrieve data from an Amazon DynamoDB instance. Users recently reported intermittent high latency in the application's response times. A data engineer finds that the Lambda function experiences frequent throttling when the company's other Lambda functions experience increased invocations.\nThe company wants to ensure the API's Lambda function operate without being affected by other Lambda functions.\nWhich solution will meet this requirement MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Increase the number of read capacity unit (RCU) in DynamoDB."
      },
      {
        "letter": "B",
        "text": "Configure provisioned concurrency for the Lambda function."
      },
      {
        "letter": "C",
        "text": "Configure reserved concurrency for the Lambda function."
      },
      {
        "letter": "D",
        "text": "Increase the Lambda function timeout and allocated memory."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "daed09",
        "date": "Tue 11 Mar 2025 14:51",
        "comment": "C: It is more cost-effectively, because we reserve slots for the critical/specific Lambda function (configuration instead of more resources).\n\nWith answer B we increase concurrency allocation, but it will increase costs, additionally if more Lambda functions are created this issue will ocurr again",
        "selected_answer": "C"
      },
      {
        "author": "italiancloud2025",
        "date": "Tue 18 Feb 2025 23:36",
        "comment": "C: Sí, la concurrencia reservada aísla la Lambda del impacto de otras funciones, siendo la opción más rentable.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/304999-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 208 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 208,
    "question_text": "A company has as JSON file that contains personally identifiable information (PII) data and non-PII data. The company needs to make the data available for querying and analysis.\nThe non-PII data must be available to everyone in the company. The PII data must be available only to a limited group of employees.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Store the JSON file in an Amazon S3 bucket. Configure AWS Glue to split the file into one file that contains the PII data and one file that contains the non-PII data. Store the output files in separate S3 buckets. Grant the required access to the buckets based on the type of user."
      },
      {
        "letter": "B",
        "text": "Store the JSON file in an Amazon S3 bucket. Use Amazon Macie to identify PII data and to grant access based on the type of user."
      },
      {
        "letter": "C",
        "text": "Store the JSON file in an Amazon S3 bucket. Catalog the file schema in AWS Lake Formation. Use Lake Formation permissions to provide access to the required data based on the type of user."
      },
      {
        "letter": "D",
        "text": "Create two Amazon RDS PostgreSQL databases. Load the PII data and the non-PII data into the separate databases. Grant access to the databases based on the type of user."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "habros",
        "date": "Tue 17 Jun 2025 04:38",
        "comment": "Lake Formation is best used to control data in data lakes. JSON data will most likely be stored in S3 and DynamoDB, but DynamoDB can store JSON objects but not JSON files. \n\nStoring as S3 = data lake. Data lake = Lake Formation to control access\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/lf-permissions-overview.html",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/304581-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 209 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 209,
    "question_text": "A company uses AWS Key Management Service (AWS KMS) to encrypt an Amazon Redshift cluster. The company wants to configure a cross-Region snapshot of the Redshift cluster as part of disaster recovery (DR) strategy.\nA data engineer needs to use the AWS CLI to create the cross-Region snapshot.\nWhich combination of steps will meet these requirements? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Create a KMS key and configure a snapshot copy grant in the source AWS Region."
      },
      {
        "letter": "B",
        "text": "In the source AWS Region, enable snapshot copying. Specify the name of the snapshot copy grant that is created in the destination AWS Region."
      },
      {
        "letter": "C",
        "text": "In the source AWS Region, enable snapshot copying. Specify the name of the snapshot copy grant that is created in the source AWS Region."
      },
      {
        "letter": "D",
        "text": "Create a KMS key and configure a snapshot copy grant in the destination AWS Region."
      },
      {
        "letter": "E",
        "text": "Convert the cluster to a Multi-AZ deployment."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alexcarran",
        "date": "Thu 16 Oct 2025 20:05",
        "comment": "Create a KMS key and configure a snapshot copy grant in the source AWS Region.\nThis step is required because you need to create a KMS key in the source region to enable encrypted snapshots to be copied to another region. The snapshot copy grant is needed to specify permissions for copying encrypted snapshots.\nCreate a KMS key and configure a snapshot copy grant in the destination AWS Region.\n\nThis step ensures that you have the necessary permissions in the destination region to decrypt the snapshots. When copying encrypted snapshots to a different region, you need a KMS key in the destination region to handle decryption.",
        "selected_answer": "AD"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 03:50",
        "comment": "When using cross-Region snapshot copy for Amazon Redshift clusters encrypted with AWS KMS, you need to:\n\t1.\tCreate a snapshot copy grant in the source Region. This allows Redshift to use your KMS key for encrypting snapshots that will be copied to another Region.\n\t2.\tEnable snapshot copy on the Redshift cluster and specify the snapshot copy grant name.",
        "selected_answer": "AC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/304582-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 210 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 210,
    "question_text": "A company is using Amazon S3 to build a data lake. The company needs to replicate records from multiple source databases into Apache Parquet format.\nMost of the source databases are hosted on Amazon RDS. However, one source database is an on-premises Microsoft SQL Server Enterprise instance. The company needs to implement a solution to replicate existing data from all source databases and all future changes to the target S3 data lake.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use one AWS Glue job to replicate existing data. Use a second AWS Glue job to replicate future changes."
      },
      {
        "letter": "B",
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate existing data. Use AWS Glue jobs to replicate future changes."
      },
      {
        "letter": "C",
        "text": "Use AWS Database Migration Service (AWS DMS) to replicate existing data and future changes."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue jobs to replicate existing data. Use Amazon Kinesis Data Streams to replicate future changes."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AnsonCert",
        "date": "Sat 07 Jun 2025 08:41",
        "comment": "AWS DMS provides full support of replicating records from RDS and on-premises SQL Server in Full Load, CDC, Full Load + CDC. Also, DMS supports converting data to Parquet format and sends to S3 for building data lake. So DMS can provides both replication and CDC, all in a single service.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305563-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 211 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 211,
    "question_text": "A data engineer needs to optimize the performance of a data pipeline that handles retail orders. Data about the orders is ingested daily into an Amazon S3 bucket.\nThe data engineer runs queries once each week to extract metrics from the orders data based the order date for multiple date ranges. The data engineer needs an optimization solution that ensures the query performance will not degrade when the volume of data increases.\nWhich solution will meet this requirement MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Partition the data based on order date. Use Amazon Athena to query the data."
      },
      {
        "letter": "B",
        "text": "Partition the data based on order date. Use Amazon Redshift to query the data."
      },
      {
        "letter": "C",
        "text": "Partition the data based on load date. Use Amazon EMR to query the data."
      },
      {
        "letter": "D",
        "text": "Partition the data based on load date. Use Amazon Aurora to query the data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "ude",
        "date": "Thu 21 Aug 2025 15:00",
        "comment": "Scales automatically as data grows.\n\nCost-efficient (pay per query).\n\nQuery pattern aligned with partition key.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305001-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 212 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 212,
    "question_text": "A data engineer has two datasets that contain sales information for multiple cities and states. One dataset is named reference, and the other dataset is named primary.\nThe data engineer needs a solution to determine whether a specific set of values in the city and state columns of the primary dataset exactly match the same specific values in the reference dataset. The data engineer wants to use Data Quality Definition Language (DQDL) rules in an AWS Glue Data Quality job.\nWhich rule will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "DatasetMatch \"reference” “city->ref_city, state->ref_state” = 1.0"
      },
      {
        "letter": "B",
        "text": "Referentiallntegrity “city,state” “reference.{ref_city,ref_state}” = 1.0"
      },
      {
        "letter": "C",
        "text": "DatasetMatch “reference” “city->ref_city, state->ref_state” = 100"
      },
      {
        "letter": "D",
        "text": "Referentialintegrity “city,state” \"reference.{ref_city,ref_state}” = 100"
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "XP_2600",
        "date": "Tue 01 Jul 2025 15:26",
        "comment": "Seems A to me",
        "selected_answer": "A"
      },
      {
        "author": "habros",
        "date": "Tue 17 Jun 2025 05:04",
        "comment": "exact match = checking referential integrity. can only be between 0 and 1 (decimal notation). 1 meaning strict compliance (must match or fail)\nhttps://aws.amazon.com/blogs/big-data/set-up-advanced-rules-to-validate-quality-of-multiple-datasets-with-aws-glue-data-quality/",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305002-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 213 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 213,
    "question_text": "A company has an on-premises PostgreSQL database that contains customer data. The company wants to migrate the customer data to an Amazon Redshift data warehouse. The company has established a VPN connection between the on-premises database and AWS.\nThe on-premises database is continuously updated. The company must ensure that the data in Amazon Redshift is updated as quickly as possible.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use the pg_dump utility to generate a backup of the PostgreSQL database. Use the AWS Schema Conversion Tool (AWS SCT) to upload the backup to Amazon Redshift. Set up a cron job to perform a backup. Upload the backup to Amazon Redshift every night."
      },
      {
        "letter": "B",
        "text": "Create an AWS Database Migration Service (AWS DMS) full-load task. Set Amazon Redshift as the target. Configure the task to use the change data capture (CDC) feature."
      },
      {
        "letter": "C",
        "text": "Use the pg_dump utility to generate a backup of the PostgreSQL database. Upload the backup to an Amazon S3 bucket. Use the COPY command to import the data into Amazon Redshift."
      },
      {
        "letter": "D",
        "text": "Create an AWS Database Migration Service (AWS DMS) full-load task. Set Amazon Redshift as the target. Configure the task to perform a full load of the database to Amazon Redshift every night."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "habros",
        "date": "Tue 17 Jun 2025 05:07",
        "comment": "less overhead = AWS DMS, since there is s2svpn. DMS to just use the connection directly to psql source db. \n\nredshift can also be used as a target (destination) db https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305564-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 214 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 214,
    "question_text": "A company has several new datasets in CSV and JSON formats. A data engineer needs to make the data available to a team of data analysts who will analyze the data by using SQL queries.\nWhich solution will meet these requirements in the MOST cost-effective way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Amazon RDS MySQL cluster. Use AWS Glue to transform and load the CSV and JSON files into database tables. Provide the data analysts access to the MySQL cluster."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue DataBrew project that contains the new data. Make the DataBrew project available to the data analysts."
      },
      {
        "letter": "C",
        "text": "Store the data in an Amazon S3 bucket. Use an AWS Glue crawler to catalog the S3 bucket as tables. Create an Amazon Athena workgroup that has a data usage threshold. Grant the data analysts access to the Athena workgroup."
      },
      {
        "letter": "D",
        "text": "Load the data into Super-fast, Parallel, In-memory Calculation Engine (SPICE) in Amazon QuickSight. Allow the data analysts to create analyses and dashboards in QuickSight."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "ude",
        "date": "Thu 21 Aug 2025 15:05",
        "comment": "Cost-efficient\n\nSupports ad-hoc SQL queries on CSV/JSON\n\nServerless and scalable",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305003-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 215 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 215,
    "question_text": "A retail company stores order information in an Amazon Aurora table named Orders. The company needs to create operational reports from the Orders table with minimal latency. The Orders table contains billions of rows, and over 100,000 transactions can occur each second.\nA marketing team needs to join the Orders data with an Amazon Redshift table named Campaigns in the marketing team's data warehouse. The operational Aurora database must not be affected.\nWhich solution will meet these requirements with the LEAST operational effort?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Database Migration Service (AWS DMS) Serverless to replicate the Orders table to Amazon Redshift. Create a materialized view in Amazon Redshift to join with the Campaigns table."
      },
      {
        "letter": "B",
        "text": "Use the Aurora zero-ETL integration with Amazon Redshift to replicate the Orders table. Create a materialized view in Amazon Redshift to join with the Campaigns table."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue to replicate the Orders table to Amazon Redshift. Create a materialized view in Amazon Redshift to join with the Campaigns table."
      },
      {
        "letter": "D",
        "text": "Use federated queries to query the Orders table directly from Aurora. Create a materialized view in Amazon Redshift to join with the Campaigns table."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "habros",
        "date": "Tue 17 Jun 2025 05:13",
        "comment": "Aurora got built in zero-ETL integration to Redshift\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/zero-etl.html",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305565-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 216 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 216,
    "question_text": "A company is building a new application that ingests CSV files into Amazon Redshift. The company has developed the frontend for the application.\nThe files are stored in an Amazon S3 bucket. Files are no larger than 5 MB.\nA data engineer is developing the extract, transform, and load (ETL) pipeline for the CSV files. The data engineer configured a Redshift cluster and an AWS Lambda function that copies the data out of the files into the Redshift cluster.\nWhich additional steps should the data engineer perform to meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the bucket to send S3 event notifications to Amazon EventBridge. Configure an EventBridge rule that matches S3 new object created events. Set the Lambda function as the target."
      },
      {
        "letter": "B",
        "text": "Configure the $3 bucket to send S3 event notifications to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to process the queue."
      },
      {
        "letter": "C",
        "text": "Configure AWS Database Migration Service (AWS DMS) to stream new S3 objects to a data stream in Amazon Kinesis Data Streams. Set the Lambda function as the target of the data stream."
      },
      {
        "letter": "D",
        "text": "Configure an Amazon EventBridge rule that matches S3 new object created events. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target of the rule. Configure the Lambda function to process the queue."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AminTriton",
        "date": "Sun 31 Aug 2025 12:31",
        "comment": "Why not A? you don’t “configure the bucket to send to EventBridge.” S3 events show up in EventBridge without bucket notification config; adding both is unnecessary, and you can target Lambda directly from EventBridge anyway.",
        "selected_answer": "B"
      },
      {
        "author": "ude",
        "date": "Thu 21 Aug 2025 15:19",
        "comment": "Reliable, scalable, and cost-effective.\n\nHandles concurrent file arrivals and retries automatically.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305566-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 217 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 217,
    "question_text": "A company stores sensitive data in an Amazon Redshift table. The company needs to give specific users the ability to access the sensitive data. The company must not create duplication in the data.\nCustomer support users must be able to see the last four characters of the sensitive data. Audit users must be able to see the full value of the sensitive data. No other users can have the ability to access the sensitive information.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a dynamic data masking policy to allow access based on each user role. Create IAM roles that have specific access permissions. Attach the masking policy to the column that contains sensitive data."
      },
      {
        "letter": "B",
        "text": "Enable metadata security on the Redshift cluster. Create IAM users and IAM roles for the customer support users and the audit users. Grant the IAM users and IAM roles permissions to view the metadata in the Redshift cluster."
      },
      {
        "letter": "C",
        "text": "Create a row-level security policy to allow access based on each user role. Create IAM roles that have specific access permissions. Attach the security policy to the table."
      },
      {
        "letter": "D",
        "text": "Create an AWS Glue job to redact the sensitive data and to load the data into a new Redshift table."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alazar",
        "date": "Thu 16 Oct 2025 22:52",
        "comment": "Dynamic Data Masking (DDM) in Amazon Redshift is specifically designed for this exact use case:",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305567-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 218 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 218,
    "question_text": "A data engineer uses AWS Lake Formation to manage access to data that is stored in an Amazon S3 bucket. The data engineer configures an AWS Glue crawler to discover data at a specific file location in the bucket, s3://examplepath. The crawler execution fails with the following error: “The S3 location: s3://examplepath is not registered.”\nThe data engineer needs to resolve the error.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Attach an appropriate IAM policy to the IAM role of the AWS Glue crawler to grant the crawler permission to read the S3 location."
      },
      {
        "letter": "B",
        "text": "Register the S3 location in Lake Formation to allow the crawler to access the data."
      },
      {
        "letter": "C",
        "text": "Create a new AWS Glue database. Assign the correct permissions to the database for the crawler."
      },
      {
        "letter": "D",
        "text": "Configure the S3 bucket policy to allow cross-account access."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alazar",
        "date": "Thu 16 Oct 2025 22:57",
        "comment": "If an AWS Glue crawler attempts to access a data location that has not been registered, Lake Formation will block the request, resulting in the error \"The S3 location: s3://examplepath is not registered\".",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/304583-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 219 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 219,
    "question_text": "A company built a data lake and a data warehouse on AWS. The company wants to implement a data catalog to enhance the current data storage solutions. The company wants to have the capability to add business metadata and glossary information to the data catalog for every asset.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue Catalog. Create a user table for the business glossary. Use the AWS Glue API to change table properties to add business metadata. Create a web application to access the metadata."
      },
      {
        "letter": "B",
        "text": "Use an Apache Hive metastore. Create a user table for the business glossary. Use the ALTER TABLE command to change table properties to add business metadata. Create a web application to access the metadata."
      },
      {
        "letter": "C",
        "text": "Use Amazon DataZone. Create the business glossaries. Create metadata forms. Use the Amazon DataZone data portal to access the metadata."
      },
      {
        "letter": "D",
        "text": "Use Amazon OpenSearch Service. Create an index for the business glossary. Create a second index for the business metadata. Use the OpenSearch Service dashboard to access the metadata."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alazar",
        "date": "Thu 16 Oct 2025 23:04",
        "comment": "Amazon DataZone is a managed data governance and data-catalog product that natively supports business glossaries, metadata forms, asset catalogs, policy controls, and a data portal.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305095-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 220 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 220,
    "question_text": "A data engineer is using an AWS Glue ETL job to remove outdated customer records from a table that contains customer account information. The data engineer is using the following SQL command to remove customers that exist in a table named monthly_accounts_update table from the customer accounts table:\nMERGE INTO accounts t USING monthly_accounts_update s\nON t.customer = s.customer -\nWHEN MATCHED -\nTHEN DELETE -\nWhat will happen when the data engineer runs the SQL command?",
    "choices": [
      {
        "letter": "A",
        "text": "All customer records that exist in both the customer accounts table and the monthly_accounts_update table will be deleted from the accounts table."
      },
      {
        "letter": "B",
        "text": "Only customer records that are present in both tables will be retained in the customer accounts table."
      },
      {
        "letter": "C",
        "text": "The monthly_accounts_update table will be deleted."
      },
      {
        "letter": "D",
        "text": "No records will be deleted because the command syntax is not valid in AWS Glue."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rebasheer",
        "date": "Fri 22 Aug 2025 11:40",
        "comment": "answer is D\n\nThe MERGE INTO statement is a powerful SQL command used for performing an \"upsert\" operation—a combination of INSERT and UPDATE. Some SQL dialects also support DELETE within a MERGE statement. However, the syntax provided is not valid for any standard SQL dialect, and specifically, AWS Glue's Spark SQL does not natively support the MERGE INTO ... DELETE action in this form.",
        "selected_answer": "D"
      },
      {
        "author": "XP_2600",
        "date": "Tue 01 Jul 2025 16:15",
        "comment": "Seems i was mistaken or there is some typos in post, i think its A",
        "selected_answer": "A"
      },
      {
        "author": "XP_2600",
        "date": "Wed 18 Jun 2025 16:01",
        "comment": "Syntax is not valid glue syntax",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305568-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 221 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 221,
    "question_text": "A company receives marketing campaign data from a vendor. The company ingests the data into an Amazon S3 bucket every 40 to 60 minutes. The data is in CSV format. File sizes are between 100 KB and 300 KB.\nA data engineer needs to set-up an extract, transform, and load (ETL) pipeline to upload the content of each file to Amazon Redshift.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Lambda function that connects to Amazon Redshift and runs a COPY command. Use Amazon EventBridge to invoke the Lambda function based on an Amazon S3 upload trigger."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Data Firehose stream. Configure the stream to use an AWS Lambda function as a source to pull data from the S3 bucket. Set Amazon Redshift as the destination."
      },
      {
        "letter": "C",
        "text": "Use Amazon Redshift Spectrum to query the S3 bucket. Configure an AWS Glue Crawler for the S3 bucket to update metadata in an AWS Glue Data Catalog."
      },
      {
        "letter": "D",
        "text": "Creates an AWS Database Migration Service (AWS DMS) task. Specify an appropriate data schema to migrate. Specify the appropriate type of migration to use."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Sun 31 Aug 2025 17:23",
        "comment": "EventBridge can automatically trigger the Lambda function whenever a new file lands in S3, eliminating manual intervention.\nThe COPY command is optimized for loading data from S3 into Redshift, especially for CSV files.\nIt is simple and tailored for periodic batch loads.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305569-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 222 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 222,
    "question_text": "A company wants to build a dimension table in an Amazon S3 bucket. The bucket contains historical data that includes 10 million records. The historical data is 1 TB in size.\nA data engineer needs a solution to update changes for up to 10,000 records in the base table every day.\nWhich solution will meet this requirement with the LOWEST runtime?",
    "choices": [
      {
        "letter": "A",
        "text": "Develop an Apache Spark job in Amazon EMR to read the historical data and the new changes into two Spark DataFrames. Use the Spark update method to update the base table."
      },
      {
        "letter": "B",
        "text": "Develop an AWS Glue Python job to read the historical data and new changes into two Pandas DataFrames. Use the Pandas update method to update the base table."
      },
      {
        "letter": "C",
        "text": "Develop an AWS Glue Apache Spark job to read the historical data and new changes into two Spark DataFrames. Use the Spark update method to update the base table."
      },
      {
        "letter": "D",
        "text": "Develop an Amazon EMR job to read new changes into Apache Spark DataFrames. Use the Apache Hudi framework to create the base table in Amazon S3. Use the Spark update method to update the base table."
      }
    ],
    "correct_answer": "D",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Sun 31 Aug 2025 17:55",
        "comment": "Apache Hudi is specifically designed for managing large datasets on cloud storage like Amazon S3, with support for incremental updates and record-level inserts, updates, and deletes.\n\nIt enables efficient upserts (update + insert) without needing to rewrite the entire dataset, which is crucial when updating only 10,000 records out of 10 million.\n\nAmazon EMR provides a scalable and cost-effective environment for running Spark jobs, and it integrates well with Hudi.",
        "selected_answer": "D"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305570-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 223 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 223,
    "question_text": "A data engineer develops an AWS Glue Apache Spark ETL job to perform transformations on a dataset. When the data engineer runs the job, the job returns an error that reads, “No space left on device.”\nThe data engineer needs to identify the source of the error and provide a solution.\nWhich combinations of steps will meet this requirement MOST cost-effectively? (Choose two.)",
    "choices": [
      {
        "letter": "A",
        "text": "Scale out the workers vertically to address data skewness."
      },
      {
        "letter": "B",
        "text": "Use the Spark UI and AWS Glue metrics to monitor data skew in the Spark executors."
      },
      {
        "letter": "C",
        "text": "Scale out the number of workers horizontally to address data skewness."
      },
      {
        "letter": "D",
        "text": "Enable the --write-shuffie-files-to-s3 job parameter. Use the salting technique."
      },
      {
        "letter": "E",
        "text": "Use error logs in Amazon CloudWatch to monitor data skew."
      }
    ],
    "correct_answer": "BD",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alazar",
        "date": "Sat 18 Oct 2025 16:00",
        "comment": "--write-shuffle-files-to-s3 offloads shuffle/temp files to S3 instead of filling local disk, avoiding “No space left on device” without increasing instance size. This is cheap because it uses S3 for spill storage and reduces need for larger/more expensive nodes.",
        "selected_answer": "BD"
      },
      {
        "author": "AminTriton",
        "date": "Mon 01 Sep 2025 14:41",
        "comment": "•\tB. Use the Spark UI and AWS Glue metrics to spot data skew and large shuffles that exhaust local disk.\n\t•\tD. Enable --write-shuffle-files-to-s3 and use salting to mitigate skew—this offloads shuffle spill from local disks to S3 and reduces the oversized partition problem.\n\nScaling workers up or out (A/C) increases cost and doesn’t address the root skew issue; CloudWatch error logs (E) won’t pinpoint skew as clearly as the Spark UI/metrics.",
        "selected_answer": "BD"
      },
      {
        "author": "rebasheer",
        "date": "Fri 22 Aug 2025 12:05",
        "comment": "answer BC \nB. Use the Spark UI and AWS Glue metrics to monitor data skew in the Spark executors.\nMonitoring data skew in the Spark executors can help identify if certain partitions are unevenly distributed, which could lead to the \"No space left on device\" error.\n\nC. Scale out the number of workers horizontally to address data skewness.\nScaling out horizontally by adding more workers can help distribute the workload and alleviate data skew issues, potentially resolving the error related to insufficient space on the device.\n\nBy monitoring data skew and scaling out horizontally, the data engineer can effectively address the issue of data skewness causing the \"No space left on device\" error in a cost-effective manner.",
        "selected_answer": "BC"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/311801-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 224 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 224,
    "question_text": "A company has a data pipeline that uses an Amazon RDS instance, AWS Glue jobs, and an Amazon S3 bucket. The RDS instance and AWS Glue jobs run in a private subnet of a VPC and in the same security group.\nA user made a change to the security group that prevents the AWS Glue jobs from connecting to the RDS instance. After the change, the security group contains a single rule that allows inbound SSH traffic from a specific IP address.\nThe company must resolve the connectivity issue.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Add an inbound rule that allows all TCP traffic on all TCP ports. Set the security group as the source."
      },
      {
        "letter": "B",
        "text": "Add an inbound rule that allows all TCP traffic on all UDP ports. Set the private IP address of the RDS instance as the source."
      },
      {
        "letter": "C",
        "text": "Add an inbound rule that allows all TCP traffic on all TCP ports. Set the DNS name of the RDS instance as the source."
      },
      {
        "letter": "D",
        "text": "Replace the source of the existing SSH rule with the private IP address of the RDS instance. Create an outbound rule with the same source, destination, and protocol as the inbound SSH rule."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "alazar",
        "date": "Sat 18 Oct 2025 16:17",
        "comment": "When AWS Glue jobs and an Amazon RDS instance are in the same VPC and security group, they communicate with each other using private IP addresses. For this to work, the security group must have an inbound rule that allows traffic from itself.",
        "selected_answer": "A"
      },
      {
        "author": "llk",
        "date": "Sun 31 Aug 2025 22:55",
        "comment": "The current rule only allows inbound SSH from a specific IP, which doesn’t help Glue connect to RDS.\n\nBy adding a rule that allows all TCP traffic from the same security group, you enable Glue jobs to connect to RDS over the necessary ports (typically port 3306 for MySQL, for example).",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/304584-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 225 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 225,
    "question_text": "A company builds a new data pipeline to process data for business intelligence reports. Users have noticed that data is missing from the reports.\nA data engineer needs to add a data quality check for columns that contain null values and for referential integrity at a stage before the data is added to storage.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Use Amazon SageMaker Data Wrangler to create a Data Quality and Insights report."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue ETL jobs to perform a data quality evaluation transform on the data. Use an IsComplete rule on the requested columns. Use a ReferentialItegrity rule for each join."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue ETL jobs to perform a SQL transform on the data to determine whether requested column contain null values. Use a second SQL transform to check referential integrity."
      },
      {
        "letter": "D",
        "text": "Use Amazon SageMaker Data Wrangler and a custom Python transform to create custom rules to check for null values and referential integrity."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "AnsonCert",
        "date": "Sat 07 Jun 2025 09:49",
        "comment": "Glue build-in Data Quality Definition Language (DQDL) supports:\nIsComplete to check columns are non-null or not,\nReferentialIntegrity to check columns' existance in another data table (Join relationship check)\nhttps://aws.amazon.com/glue/faqs/#topic-3\nGlue's Data Quality transform is serverless and supprt integration with Spark/ETL workflows - Least operational overhead.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305571-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 226 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 226,
    "question_text": "A company is setting up a data pipeline in AWS. The pipeline extracts client data from Amazon S3 buckets, performs quality checks, and transforms the data. The pipeline stores the processed data in a relational database. The company will use the processed data for future queries.\nWhich solution will meet these requirements MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use AWS Glue ETL to extract the data from the S3 buckets and perform the transformations. Use AWS Glue Data Quality to enforce suggested quality rules. Load the data and the quality check results into an Amazon RDS for MySQL instance."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue Studio to extract the data from the S3 buckets. Use AWS Glue DataBrew to perform the transformations and quality checks. Load the processed data into an Amazon RDS for MySQL instance. Load the quality check results into a new S3 bucket."
      },
      {
        "letter": "C",
        "text": "Use AWS Glue ETL to extract the data from the S3 buckets and perform the transformations. Use AWS Glue DataBrew to perform quality checks. Load the processed data and the quality check results into a new S3 bucket."
      },
      {
        "letter": "D",
        "text": "Use AWS Glue Studio to extract the data from the S3 buckets. Use AWS Glue DataBrew to perform the transformations and quality checks. Load the processed data and quality check results into an Amazon RDS for MySQL instance."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:43",
        "comment": "AWS Glue ETL provides a serverless, scalable, and cost-effective way to extract and transform data from S3.\n\t•\tAWS Glue Data Quality is natively integrated into Glue ETL and is designed for automated, rule-based data validation, with low operational overhead.\n\t•\tAmazon RDS for MySQL is a cost-effective managed relational database suitable for structured data and querying.\n\t•\tThis option keeps all steps within a unified serverless environment, minimizing costs and complexity.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305573-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 227 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 227,
    "question_text": "A company uses Amazon Redshift as a data warehouse solution. One of the datasets that the company stores in Amazon Redshift contains data for a vendor.\nRecently, the vendor asked the company to transfer the vendor’s data into the vendor’s Amazon S3 bucket once each week.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an AWS Lambda function to connect to the Redshift data warehouse. Configure the Lambda function to use the Redshift COPY command to copy the required data to the vendor’s S3 bucket on a schedule."
      },
      {
        "letter": "B",
        "text": "Create an AWS Glue job to connect to the Redshift data warehouse. Configure the AWS Glue job to use the Redshift UNLOAD command to load the required data to the vendor’s S3 bucket on a schedule."
      },
      {
        "letter": "C",
        "text": "Use the Amazon Redshift data sharing feature. Set the vendor’s S3 bucket as the destination. Configure the source to be as a custom SQL query that selects the required data."
      },
      {
        "letter": "D",
        "text": "Configure Amazon Redshift Spectrum to use the vendor’s S3 bucket a destination, Enable data querying in both directions."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:45",
        "comment": "Redshift’s UNLOAD command is used to export data from Redshift to Amazon S3 in an efficient, parallelized manner.\n\t•\tAWS Glue can be scheduled and orchestrated to automate the weekly data export.\n\t•\tThis solution allows precise control of which data to extract and where to deliver it — in this case, the vendor’s S3 bucket.\n\t•\tYou can securely configure cross-account access if the vendor’s S3 bucket is in another AWS account.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305574-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 228 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 228,
    "question_text": "A company uses an Amazon Redshift cluster as a data warehouse that is shared across two departments. To comply with a security policy, each department must have unique access permissions.\nDepartment A must have access to tables and views for Department A. Department B must have access to tables and views for Department B.\nThe company often runs SQL queries that use objects from both departments in one query.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Group tables and views for each department into dedicated schemas. Manage permissions at the schema level."
      },
      {
        "letter": "B",
        "text": "Group tables and views for each department into dedicated databases. Manage permissions at the database level."
      },
      {
        "letter": "C",
        "text": "Update the names of the tables and views to follow a naming convention that contains the department names. Manage permissions based on the new naming convention."
      },
      {
        "letter": "D",
        "text": "Create an IAM user group for each department. Use identity-based IAM policies to grant table and view permissions based on the IAM user group."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:47",
        "comment": "Schemas in Amazon Redshift are logical groupings of database objects (tables, views, etc.) within the same database.\n\t•\tUsing separate schemas for each department (e.g., schema_a, schema_b) allows granular access control via GRANT and REVOKE statements.\n\t•\tCross-schema queries are supported in Redshift, so joining tables across departments is straightforward with fully qualified names (e.g., schema_a.table1 JOIN schema_b.table2).\n\t•\tThis approach keeps everything within the same Redshift database, minimizing operational overhead and maximizing flexibility.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305426-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 229 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 229,
    "question_text": "A company wants to ingest streaming data into an Amazon Redshift data warehouse from an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. A data engineer needs to develop a solution that provides low data access time and that optimizes storage costs.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an external schema that maps to the MSK cluster. Create a materialized view that references the external schema to consume the streaming data from the MSK topic."
      },
      {
        "letter": "B",
        "text": "Develop an AWS Glue streaming extract, transform, and load (ETL) job to process the incoming data from Amazon MSK. Load the data into Amazon S3. Use Amazon Redshift Spectrum to read the data from Amazon S3."
      },
      {
        "letter": "C",
        "text": "Create an external schema that maps to the streaming data source. Create a new Amazon Redshift table that references the external schema."
      },
      {
        "letter": "D",
        "text": "Create an Amazon S3 bucket. Ingest the data from Amazon MSK. Create an event-driven AWS Lambda function to load the data from the S3 bucket to a new Amazon Redshift table."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Sat 30 Aug 2025 23:21",
        "comment": "Amazon Redshift Streaming Ingestion allows you to directly query streaming data from sources like Amazon MSK using materialized views, which automatically refresh and keep data up to date.\n\nThis approach avoids intermediate storage (like S3), reducing latency and storage costs.\n\nIt’s low-maintenance: no need to manage ETL pipelines or orchestrate jobs—just define the schema and view.\n\nB. AWS Glue + S3 + Spectrum: Adds multiple layers (ETL, storage, external query), increasing latency and operational overhead.\n\nC. External schema + Redshift table: Redshift tables don’t directly reference external schemas; you’d still need a materialized view or ETL.\n\nD. S3 + Lambda: Event-driven Lambda functions require custom code and orchestration, which increases complexity.",
        "selected_answer": "A"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:50",
        "comment": "AWS Glue streaming ETL supports reading from Amazon MSK, applying transformations, and writing the result to Amazon S3 in near-real time.\n\t•\tAmazon Redshift Spectrum allows querying data directly from S3 without loading it into Redshift storage, which helps optimize storage costs.\n\t•\tThis solution:\n\t•\tProvides low-latency data access via Spectrum.\n\t•\tAvoids duplicating or materializing unnecessary data in Redshift.\n\t•\tHas low operational overhead because Glue streaming jobs are managed, scalable, and require minimal infrastructure maintenance.",
        "selected_answer": "B"
      },
      {
        "author": "XP_2600",
        "date": "Tue 01 Jul 2025 18:10",
        "comment": "Seems B",
        "selected_answer": "B"
      },
      {
        "author": "Tani0908",
        "date": "Fri 27 Jun 2025 19:53",
        "comment": "Streaming data can be directly ingested from Amazon MSK to Redshift using external schemas and materialized views, there is no need for etl process or s3 in between",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305575-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 230 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 230,
    "question_text": "A sales company uses AWS Glue ETL to collect, process, and ingest data into an Amazon S3 bucket. The AWS Glue pipeline creates a new file in the S3 bucket every hour. File sizes vary from 200 KB to 300 KB. The company wants to build a sales prediction model by using data from the previous 5 years. The historic data includes 44,000 files.\nThe company builds a second AWS Glue ETL pipeline by using the smallest worker type. The second pipeline retrieves the historic files from the S3 bucket and processes the files for downstream analysis. The company notices significant performance issues with the second ETL pipeline.\nThe company needs to improve the performance of the second pipeline.\nWhich solution will meet this requirement MOST cost-effectively?",
    "choices": [
      {
        "letter": "A",
        "text": "Use a larger worker type."
      },
      {
        "letter": "B",
        "text": "Increase the number of workers in the AWS Glue ETL jobs."
      },
      {
        "letter": "C",
        "text": "Use the AWS Glue DynamicFrame grouping option."
      },
      {
        "letter": "D",
        "text": "Enable AWS Glue auto scaling."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:52",
        "comment": "AWS Glue DynamicFrame grouping allows you to group multiple small files into larger partitions in-memory before processing.\n\t•\tWhen processing tens of thousands of small files (as in this case with 44,000 files), grouping improves performance dramatically by reducing I/O overhead and optimizing Spark shuffle operations.\n\t•\tThis solution does not require increasing costs (no larger worker types or scaling), so it is the most cost-effective approach.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305576-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 231 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 231,
    "question_text": "A company wants to combine data from multiple software as a service (SaaS) applications for analysis.\nA data engineering team needs to use Amazon QuickSight to perform the analysis and build dashboards. A data engineer needs to extract the data from the SaaS applications and make the data available for QuickSight queries.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create AWS Lambda functions that call the required APIs to extract the data from the applications. Store the data in an Amazon S3 bucket. Use AWS Glue to catalog the data in the S3 bucket. Create a data source and a dataset in QuickSight."
      },
      {
        "letter": "B",
        "text": "Use AWS Lambda functions as Amazon Athena data source connectors to run federated queries against the SaaS applications. Create an Athena data source and a dataset in QuickSight."
      },
      {
        "letter": "C",
        "text": "Use Amazon AppFlow to create a flow for each SaaS application. Set an Amazon S3 bucket as the destination. Schedule the flows to extract the data to the bucket. Use AWS Glue to catalog the data in the S3 bucket. Create a data source and a dataset in QuickSight."
      },
      {
        "letter": "D",
        "text": "Export data the from the SaaS applications as Microsoft Excel files. Create a data source and a dataset in QuickSight by uploading the Excel files."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:54",
        "comment": "Amazon AppFlow is a fully managed service designed specifically to connect with SaaS applications (e.g., Salesforce, Zendesk, Google Analytics) and move data into AWS services like Amazon S3.\n\t•\tIt supports scheduled flows, incremental updates, and transformation options out of the box.\n\t•\tThis option minimizes operational effort: no custom code, no manual API integrations, and no infrastructure management.\n\t•\tOnce the data is in S3 and cataloged with AWS Glue, it can be easily queried in Amazon QuickSight.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305577-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 232 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 232,
    "question_text": "A company runs multiple applications on AWS. The company configured each application to output logs. The company wants to query and visualize the application logs in near real time.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the applications to output logs to Amazon CloudWatch Logs log groups. Create an Amazon S3 bucket. Create an AWS Lambda function that runs on a schedule to export the required log groups to the S3 bucket. Use Amazon Athena to query the log data in the S3 bucket."
      },
      {
        "letter": "B",
        "text": "Create an Amazon OpenSearch Service domain. Configure the applications to output logs to Amazon CloudWatch Logs log groups. Create an OpenSearch Service subscription filter for each log group to stream the data to OpenSearch. Create the required queries and dashboards in OpenSearch Service to analyze and visualize the data."
      },
      {
        "letter": "C",
        "text": "Configure the applications to output logs to Amazon CloudWatch Logs log groups. Use CloudWatch log anomaly detection to query and visualize the log data."
      },
      {
        "letter": "D",
        "text": "Update the application code to send the log data to Amazon QuickSight by using Super-fast, Parallel, In-memory Calculation Engine (SPICE). Create the required analyses and dashboards in QuickSight."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:56",
        "comment": "Amazon OpenSearch Service is designed for log ingestion, indexing, searching, and visualization in near real time.\n\t•\tCloudWatch Logs subscription filters can stream log data directly to OpenSearch with minimal latency.\n\t•\tOpenSearch includes Kibana-based dashboards, enabling real-time visualization and exploration of logs.\n\t•\tThis setup is scalable, serverless, and meets both querying and visualization needs with low latency.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305578-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 233 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 233,
    "question_text": "An ecommerce company processes millions of orders each day. The company uses AWS Glue ETL to collect data from multiple sources, clean the data, and store the data in an Amazon S3 bucket in CSV format by using the S3 Standard storage class. The company uses the stored data to conduct daily analysis.\nThe company wants to optimize costs for data storage and retrieval.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Transition the data to Amazon S3 Glacier Flexible Retrieval."
      },
      {
        "letter": "B",
        "text": "Transition the data from Amazon S3 to an Amazon Aurora cluster."
      },
      {
        "letter": "C",
        "text": "Configure AWS Glue ETL to transform the incoming data to Apache Parquet format."
      },
      {
        "letter": "D",
        "text": "Configure AWS Glue ETL to use Amazon EMR to process incoming data in parallel."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:57",
        "comment": "Apache Parquet is a columnar storage format that is highly efficient for analytics workloads:\n\t•\tReduces storage costs because it compresses data better than CSV.\n\t•\tImproves query performance by reading only relevant columns.\n\t•\tSince the company runs daily analytics, using a format optimized for analytical queries (like Parquet) significantly reduces I/O and compute costs.\n\t•\tIt works seamlessly with services like Athena, Redshift Spectrum, and EMR.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305579-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 234 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 234,
    "question_text": "A data engineer is optimizing query performance in Amazon Athena notebooks that use Apache Spark to analyze large datasets that are stored in Amazon S3. The data is partitioned.\nAn AWS Glue crawler updates the partitions.\nThe data engineer wants to minimize the amount of data that is scanned to improve efficiency of Athena queries.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Apply partition filters in the queries."
      },
      {
        "letter": "B",
        "text": "Increase the frequency of AWS Glue crawler invocations to update the data catalog more often."
      },
      {
        "letter": "C",
        "text": "Organize the data that is in Amazon S3 by using a nested directory structure."
      },
      {
        "letter": "D",
        "text": "Configure Spark to use in-memory caching for frequently accessed data."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Fri 29 Aug 2025 04:04",
        "comment": "When you apply filters that match partition keys (e.g., WHERE year = '2023'), Athena scans only the relevant partitions instead of the entire dataset.\n\nThis dramatically reduces the amount of data scanned, which improves performance and lowers cost.",
        "selected_answer": "A"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 04:59",
        "comment": "Partitions in query",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305580-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 235 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 235,
    "question_text": "A company manages an Amazon Redshift data warehouse. The data warehouse is in a public subnet inside a custom VPC. A security group allows only traffic from within itself. An ACL is open to all traffic.\nThe company wants to generate several visualizations in Amazon QuickSight for an upcoming sales event. The company will run QuickSight Enterprise edition in a second AWS account inside a public subnet within a second custom VPC. The new public subnet has a security group that allows outbound traffic to the existing Redshift cluster.\nA data engineer needs to establish connections between Amazon Redshift and QuickSight. QuickSight must refresh dashboards by querying the Redshift cluster.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Configure the Redshift security group to allow inbound traffic on the Redshift port from the QuickSight security group."
      },
      {
        "letter": "B",
        "text": "Assign Elastic IP addresses to the QuickSight visualizations. Configure the QuickSight security group to allow inbound traffic on the Redshift port from the Elastic IP addresses."
      },
      {
        "letter": "C",
        "text": "Confirm that the CIDR ranges of the Redshift VPC and the QuickSight VPC are the same. If CIDR ranges are different, reconfigure one CIDR range to match the other. Establish network peering between the VPCs."
      },
      {
        "letter": "D",
        "text": "Create a QuickSight gateway endpoint in the Redshift VPC. Attach an endpoint policy to the gateway endpoint to ensure only specific QuickSight accounts can use the endpoint."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Fri 29 Aug 2025 04:20",
        "comment": "Security group referencing allows traffic between resources in different accounts or VPCs, as long as the security groups are configured to allow it.\n\nBy allowing inbound traffic from the QuickSight security group to the Redshift port (default is 5439), you enable QuickSight to connect securely to Redshift.\n\nThis approach is simple, secure, and scalable, especially when both resources are in public subnets.",
        "selected_answer": "A"
      },
      {
        "author": "rssrss",
        "date": "Thu 21 Aug 2025 18:14",
        "comment": "Option C is the correct solution because it establishes a secure and functional networking path between the two VPCs, enabling QuickSight to query the Redshift cluster. This approach ensures that the requirements for visualisation and dashboard refresh are met effectively.",
        "selected_answer": "C"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 05:01",
        "comment": "The Redshift cluster must explicitly allow inbound connections on its port (default is 5439) from QuickSight.\n\t•\tSince the QuickSight deployment is in a different AWS account and VPC, and security groups are stateful, you need to modify the Redshift security group to accept traffic from the QuickSight security group or from the specific IP range or CIDR block used by QuickSight.\n\t•\tThis enables QuickSight to initiate queries to Redshift to refresh dashboards.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305582-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 236 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 236,
    "question_text": "A data engineer is building a data pipeline. A large data file is uploaded to an Amazon S3 bucket once each day at unpredictable times. An AWS Glue workflow uses hundreds of workers to process the file and load the data into Amazon Redshift. The company wants to process the file as quickly as possible.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an on-demand AWS Glue trigger to start the workflow. Create an AWS Lambda function that runs every 15 minutes to check the S3 bucket for the daily file. Configure the function to start the AWS Glue workflow if the file is present."
      },
      {
        "letter": "B",
        "text": "Create an event-based AWS Glue trigger to start the workflow. Configure Amazon S3 to log events to AWS CloudTrail. Create a rule in Amazon EventBridge to forward PutObject events to the AWS Glue trigger."
      },
      {
        "letter": "C",
        "text": "Create a scheduled AWS Glue trigger to start the workflow. Create a cron job that runs the AWS Glue job every 15 minutes. Set up the AWS Glue job to check the S3 bucket for the daily file. Configure the job to stop if the file is not present."
      },
      {
        "letter": "D",
        "text": "Create an on-demand AWS Glue trigger to start the workflow. Create an AWS Database Migration Service (AWS DMS) migration task. Set the DMS source as the S3 bucket. Set the target endpoint as the AWS Glue workflow."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 06:52",
        "comment": "This solution uses event-driven architecture: it automatically triggers the AWS Glue workflow as soon as the file is uploaded to S3.\n\t•\tCloudTrail logs the PutObject event, and EventBridge detects that event and invokes the Glue trigger.\n\t•\tIt is the fastest and most efficient way to start the workflow without polling or delay.\n\t•\tAlso, it avoids unnecessary Glue invocations, which saves costs.",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305583-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 237 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 237,
    "question_text": "A data engineer needs to run a data transformation job whenever a user adds a file to an Amazon S3 bucket. The job will run for less than 1 minute. The job must send the output through an email message to the data engineer. The data engineer expects users to add one file every hour of the day.\nWhich solution will meet these requirements in the MOST operationally efficient way?",
    "choices": [
      {
        "letter": "A",
        "text": "Create a small Amazon EC2 instance that polls the S3 bucket for new files. Run transformation code on a schedule to generate the output. Use operating system commands to send email messages."
      },
      {
        "letter": "B",
        "text": "Run an Amazon Elastic Container Service (Amazon ECS) task to poll the S3 bucket for new files. Run transformation code on a schedule to generate the output. Use operating system commands to send email messages."
      },
      {
        "letter": "C",
        "text": "Create an AWS Lambda function to transform the data. Use Amazon S3 Event Notifications to invoke the Lambda function when a new object is created. Publish the output to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the data engineer’s email account to the topic."
      },
      {
        "letter": "D",
        "text": "Deploy an Amazon EMR cluster. Use EMR File System (EMRFS) to access the files in the S3 bucket. Run transformation code on a schedule to generate the output to a second S3 bucket. Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure Amazon S3 Event Notifications to notify the topic when a new object is created."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:01",
        "comment": "Create an AWS Lambda function to transform the data. Use Amazon S3 Event Notifications to invoke the Lambda function when a new object is created. Publish the output to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the data engineer’s email account to the topic.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305584-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 238 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 238,
    "question_text": "A company uses Amazon S3 and AWS Glue Data Catalog to manage a data lake that contains contact information for customers. The company uses PySpark and AWS Glue jobs with a DynamicFrame to run a workflow that processes data within the data lake.\nA data engineer notices that the workflow is generating errors as a result of how customer postal codes are stored in the data lake. Some postal codes include unnecessary numbers or invalid characters.\nThe data engineer needs a solution to address the errors and correct the postal codes in the data lake.",
    "choices": [
      {
        "letter": "A",
        "text": "Create a schema definition for PySpark that matches the format the processing workflow requires for postal codes. Pass the schema to the DynamicFrame during processing."
      },
      {
        "letter": "B",
        "text": "Use AWS Glue workflow properties to allow job state sharing. Configure the AWS Glue jobs to read values from the postal code column by using the properties from a previously successful run of the jobs."
      },
      {
        "letter": "C",
        "text": "Configure the column.push_down_predicate setting and the catalogPartitionPredicate settings for the postal code column in the DynamicFrame."
      },
      {
        "letter": "D",
        "text": "Set the DynamicFrame additional_options parameter ‘useS3ListImplementation’ to True."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:03",
        "comment": "The core issue is inconsistent or invalid postal code formats in the data, which causes errors during processing.\n\t•\tOption A addresses this by enforcing a defined schema — this ensures postal codes are interpreted correctly (e.g., as strings with a certain format).\n\t•\tWhen you pass a schema to a DynamicFrame, PySpark will cast and clean the data according to the schema. This helps to:\n\t•\tFilter out or transform invalid entries.\n\t•\tStandardize the data format before further processing.\n\t•\tThis is a standard and effective approach to deal with data format inconsistencies in ETL workflows.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305585-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 239 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 239,
    "question_text": "A data engineer is troubleshooting an AWS Glue workflow that occasionally fails. The engineer determines that the failures are a result of data quality issues. A business reporting team needs to receive an email notification any time the workflow fails in the future.\nWhich solution will meet this requirement?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe the team’s email account to the SNS topic. Create an AWS Lambda function that initiates when the AWS Glue job state changes to FAILED. Set the SNS topic as the target."
      },
      {
        "letter": "B",
        "text": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Subscribe the team’s email account to the SNS topic. Create an Amazon EventBridge rule that triggers when the AWS Glue job state changes to FAILED. Set the SNS topic as the target."
      },
      {
        "letter": "C",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Subscribe the team’s email account to the SQS queue. Create an AWS Config rule that triggers when the AWS Glue job state changes to FAILED. Set the SQS queue as the target."
      },
      {
        "letter": "D",
        "text": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue. Subscribe the team’s email account to the SQS queue. Create an Amazon EventBridge rule that triggers when the AWS Glue job state changes to FAILESet the SQS queue as the target."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Thu 28 Aug 2025 04:43",
        "comment": "Answer B\nEventBridge can directly detect AWS Glue job state changes, including failures.\n\nYou can configure it to trigger notifications automatically without needing custom code or manual intervention.\n\nSNS standard topics support email subscriptions. setup is serverless, low-maintenance, and scalable, which means minimal operational overhead.\n\nA. SNS FIFO + Lambda\tFIFO topics are unnecessary for alerts, and Lambda adds complexity.\nC. SQS FIFO + AWS Config\tAWS Config doesn’t monitor Glue job states; SQS isn’t ideal for email alerts.\nD. SQS standard + EventBridge\tSQS doesn’t support direct email notifications; adds unnecessary queue management.",
        "selected_answer": "B"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:07",
        "comment": "Sns standard",
        "selected_answer": "B"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305586-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 240 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 240,
    "question_text": "A company uses AWS Glue jobs to implement several data pipelines. The pipelines are critical to the company.\nThe company needs to implement a monitoring mechanism that will alert stakeholders if the pipelines fail.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "choices": [
      {
        "letter": "A",
        "text": "Create an Amazon EventBridge rule to match AWS Glue job failure events. Configure the rule to target an AWS Lambda function to process events. Configure the function to send notifications to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      {
        "letter": "B",
        "text": "Configure an Amazon CloudWatch Logs log group for the AWS Glue jobs. Create an Amazon EventBridge rule to match new log creation events in the log group. Configure the rule to target an AWS Lambda function that reads the logs and sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic if AWS Glue job failure logs are present."
      },
      {
        "letter": "C",
        "text": "Create an Amazon EventBridge rule to match AWS Glue job failure events. Define an Amazon CloudWatch metric based on the EventBridge rule. Set up a CloudWatch alarm based on the metric to send notifications to an Amazon Simple Notification Service (Amazon SNS) topic."
      },
      {
        "letter": "D",
        "text": "Configure an Amazon CloudWatch Logs log group for the AWS Glue jobs. Create an Amazon EventBridge rule to match new log creation events in the log group. Configure the rule to send notifications to an Amazon Simple Notification Service (Amazon SNS) topic."
      }
    ],
    "correct_answer": "C",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Thu 28 Aug 2025 04:08",
        "comment": "Answer C\nEventBridge natively supports AWS Glue job state change events, including failures.\n\nYou can directly create a CloudWatch metric from these events without needing to process logs or run custom code.\n\nThen, you simply configure a CloudWatch alarm to notify stakeholders via SNS—a fully managed and scalable solution.\n\nThis setup is serverless, automated, and low-maintenance, which minimizes operational burden.\n\n\nA. EventBridge → Lambda → SNS\tRequires maintaining a Lambda function, adding complexity.\nB. CloudWatch Logs → Lambda → SNS\tInvolves parsing logs and managing Lambda—more overhead.\nD. CloudWatch Logs → SNS\tDoesn’t reliably detect job failures without custom parsing logic.",
        "selected_answer": "C"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:11",
        "comment": "Not sure. A or C.",
        "selected_answer": "C"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305587-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 241 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 241,
    "question_text": "A company uses AWS Glue Apache Spark jobs to handle extract, transform, and load (ETL) workloads. The company has enabled logging and monitoring for all AWS Glue jobs.\nOne of the AWS Glue jobs begins to fail. A data engineer investigates the error and wants to examine metrics for all individual stages within the job.\nHow can the data engineer access the stage metrics?",
    "choices": [
      {
        "letter": "A",
        "text": "Examine the AWS Glue job and stage details in the Spark UI."
      },
      {
        "letter": "B",
        "text": "Examine the AWS Glue job and stage metrics in Amazon CloudWatch."
      },
      {
        "letter": "C",
        "text": "Examine the AWS Glue job and stage logs in AWS CloudTrail logs."
      },
      {
        "letter": "D",
        "text": "Examine the AWS Glue job and stage details by using the run insights feature on the job."
      }
    ],
    "correct_answer": "A",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Thu 28 Aug 2025 03:41",
        "comment": "Answer is A",
        "selected_answer": "A"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:14",
        "comment": "AWS Glue uses Apache Spark under the hood for distributed data processing. When you run a Glue job, especially a Spark-based ETL job, AWS Glue provides a Spark UI where you can inspect:\n\t•\tStages\n\t•\tTasks\n\t•\tExecution time\n\t•\tShuffle read/write\n\t•\tInput/output records\n\nThis is the most detailed and appropriate way to examine metrics at the stage level.",
        "selected_answer": "A"
      }
    ]
  },
  {
    "url": "https://www.examtopics.com/discussions/amazon/view/305588-exam-aws-certified-data-engineer-associate-dea-c01-topic-1/",
    "title": "Exam AWS Certified Data Engineer - Associate DEA-C01 topic 1 question 242 discussion - ExamTopics",
    "topic": "Topic 1",
    "question_number": 242,
    "question_text": "A data engineer notices slow query performance on a highly partitioned table that is in Amazon Athena. The table contains daily data for the previous 5 years, partitioned by date.\nThe data engineer wants to improve query performance and to automate partition management.\nWhich solution will meet these requirements?",
    "choices": [
      {
        "letter": "A",
        "text": "Use an AWS Lambda function that runs daily. Configure the function to manually create new partitions in AWS Glue for each day’s data."
      },
      {
        "letter": "B",
        "text": "Use partition projection in Athena. Configure the table properties by using a date range from 5 years ago to the present."
      },
      {
        "letter": "C",
        "text": "Reduce the number of partitions by changing the partitioning schema from daily to monthly granularity."
      },
      {
        "letter": "D",
        "text": "Increase the processing capacity of Athena queries by allocating more compute resources."
      }
    ],
    "correct_answer": "B",
    "vote_distribution": [
      {
        "answer": "A",
        "percentage": 35
      },
      {
        "answer": "C",
        "percentage": 25
      },
      {
        "answer": "B",
        "percentage": 20
      }
    ],
    "discussion": [
      {
        "author": "llk",
        "date": "Thu 28 Aug 2025 03:58",
        "comment": "Answer is B \nA. Lambda function to create partitions\tManual and operationally heavy. Doesn’t scale well or improve query performance.\nC. Change to monthly partitions\tReduces partition count but sacrifices granularity. Doesn’t automate management.\nD. Allocate more compute resources\tAthena is serverless—you can't manually allocate compute. Performance depends more on query optimization and partitioning.",
        "selected_answer": "B"
      },
      {
        "author": "rdiaz",
        "date": "Fri 04 Jul 2025 07:17",
        "comment": "Partition projection in Amazon Athena improves performance and eliminates the need to manually manage partitions. It allows Athena to calculate partition values dynamically at query time, rather than relying on a metadata catalog lookup for every partition.\n\nThis is particularly effective for highly partitioned tables, such as those with daily partitions over multiple years (i.e., 1,800+ partitions in this case). It reduces query planning time and improves performance.",
        "selected_answer": "B"
      }
    ]
  }
]